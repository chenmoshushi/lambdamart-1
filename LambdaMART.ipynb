{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from copy import deepcopy\n",
    "from collections import defaultdict\n",
    "import matplotlib as plt\n",
    "import pickle\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results_buf1 = []\n",
    "results_buf2 = []\n",
    "results_buf3 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_dict_by_qid(qids, pred_y):\n",
    "    \"\"\"\n",
    "        Возвращает словарь, в котором по qid можно получить список индексов. \n",
    "        Каждый индекс соответствует месту элемента с этим индексом в поисковой выдаче для данного qid.\n",
    "    \"\"\"\n",
    "    # Найдём для каждого из запросов порядок поисковой выдачи\n",
    "    unique_qids = np.unique(qids)\n",
    "    id_y = defaultdict(np.array)\n",
    "    n_elems = pred_y.shape[0]\n",
    "    for qid in unique_qids:\n",
    "        # Индексы тех элементов, у которых запрос равен qid\n",
    "        ids_for_qid = np.arange(n_elems)[qids==qid]\n",
    "        # Тот же pred_y, только, у которого элементы с неравным qid будут равны 0\n",
    "        buf = np.zeros(n_elems)\n",
    "        for idx in ids_for_qid:\n",
    "            buf[idx] = pred_y[idx]\n",
    "        id_y[qid] = np.argsort(np.argsort(buf)[::-1])\n",
    "    return id_y\n",
    "\n",
    "\n",
    "def ndcgl(pred_y, y, qids, L=10):\n",
    "    \"\"\"\n",
    "        pred_y : предсказанные значения функции ранжирования\n",
    "        y : метки релевантности\n",
    "        qids : id запросов, для каждого элемента\n",
    "    \"\"\"\n",
    "    id_y = get_dict_by_qid(qids, pred_y)\n",
    "    buf = 0.\n",
    "    for qid in np.unique(qids):\n",
    "        dcgl = 0.\n",
    "        idcgl = 0.\n",
    "        # Отсортируем значения по убыванию функции ранжирования\n",
    "        #idx = np.argsort(y_pred)[::-1]\n",
    "        idx = id_y[qid]\n",
    "        # Метки релевантности для отсортированного массива y_pred\n",
    "        #l = y[idx]\n",
    "        for i in xrange(L):\n",
    "            #if qids[i] == qid:\n",
    "            # Метка релевантности для данного элемента\n",
    "            l = y[idx == i][0]\n",
    "            dcgl += (2. ** l - 1) / np.log2(i + 2)\n",
    "            idcgl += 1 / np.log2(i + 2)\n",
    "        buf += dcgl / idcgl\n",
    "    return buf / len(np.unique(qids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class LambdaMART:\n",
    "    def __init__(self, n_estimators, base_estimator=DecisionTreeRegressor(), step_estimator=DecisionTreeRegressor(),\n",
    "                 alpha=1., beta=1., adaptive_step=True, feature_subset=False, feature_fraction=1., stochastic=False):\n",
    "        \"\"\"\n",
    "            n_estimators : число деревьев в обучении\n",
    "            base_estimator : выбор модели для каждой итерации\n",
    "            step_estimator : выбор модели для предсказания темпа обучения\n",
    "            alpha : коэффициент регуляризации XGBoost\n",
    "            beta : коэффициент при предсказании темпа обучения\n",
    "            adaptive_step : использовать адаптивный шаг бустинга или нет\n",
    "            feature_subset : использовать случайный набор признаков или нет\n",
    "        \"\"\"\n",
    "        self.estimators = []\n",
    "        self.step_estimators = []\n",
    "        self.feature_idx = []\n",
    "        self.n_estimators = n_estimators\n",
    "        self.base_estimator = base_estimator\n",
    "        self.step_estimator = step_estimator\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.adaptive_step = adaptive_step\n",
    "        self.feature_subset = feature_subset\n",
    "        self.feature_fraction = feature_fraction\n",
    "        self.stochastic = stochastic\n",
    "        \n",
    "    def fit(self, X_train, y_train, qids_train, queries_train, X_test, y_test, qids_test, queries_test, verbose=False):\n",
    "        \"\"\"\n",
    "            X : признаки пар запрос-документ\n",
    "            y : метки релевантности\n",
    "            qids : id запросов\n",
    "            queries : признаки запросов\n",
    "        \"\"\"\n",
    "        X = X_train\n",
    "        y = y_train\n",
    "        qids = qids_train\n",
    "        queries = queries_train\n",
    "        self.estimators = []\n",
    "        self.step_estimators = []\n",
    "        self.last_iteration = 0\n",
    "        # Препроцессинг для ускорения подсчитывания градиента\n",
    "        # id_y = np.argsort(np.argsort(y_train)[::-1])\n",
    "        for i in xrange(self.n_estimators):\n",
    "            predicted = self.predict(X_train, qids_train, queries_train, verbose)\n",
    "            y_pred = self.predict(X_test, qids_test, queries_test)\n",
    "            train_result = ndcgl(predicted, y_train, qids_train)\n",
    "            test_result = ndcgl(y_pred, y_test, qids_test)\n",
    "            print \"Iteration {}, train_result {}, test_result {}\".format(i, train_result, test_result)\n",
    "            if self.feature_subset:\n",
    "                results_buf1.append((i, train_result, test_result))\n",
    "            else:\n",
    "                results_buf2.append((i, train_result, test_result))\n",
    "                #print \"Iteration \", i, \"train_result\", ndcgl(predicted) \"\" test_result \", \n",
    "            sub_idx = []\n",
    "            if self.stochastic:\n",
    "                # Выбираем подвыборку элементов\n",
    "                sub_idx = np.random.choice(np.arange(X_train.shape[0]), int(0.6 * X_train.shape[0]), replace=False)\n",
    "                #grad, h = self.loss_grad(predicted[sub_idx], y_train[sub_idx], id_y[sub_idx])\n",
    "                grad, h = self.loss_grad(predicted[sub_idx], y_train[sub_idx], qids_train[sub_idx])\n",
    "                X = X_train[sub_idx]\n",
    "                y = y_train[sub_idx]\n",
    "                qids = qids_train[sub_idx]\n",
    "                queries = queries_train[sub_idx]\n",
    "            else:\n",
    "                #grad, h = self.loss_grad(predicted, y, id_y)\n",
    "                grad, h = self.loss_grad(predicted, y, qids)\n",
    "            if verbose:\n",
    "                print \"Grad:\"\n",
    "                print grad\n",
    "                print \"H:\"\n",
    "                print h\n",
    "            estimator = deepcopy(self.base_estimator)\n",
    "            if self.feature_subset:\n",
    "                # Список индексов фичей\n",
    "                feature_idx = np.random.choice(np.arange(X.shape[1]), int(self.feature_fraction * X.shape[1]), replace=False)\n",
    "                self.feature_idx.append(feature_idx)\n",
    "                estimator.fit(X.T[feature_idx].T, -grad/ (self.alpha + h))\n",
    "            else:\n",
    "                estimator.fit(X, -grad/ (self.alpha + h))\n",
    "            self.estimators.append(estimator)\n",
    "            if verbose:\n",
    "                print \"Predict:\"\n",
    "                print predicted\n",
    "            # Обучим предсказатель шага\n",
    "            if self.adaptive_step:\n",
    "                step_estimator = deepcopy(self.step_estimator)\n",
    "                # Список всех запросов(уникальных)\n",
    "                qs = np.unique(qids)\n",
    "                # Список значений того, что нужно предсказывать\n",
    "                pred_values = np.zeros(len(qs))\n",
    "                q_list = np.zeros((len(qs), queries.shape[1]))\n",
    "                for idx, q in enumerate(qs):\n",
    "                    # Суммируем значения по всем элементам с данным запросом q\n",
    "                    pred_values[idx] = 0.\n",
    "                    for j in xrange(X.shape[0]):\n",
    "                        if qids[j] == q:\n",
    "                            q_list[idx] = queries[j]\n",
    "                            pred_values[idx] += 1. / (1. + self.beta * h[j])            \n",
    "                    #q_list[idx] = q_dict[q]\n",
    "                sum_pred_values = np.sum(pred_values)\n",
    "                step_estimator.fit(q_list, pred_values * len(qs) / sum_pred_values)\n",
    "                self.step_estimators.append(step_estimator)\n",
    "            self.last_iteration = i\n",
    "            if i % 50 == 0:\n",
    "                # Выгрузить информацию об алгоритме в файл\n",
    "                name = \"\"\n",
    "                if self.feature_subset:\n",
    "                    name = \"fs_algo\"\n",
    "                else:\n",
    "                    name = \"no_fs_algo\"\n",
    "                if self.stochastic:\n",
    "                    name = \"stoch_algo\"\n",
    "                pickle.dump(self, open(name, \"wb\"))\n",
    "        return self\n",
    "            \n",
    "    def predict(self, X, qids, queries, verbose=False):\n",
    "        y_pred = np.zeros(X.shape[0])\n",
    "        for est_i, estimator in enumerate(self.estimators):\n",
    "            if self.feature_subset:\n",
    "                feature_idx = self.feature_idx[est_i]\n",
    "                est_result = estimator.predict(X.T[feature_idx].T)\n",
    "            else:\n",
    "                est_result = estimator.predict(X)\n",
    "            for i in xrange(X.shape[0]):\n",
    "                step = 1. / (i + 1.)\n",
    "                if self.adaptive_step:\n",
    "                    step = self.step_estimators[est_i].predict(queries[i].reshape(1,-1)[0])\n",
    "                if verbose:\n",
    "                    print \"Step: \", step, \" for qid=\", qids[i]\n",
    "                y_pred[i] += est_result[i] * step\n",
    "        return y_pred\n",
    "    \n",
    "    def continue_fit(self, n_estimators, X_train, y_train, qids_train, queries_train, X_test, y_test, qids_test, queries_test):\n",
    "        X = X_train\n",
    "        y = y_train\n",
    "        qids = qids_train\n",
    "        queries = queries_train\n",
    "        prev_n_estimators = self.n_estimators\n",
    "        self.n_estimators = n_estimators\n",
    "        # Препроцессинг для ускорения подсчитывания градиента\n",
    "        # id_y = np.argsort(np.argsort(y_train)[::-1])\n",
    "        for i in xrange(prev_n_estimators, self.n_estimators):\n",
    "            predicted = self.predict(X_train, qids_train, queries_train, False)\n",
    "            y_pred = self.predict(X_test, qids_test, queries_test)\n",
    "            train_result = ndcgl(predicted, y_train, qids_train)\n",
    "            test_result = ndcgl(y_pred, y_test, qids_test)\n",
    "            print \"Iteration {}, train_result {}, test_result {}\".format(i, train_result, test_result)\n",
    "            if self.feature_subset:\n",
    "                results_buf1.append((i, train_result, test_result))\n",
    "            else:\n",
    "                results_buf2.append((i, train_result, test_result))\n",
    "            sub_idx = []\n",
    "            if self.stochastic:\n",
    "                # Выбираем подвыборку элементов\n",
    "                sub_idx = np.random.choice(np.arange(X_train.shape[0]), int(0.6 * X_train.shape[0]), replace=False)\n",
    "                grad, h = self.loss_grad(predicted[sub_idx], y_train[sub_idx], qids_train[sub_idx])\n",
    "                X = X_train[sub_idx]\n",
    "                y = y_train[sub_idx]\n",
    "                qids = qids_train[sub_idx]\n",
    "                queries = queries_train[sub_idx]\n",
    "            else:\n",
    "                grad, h = self.loss_grad(predicted, y, qids)\n",
    "            estimator = deepcopy(self.base_estimator)\n",
    "            if self.feature_subset:\n",
    "                # Список индексов фичей\n",
    "                feature_idx = np.random.choice(np.arange(X.shape[1]), int(self.feature_fraction * X.shape[1]), replace=False)\n",
    "                self.feature_idx.append(feature_idx)\n",
    "                estimator.fit(X.T[feature_idx].T, -grad/ (self.alpha + h))\n",
    "            else:\n",
    "                estimator.fit(X, -grad/ (self.alpha + h))\n",
    "            self.estimators.append(estimator)\n",
    "            # Обучим предсказатель шага\n",
    "            if self.adaptive_step:\n",
    "                step_estimator = deepcopy(self.step_estimator)\n",
    "                # Список всех запросов(уникальных)\n",
    "                qs = np.unique(qids)\n",
    "                # Список значений того, что нужно предсказывать\n",
    "                pred_values = np.zeros(len(qs))\n",
    "                q_list = np.zeros((len(qs), queries.shape[1]))\n",
    "                for idx, q in enumerate(qs):\n",
    "                    # Суммируем значения по всем элементам с данным запросом q\n",
    "                    pred_values[idx] = 0.\n",
    "                    for j in xrange(X.shape[0]):\n",
    "                        if qids[j] == q:\n",
    "                            q_list[idx] = queries[j]\n",
    "                            pred_values[idx] += 1. / (1. + self.beta * h[j])            \n",
    "                    #q_list[idx] = q_dict[q]\n",
    "                sum_pred_values = np.sum(pred_values)\n",
    "                step_estimator.fit(q_list, pred_values * len(qs) / sum_pred_values)\n",
    "                self.step_estimators.append(step_estimator)\n",
    "            self.last_iteration = i\n",
    "            if i % 50 == 0:\n",
    "                # Выгрузить информацию об алгоритме в файл\n",
    "                name = \"\"\n",
    "                if self.feature_subset:\n",
    "                    name = \"fs_algo\"\n",
    "                else:\n",
    "                    name = \"no_fs_algo\"\n",
    "                if self.stochastic:\n",
    "                    name = \"stoch_algo\"\n",
    "                pickle.dump(self, open(name, \"wb\"))\n",
    "        return self\n",
    "    \n",
    "    def loss_grad(self, pred_y, y, qids):\n",
    "        n_elems = y.shape[0]\n",
    "        grad = np.zeros(n_elems)\n",
    "        h = np.zeros(n_elems)\n",
    "        # id_y - индексы в массиве pred_y, отсортированные по убыванию ранжирующей функции \n",
    "        #id_y = np.argsort(np.argsort(y)[::-1])\n",
    "        # Найдём для каждого из запросов порядок поисковой выдачи\n",
    "        id_y = get_dict_by_qid(qids, pred_y)\n",
    "        for i in xrange(n_elems):\n",
    "            for j in xrange(n_elems):\n",
    "                if qids[i] == qids[j]:\n",
    "                    buf = 0.\n",
    "                    delta_ndcg = np.abs(self.ndcg_replace(y[i], y[j], id_y[qids[i]][i], id_y[qids[j]][j]))\n",
    "                    if y[i] > y[j]:\n",
    "                        buf = -self.rho(pred_y[i], pred_y[j])\n",
    "                    if y[i] < y[j]:\n",
    "                        buf = self.rho(pred_y[j], pred_y[i])\n",
    "                    if buf != 0.:\n",
    "                        grad[i] += delta_ndcg * buf\n",
    "                    h[i] += delta_ndcg * self.rho(pred_y[i], pred_y[j]) * (1 - self.rho(pred_y[i], pred_y[j]))\n",
    "        return grad, h\n",
    "    \n",
    "    def ndcg_replace(self, value_a, value_b, id_a, id_b):\n",
    "        return (2. ** value_b - 2. ** value_a) * (1./ np.log2(2 + id_a) - 1./ np.log2(2 + id_b))\n",
    "    \n",
    "    def rho(self, a, b):\n",
    "        return 1. / (1. + np.exp(a - b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Считаем часть датасета MQ2007"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Проверка на части датасета MQ2007\n",
    "fin = open('train.txt', 'r')\n",
    "\n",
    "num_elems = 10000\n",
    "X = np.zeros((num_elems, 46))\n",
    "y = np.zeros(num_elems)\n",
    "queries = np.zeros(num_elems)\n",
    "\n",
    "num = 0\n",
    "i = 0\n",
    "\n",
    "lines = fin.readlines()[:num_elems]\n",
    "\n",
    "for i, line in enumerate(lines):\n",
    "    parsed = line.split(' ')\n",
    "    y[i] = int(parsed[0])\n",
    "    queries[i] = int(parsed[1][4:])\n",
    "    for j in xrange(46):\n",
    "        if j < 9:\n",
    "            X[i][j] = float(parsed[j + 2][2:])\n",
    "        else:\n",
    "            X[i][j] = float(parsed[j + 2][3:])\n",
    "                \n",
    "# Нормализуем y\n",
    "y /= np.max(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Генерируем признаки для запросов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# Признаки для запросов\n",
    "query_d = 20 # Число признаков в запросе. \n",
    "q_dict = defaultdict(lambda : np.zeros(query_d)) # dictionary, который по id запроса возвращает его признаки\n",
    "\n",
    "unique_q, counts = np.unique(queries, return_counts=True)\n",
    "for i,q in enumerate(unique_q):\n",
    "    for j in xrange(X.shape[0]):\n",
    "        if queries[j] == q:\n",
    "            q_dict[q] += X[j][11:31]/ counts[i]\n",
    "            #q_dict[q] += X[j][:40]/ counts[i]\n",
    "\n",
    "# Финальная матрица для запросов. По индексу получаем признаки запроса, соответствующие элементу из X с этим индексом\n",
    "q = np.zeros((num_elems, query_d))\n",
    "for i in xrange(num_elems):\n",
    "    q[i] = q_dict[queries[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Хорошее перемешивание\n",
    "unique_q = np.unique(queries)\n",
    "num_q = unique_q.shape[0]\n",
    "q_idx = np.random.permutation(num_q)\n",
    "Q_for_train = unique_q[q_idx][:num_q*3/4]\n",
    "Q_for_test = unique_q[q_idx][num_q*3/4:]\n",
    "\n",
    "train_idx = np.zeros(0, dtype=int32)\n",
    "test_idx = np.zeros(0, dtype=int32)\n",
    "\n",
    "for i, q_id in enumerate(queries):\n",
    "    if (Q_for_train == q_id).any():\n",
    "        train_idx = np.append(train_idx, int(i))\n",
    "    else:\n",
    "        test_idx = np.append(test_idx, int(i))\n",
    "\n",
    "X_train = X[train_idx]\n",
    "y_train = y[train_idx]\n",
    "q_train = queries[train_idx]\n",
    "queries_train = q[train_idx]\n",
    "\n",
    "X_test = X[test_idx]\n",
    "y_test = y[test_idx]\n",
    "q_test = queries[test_idx]\n",
    "queries_test = q[test_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Результаты на 10000 элементах"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.155186474972\n",
      "0.432043887205\n"
     ]
    }
   ],
   "source": [
    "clf_temp = DecisionTreeRegressor().fit(X_train, y_train)\n",
    "print ndcgl(clf_temp.predict(X_test), y_test, q_test)\n",
    "print ndcgl(clf_temp.predict(X_train), y_train, q_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n = 301\n",
    "clf2 = LambdaMART(n, alpha=0.5, beta=1., feature_subset=True, feature_fraction=0.3)\n",
    "clf1 = LambdaMART(n, alpha=0.5, beta=1.)\n",
    "clf3 = LambdaMART(n, alpha=0.5, beta=1., feature_subset=True, feature_fraction=0.3, stochastic=True)\n",
    "results_buf1 = []\n",
    "results_buf2 = []\n",
    "results_buf3 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, train_result 0.0, test_result 0.0\n",
      "Iteration 1, train_result 0.4099556694, test_result 0.151405349521\n",
      "Iteration 2, train_result 0.42131550436, test_result 0.164070825496\n",
      "Iteration 3, train_result 0.421515919601, test_result 0.16653721379\n",
      "Iteration 4, train_result 0.421371916359, test_result 0.171495621272\n",
      "Iteration 5, train_result 0.421371916359, test_result 0.175859514664\n",
      "Iteration 6, train_result 0.421521880282, test_result 0.173748044054\n",
      "Iteration 7, train_result 0.421093474441, test_result 0.173461176337\n",
      "Iteration 8, train_result 0.420878924571, test_result 0.175060916901\n",
      "Iteration 9, train_result 0.420686206352, test_result 0.178050653385\n",
      "Iteration 10, train_result 0.420830209595, test_result 0.178818616504\n",
      "Iteration 11, train_result 0.420830209595, test_result 0.175598856777\n",
      "Iteration 12, train_result 0.420686206352, test_result 0.177729969935\n",
      "Iteration 13, train_result 0.420536242429, test_result 0.180014896336\n",
      "Iteration 14, train_result 0.420536242429, test_result 0.177339745994\n",
      "Iteration 15, train_result 0.420536242429, test_result 0.179911621298\n",
      "Iteration 16, train_result 0.42088389701, test_result 0.18116979071\n",
      "Iteration 17, train_result 0.421027900252, test_result 0.181419639961\n",
      "Iteration 18, train_result 0.421205351762, test_result 0.182227673432\n",
      "Iteration 19, train_result 0.421349355004, test_result 0.182001606283\n",
      "Iteration 20, train_result 0.421742442931, test_result 0.181025045479\n",
      "Iteration 21, train_result 0.421791157907, test_result 0.179785530604\n",
      "Iteration 22, train_result 0.422289327277, test_result 0.182905653636\n",
      "Iteration 23, train_result 0.422408799076, test_result 0.183142254579\n",
      "Iteration 24, train_result 0.422767352187, test_result 0.182658870062\n",
      "Iteration 25, train_result 0.422707518576, test_result 0.183300056073\n",
      "Iteration 26, train_result 0.422524011697, test_result 0.182108696422\n",
      "Iteration 27, train_result 0.423272516842, test_result 0.183015901929\n",
      "Iteration 28, train_result 0.422565360235, test_result 0.182988378504\n",
      "Iteration 29, train_result 0.423225455111, test_result 0.183644640225\n",
      "Iteration 30, train_result 0.423388570731, test_result 0.182553742675\n",
      "Iteration 31, train_result 0.42321836766, test_result 0.182944860635\n",
      "Iteration 32, train_result 0.42352647566, test_result 0.182464354043\n",
      "Iteration 33, train_result 0.42379207056, test_result 0.18226415736\n",
      "Iteration 34, train_result 0.42379207056, test_result 0.182177610081\n",
      "Iteration 35, train_result 0.423855628041, test_result 0.182132358794\n",
      "Iteration 36, train_result 0.424110407822, test_result 0.181974697803\n",
      "Iteration 37, train_result 0.424211083357, test_result 0.182717011452\n",
      "Iteration 38, train_result 0.42328321433, test_result 0.183770209616\n",
      "Iteration 39, train_result 0.423881143708, test_result 0.183726244825\n",
      "Iteration 40, train_result 0.423911272615, test_result 0.185443262151\n",
      "Iteration 41, train_result 0.424061236538, test_result 0.185577524489\n",
      "Iteration 42, train_result 0.423493011864, test_result 0.18607945141\n",
      "Iteration 43, train_result 0.423847290929, test_result 0.18662316954\n",
      "Iteration 44, train_result 0.423598206245, test_result 0.184314068863\n",
      "Iteration 45, train_result 0.42375830202, test_result 0.184924995077\n",
      "Iteration 46, train_result 0.424102740805, test_result 0.184842492601\n",
      "Iteration 47, train_result 0.424191729715, test_result 0.183542414728\n",
      "Iteration 48, train_result 0.424633532618, test_result 0.184389801539\n",
      "Iteration 49, train_result 0.424685188432, test_result 0.182762201568\n",
      "Iteration 50, train_result 0.424436103747, test_result 0.183651458358\n",
      "Iteration 51, train_result 0.424436103747, test_result 0.183251097237\n",
      "Iteration 52, train_result 0.424435882183, test_result 0.184443583479\n",
      "Iteration 53, train_result 0.424435882183, test_result 0.182144559935\n",
      "Iteration 54, train_result 0.424435882183, test_result 0.182165527372\n",
      "Iteration 55, train_result 0.424435882183, test_result 0.182084126559\n",
      "Iteration 56, train_result 0.424186797499, test_result 0.183318743258\n",
      "Iteration 57, train_result 0.424186797499, test_result 0.182649249519\n",
      "Iteration 58, train_result 0.423937712814, test_result 0.183913800328\n",
      "Iteration 59, train_result 0.423937712814, test_result 0.183379875303\n",
      "Iteration 60, train_result 0.423937712814, test_result 0.183496899063\n",
      "Iteration 61, train_result 0.423937712814, test_result 0.184732653259\n",
      "Iteration 62, train_result 0.423937712814, test_result 0.184700042609\n",
      "Iteration 63, train_result 0.423937712814, test_result 0.183635505821\n",
      "Iteration 64, train_result 0.423937712814, test_result 0.184617940303\n",
      "Iteration 65, train_result 0.423937712814, test_result 0.185253659491\n",
      "Iteration 66, train_result 0.423937712814, test_result 0.185110738379\n",
      "Iteration 67, train_result 0.423937712814, test_result 0.184686172708\n",
      "Iteration 68, train_result 0.423937712814, test_result 0.186607164226\n",
      "Iteration 69, train_result 0.423937712814, test_result 0.186478295729\n",
      "Iteration 70, train_result 0.423937712814, test_result 0.186432495601\n",
      "Iteration 71, train_result 0.423937712814, test_result 0.185632459691\n",
      "Iteration 72, train_result 0.423937712814, test_result 0.185738789077\n",
      "Iteration 73, train_result 0.423937712814, test_result 0.185738789077\n",
      "Iteration 74, train_result 0.423937712814, test_result 0.185804284028\n",
      "Iteration 75, train_result 0.423937712814, test_result 0.186743285634\n",
      "Iteration 76, train_result 0.423937712814, test_result 0.185786401985\n",
      "Iteration 77, train_result 0.423937712814, test_result 0.184311714213\n",
      "Iteration 78, train_result 0.423937712814, test_result 0.185062783993\n",
      "Iteration 79, train_result 0.423937712814, test_result 0.185673738007\n",
      "Iteration 80, train_result 0.423937712814, test_result 0.186282992429\n",
      "Iteration 81, train_result 0.423937712814, test_result 0.186368714833\n",
      "Iteration 82, train_result 0.423937712814, test_result 0.186368714833\n",
      "Iteration 83, train_result 0.423937712814, test_result 0.186800724559\n",
      "Iteration 84, train_result 0.423937712814, test_result 0.186735229608\n",
      "Iteration 85, train_result 0.423937712814, test_result 0.185664101471\n",
      "Iteration 86, train_result 0.423937712814, test_result 0.186989386473\n",
      "Iteration 87, train_result 0.423937712814, test_result 0.184876494944\n",
      "Iteration 88, train_result 0.423937712814, test_result 0.186961468388\n",
      "Iteration 89, train_result 0.423937712814, test_result 0.184574872464\n",
      "Iteration 90, train_result 0.423937712814, test_result 0.184284169562\n",
      "Iteration 91, train_result 0.423937712814, test_result 0.184560896618\n",
      "Iteration 92, train_result 0.423937712814, test_result 0.183578366254\n",
      "Iteration 93, train_result 0.423937712814, test_result 0.183624166381\n",
      "Iteration 94, train_result 0.423937712814, test_result 0.183715341429\n",
      "Iteration 95, train_result 0.423937712814, test_result 0.183658351541\n",
      "Iteration 96, train_result 0.423937712814, test_result 0.18488760895\n",
      "Iteration 97, train_result 0.423937712814, test_result 0.183330296403\n",
      "Iteration 98, train_result 0.423937712814, test_result 0.183139016901\n",
      "Iteration 99, train_result 0.423937712814, test_result 0.183137437975\n",
      "Iteration 100, train_result 0.423937712814, test_result 0.184196868233\n",
      "Iteration 101, train_result 0.423937712814, test_result 0.184943370957\n",
      "Iteration 102, train_result 0.423937712814, test_result 0.184961253\n",
      "Iteration 103, train_result 0.423937712814, test_result 0.185058938423\n",
      "Iteration 104, train_result 0.423937712814, test_result 0.185032233869\n",
      "Iteration 105, train_result 0.423937712814, test_result 0.184938585565\n",
      "Iteration 106, train_result 0.423937712814, test_result 0.184770594461\n",
      "Iteration 107, train_result 0.423937712814, test_result 0.18492871326\n",
      "Iteration 108, train_result 0.423937712814, test_result 0.185035814651\n",
      "Iteration 109, train_result 0.423937712814, test_result 0.184877695852\n",
      "Iteration 110, train_result 0.423937712814, test_result 0.184625795125\n",
      "Iteration 111, train_result 0.423937712814, test_result 0.184670140011\n",
      "Iteration 112, train_result 0.423937712814, test_result 0.184670140011\n",
      "Iteration 113, train_result 0.423937712814, test_result 0.184773744457\n",
      "Iteration 114, train_result 0.423937712814, test_result 0.184621794894\n",
      "Iteration 115, train_result 0.423937712814, test_result 0.184847286867\n",
      "Iteration 116, train_result 0.423937712814, test_result 0.184950891313\n",
      "Iteration 117, train_result 0.423937712814, test_result 0.184818233214\n",
      "Iteration 118, train_result 0.423937712814, test_result 0.18485754988\n",
      "Iteration 119, train_result 0.423937712814, test_result 0.184934449773\n",
      "Iteration 120, train_result 0.423937712814, test_result 0.184775197343\n",
      "Iteration 121, train_result 0.423937712814, test_result 0.185107352001\n",
      "Iteration 122, train_result 0.423937712814, test_result 0.185042580839\n",
      "Iteration 123, train_result 0.423937712814, test_result 0.184564770985\n",
      "Iteration 124, train_result 0.423937712814, test_result 0.185101944781\n",
      "Iteration 125, train_result 0.423937712814, test_result 0.184632144159\n",
      "Iteration 126, train_result 0.423937712814, test_result 0.184658848712\n",
      "Iteration 127, train_result 0.423937712814, test_result 0.1851096783\n",
      "Iteration 128, train_result 0.423937712814, test_result 0.1851096783\n",
      "Iteration 129, train_result 0.423937712814, test_result 0.1850614007\n",
      "Iteration 130, train_result 0.423937712814, test_result 0.185120739689\n",
      "Iteration 131, train_result 0.423937712814, test_result 0.186191432439\n",
      "Iteration 132, train_result 0.423937712814, test_result 0.186209329222\n",
      "Iteration 133, train_result 0.423937712814, test_result 0.186135674232\n",
      "Iteration 134, train_result 0.423937712814, test_result 0.185598375207\n",
      "Iteration 135, train_result 0.423937712814, test_result 0.186187756175\n",
      "Iteration 136, train_result 0.423937712814, test_result 0.186291859498\n",
      "Iteration 137, train_result 0.423937712814, test_result 0.186291859498\n",
      "Iteration 138, train_result 0.423937712814, test_result 0.186291859498\n",
      "Iteration 139, train_result 0.423937712814, test_result 0.186261396424\n",
      "Iteration 140, train_result 0.423937712814, test_result 0.186261396424\n",
      "Iteration 141, train_result 0.423937712814, test_result 0.18640586538\n",
      "Iteration 142, train_result 0.423937712814, test_result 0.186309741541\n",
      "Iteration 143, train_result 0.423937712814, test_result 0.186479866084\n",
      "Iteration 144, train_result 0.423937712814, test_result 0.18637576276\n",
      "Iteration 145, train_result 0.423937712814, test_result 0.18643671999\n",
      "Iteration 146, train_result 0.423937712814, test_result 0.186501439131\n",
      "Iteration 147, train_result 0.423937712814, test_result 0.186501439131\n",
      "Iteration 148, train_result 0.423937712814, test_result 0.186523012178\n",
      "Iteration 149, train_result 0.423937712814, test_result 0.186501439131\n",
      "Iteration 150, train_result 0.423937712814, test_result 0.186305763553\n",
      "Iteration 151, train_result 0.423937712814, test_result 0.186279058999\n",
      "Iteration 152, train_result 0.423937712814, test_result 0.186305763553\n",
      "Iteration 153, train_result 0.423937712814, test_result 0.186555886811\n",
      "Iteration 154, train_result 0.423937712814, test_result 0.185948623801\n",
      "Iteration 155, train_result 0.423937712814, test_result 0.185909833403\n",
      "Iteration 156, train_result 0.423937712814, test_result 0.186270664159\n",
      "Iteration 157, train_result 0.423937712814, test_result 0.186520787418\n",
      "Iteration 158, train_result 0.423937712814, test_result 0.186603440622\n",
      "Iteration 159, train_result 0.423937712814, test_result 0.18653866946\n",
      "Iteration 160, train_result 0.423937712814, test_result 0.186565374013\n",
      "Iteration 161, train_result 0.423937712814, test_result 0.186687541405\n",
      "Iteration 162, train_result 0.423937712814, test_result 0.186604164411\n",
      "Iteration 163, train_result 0.423937712814, test_result 0.186669659362\n",
      "Iteration 164, train_result 0.423937712814, test_result 0.186604164411\n",
      "Iteration 165, train_result 0.423937712814, test_result 0.186604164411\n",
      "Iteration 166, train_result 0.423937712814, test_result 0.186604164411\n",
      "Iteration 167, train_result 0.423937712814, test_result 0.187036174138\n",
      "Iteration 168, train_result 0.423937712814, test_result 0.187036174138\n",
      "Iteration 169, train_result 0.423937712814, test_result 0.186604164411\n",
      "Iteration 170, train_result 0.423937712814, test_result 0.186604164411\n",
      "Iteration 171, train_result 0.423937712814, test_result 0.186630868964\n",
      "Iteration 172, train_result 0.423937712814, test_result 0.187018292095\n",
      "Iteration 173, train_result 0.423937712814, test_result 0.187036174138\n",
      "Iteration 174, train_result 0.423937712814, test_result 0.187018292095\n",
      "Iteration 175, train_result 0.423937712814, test_result 0.187022897333\n",
      "Iteration 176, train_result 0.423937712814, test_result 0.187083787046\n",
      "Iteration 177, train_result 0.424737019479, test_result 0.187083787046\n",
      "Iteration 178, train_result 0.424737019479, test_result 0.187083787046\n",
      "Iteration 179, train_result 0.424737019479, test_result 0.18716716404\n",
      "Iteration 180, train_result 0.424737019479, test_result 0.187149281997\n",
      "Iteration 181, train_result 0.424737019479, test_result 0.187122577444\n",
      "Iteration 182, train_result 0.424737019479, test_result 0.187122577444\n",
      "Iteration 183, train_result 0.424737019479, test_result 0.187057806282\n",
      "Iteration 184, train_result 0.424737019479, test_result 0.187122577444\n",
      "Iteration 185, train_result 0.424487934794, test_result 0.18717598655\n",
      "Iteration 186, train_result 0.424487934794, test_result 0.186556210025\n",
      "Iteration 187, train_result 0.424487934794, test_result 0.186529505472\n",
      "Iteration 188, train_result 0.424487934794, test_result 0.186529505472\n",
      "Iteration 189, train_result 0.424487934794, test_result 0.18646473431\n",
      "Iteration 190, train_result 0.424487934794, test_result 0.18646473431\n",
      "Iteration 191, train_result 0.424487934794, test_result 0.185684684549\n",
      "Iteration 192, train_result 0.424487934794, test_result 0.185684684549\n",
      "Iteration 193, train_result 0.424487934794, test_result 0.185650499389\n",
      "Iteration 194, train_result 0.424487934794, test_result 0.185515713524\n",
      "Iteration 195, train_result 0.424487934794, test_result 0.185515713524\n",
      "Iteration 196, train_result 0.424487934794, test_result 0.185515713524\n",
      "Iteration 197, train_result 0.424487934794, test_result 0.185515713524\n",
      "Iteration 198, train_result 0.424487934794, test_result 0.185515713524\n",
      "Iteration 199, train_result 0.424487934794, test_result 0.185515713524\n",
      "Iteration 200, train_result 0.424487934794, test_result 0.185450218573\n",
      "Iteration 201, train_result 0.424487934794, test_result 0.185476923127\n",
      "Iteration 202, train_result 0.424487934794, test_result 0.185476923127\n",
      "Iteration 203, train_result 0.424487934794, test_result 0.185450218573\n",
      "Iteration 204, train_result 0.424487934794, test_result 0.185450218573\n",
      "Iteration 205, train_result 0.424487934794, test_result 0.185450218573\n",
      "Iteration 206, train_result 0.424487934794, test_result 0.185450218573\n",
      "Iteration 207, train_result 0.424487934794, test_result 0.185462922533\n",
      "Iteration 208, train_result 0.424487934794, test_result 0.185480804575\n",
      "Iteration 209, train_result 0.424487934794, test_result 0.185480804575\n",
      "Iteration 210, train_result 0.424487934794, test_result 0.185514989735\n",
      "Iteration 211, train_result 0.424487934794, test_result 0.185514989735\n",
      "Iteration 212, train_result 0.424487934794, test_result 0.185480804575\n",
      "Iteration 213, train_result 0.424487934794, test_result 0.185480804575\n",
      "Iteration 214, train_result 0.424487934794, test_result 0.185416033413\n",
      "Iteration 215, train_result 0.424487934794, test_result 0.185480804575\n",
      "Iteration 216, train_result 0.424487934794, test_result 0.185454100022\n",
      "Iteration 217, train_result 0.424487934794, test_result 0.185480804575\n",
      "Iteration 218, train_result 0.424487934794, test_result 0.185480804575\n",
      "Iteration 219, train_result 0.424487934794, test_result 0.185415309624\n",
      "Iteration 220, train_result 0.424487934794, test_result 0.184277092871\n",
      "Iteration 221, train_result 0.424487934794, test_result 0.18447276845\n",
      "Iteration 222, train_result 0.424487934794, test_result 0.184451195403\n",
      "Iteration 223, train_result 0.424487934794, test_result 0.184451195403\n",
      "Iteration 224, train_result 0.424487934794, test_result 0.184451195403\n",
      "Iteration 225, train_result 0.424487934794, test_result 0.18449699553\n",
      "Iteration 226, train_result 0.424487934794, test_result 0.18449699553\n",
      "Iteration 227, train_result 0.424487934794, test_result 0.185635212283\n",
      "Iteration 228, train_result 0.424487934794, test_result 0.185635212283\n",
      "Iteration 229, train_result 0.424487934794, test_result 0.18560850773\n",
      "Iteration 230, train_result 0.424487934794, test_result 0.18560850773\n",
      "Iteration 231, train_result 0.424487934794, test_result 0.185562707603\n",
      "Iteration 232, train_result 0.424487934794, test_result 0.186378381939\n",
      "Iteration 233, train_result 0.424487934794, test_result 0.186378381939\n",
      "Iteration 234, train_result 0.424487934794, test_result 0.18629203773\n",
      "Iteration 235, train_result 0.424487934794, test_result 0.185245296692\n",
      "Iteration 236, train_result 0.424487934794, test_result 0.185218592139\n",
      "Iteration 237, train_result 0.424487934794, test_result 0.185127116424\n",
      "Iteration 238, train_result 0.424487934794, test_result 0.18518052553\n",
      "Iteration 239, train_result 0.424487934794, test_result 0.18514634037\n",
      "Iteration 240, train_result 0.424487934794, test_result 0.18514634037\n",
      "Iteration 241, train_result 0.424487934794, test_result 0.18518052553\n",
      "Iteration 242, train_result 0.424487934794, test_result 0.185578350097\n",
      "Iteration 243, train_result 0.424487934794, test_result 0.185382674519\n",
      "Iteration 244, train_result 0.424487934794, test_result 0.185382674519\n",
      "Iteration 245, train_result 0.424487934794, test_result 0.185382674519\n",
      "Iteration 246, train_result 0.424487934794, test_result 0.185486278965\n",
      "Iteration 247, train_result 0.424487934794, test_result 0.185486278965\n",
      "Iteration 248, train_result 0.424487934794, test_result 0.185520464125\n",
      "Iteration 249, train_result 0.424487934794, test_result 0.185459574412\n",
      "Iteration 250, train_result 0.424487934794, test_result 0.185606808333\n",
      "Iteration 251, train_result 0.424487934794, test_result 0.18558010378\n",
      "Iteration 252, train_result 0.424487934794, test_result 0.18558010378\n",
      "Iteration 253, train_result 0.424487934794, test_result 0.185488628065\n",
      "Iteration 254, train_result 0.424487934794, test_result 0.185684303643\n",
      "Iteration 255, train_result 0.424487934794, test_result 0.185711008197\n",
      "Iteration 256, train_result 0.424487934794, test_result 0.18558010378\n",
      "Iteration 257, train_result 0.424487934794, test_result 0.185476499334\n",
      "Iteration 258, train_result 0.424487934794, test_result 0.185476499334\n",
      "Iteration 259, train_result 0.424487934794, test_result 0.185044489608\n",
      "Iteration 260, train_result 0.424487934794, test_result 0.185044489608\n",
      "Iteration 261, train_result 0.424487934794, test_result 0.185071194161\n",
      "Iteration 262, train_result 0.424487934794, test_result 0.184417069077\n",
      "Iteration 263, train_result 0.424487934794, test_result 0.185476499334\n",
      "Iteration 264, train_result 0.424487934794, test_result 0.184408246566\n",
      "Iteration 265, train_result 0.424487934794, test_result 0.184434951119\n",
      "Iteration 266, train_result 0.424487934794, test_result 0.184374061406\n",
      "Iteration 267, train_result 0.424487934794, test_result 0.184596441538\n",
      "Iteration 268, train_result 0.424487934794, test_result 0.184130246651\n",
      "Iteration 269, train_result 0.424487934794, test_result 0.184297161513\n",
      "Iteration 270, train_result 0.424487934794, test_result 0.184400765959\n",
      "Iteration 271, train_result 0.424487934794, test_result 0.184262976353\n",
      "Iteration 272, train_result 0.424487934794, test_result 0.184393285353\n",
      "Iteration 273, train_result 0.424487934794, test_result 0.184289680907\n",
      "Iteration 274, train_result 0.424487934794, test_result 0.184289680907\n",
      "Iteration 275, train_result 0.424487934794, test_result 0.18431638546\n",
      "Iteration 276, train_result 0.424487934794, test_result 0.184289680907\n",
      "Iteration 277, train_result 0.424487934794, test_result 0.184323866066\n",
      "Iteration 278, train_result 0.424487934794, test_result 0.18435057062\n",
      "Iteration 279, train_result 0.424487934794, test_result 0.184323866066\n",
      "Iteration 280, train_result 0.424487934794, test_result 0.183865151787\n",
      "Iteration 281, train_result 0.424487934794, test_result 0.184297161513\n",
      "Iteration 282, train_result 0.424487934794, test_result 0.184362656464\n",
      "Iteration 283, train_result 0.424487934794, test_result 0.184297161513\n",
      "Iteration 284, train_result 0.424487934794, test_result 0.18427045696\n",
      "Iteration 285, train_result 0.424487934794, test_result 0.184252574918\n",
      "Iteration 286, train_result 0.424487934794, test_result 0.184279279471\n",
      "Iteration 287, train_result 0.424487934794, test_result 0.184313464631\n",
      "Iteration 288, train_result 0.424487934794, test_result 0.184313464631\n",
      "Iteration 289, train_result 0.424487934794, test_result 0.184313464631\n",
      "Iteration 290, train_result 0.424487934794, test_result 0.184338753658\n",
      "Iteration 291, train_result 0.424487934794, test_result 0.184338753658\n",
      "Iteration 292, train_result 0.424487934794, test_result 0.184365458211\n",
      "Iteration 293, train_result 0.424487934794, test_result 0.184399643371\n",
      "Iteration 294, train_result 0.424487934794, test_result 0.184365458211\n",
      "Iteration 295, train_result 0.424487934794, test_result 0.184372938818\n",
      "Iteration 296, train_result 0.424487934794, test_result 0.184438433769\n",
      "Iteration 297, train_result 0.424487934794, test_result 0.184438433769\n",
      "Iteration 298, train_result 0.424487934794, test_result 0.184438433769\n",
      "Iteration 299, train_result 0.424487934794, test_result 0.184438433769\n",
      "Wall time: 10h 8min 4s\n"
     ]
    }
   ],
   "source": [
    "%time clf2 = clf2.fit(X_train, y_train, q_train, queries_train, X_test, y_test, q_test, queries_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 300, train_result 0.424487934794, test_result 0.184438433769\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.LambdaMART instance at 0x000000000863FF88>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf2.continue_fit(301, X_train, y_train, q_train, queries_train, X_test, y_test, q_test, queries_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, train_result 0.0575187075567, test_result 0.0\n",
      "Iteration 1, train_result 0.412304768093, test_result 0.145115627204\n",
      "Iteration 2, train_result 0.421008818461, test_result 0.156843561133\n",
      "Iteration 3, train_result 0.42136294864, test_result 0.150048394382\n",
      "Iteration 4, train_result 0.421471283526, test_result 0.151543081687\n",
      "Iteration 5, train_result 0.421471283526, test_result 0.160954537966\n",
      "Iteration 6, train_result 0.421305227069, test_result 0.15883096458\n",
      "Iteration 7, train_result 0.421305227069, test_result 0.163381553307\n",
      "Iteration 8, train_result 0.421305227069, test_result 0.165640210481\n",
      "Iteration 9, train_result 0.421305227069, test_result 0.170145311329\n",
      "Iteration 10, train_result 0.421305227069, test_result 0.17263287084\n",
      "Iteration 11, train_result 0.421305227069, test_result 0.170039048498\n",
      "Iteration 12, train_result 0.421471283526, test_result 0.171257269129\n",
      "Iteration 13, train_result 0.421471283526, test_result 0.172916894992\n",
      "Iteration 14, train_result 0.421628438464, test_result 0.171395186487\n",
      "Iteration 15, train_result 0.421628438464, test_result 0.171284789325\n",
      "Iteration 16, train_result 0.421674368333, test_result 0.171557195416\n",
      "Iteration 17, train_result 0.421674368333, test_result 0.169009752173\n",
      "Iteration 18, train_result 0.421674368333, test_result 0.168736042709\n",
      "Iteration 19, train_result 0.421674368333, test_result 0.167803444581\n",
      "Iteration 20, train_result 0.421674368333, test_result 0.168922649006\n",
      "Iteration 21, train_result 0.421674368333, test_result 0.169629952627\n",
      "Iteration 22, train_result 0.421674368333, test_result 0.169751338693\n",
      "Iteration 23, train_result 0.421517213394, test_result 0.1697949066\n",
      "Iteration 24, train_result 0.421517213394, test_result 0.170836977949\n",
      "Iteration 25, train_result 0.421517213394, test_result 0.170650539622\n",
      "Iteration 26, train_result 0.421517213394, test_result 0.171647297823\n",
      "Iteration 27, train_result 0.421694664904, test_result 0.171367483981\n",
      "Iteration 28, train_result 0.421694664904, test_result 0.170797786023\n",
      "Iteration 29, train_result 0.421694664904, test_result 0.170781945603\n",
      "Iteration 30, train_result 0.421694664904, test_result 0.172046275944\n",
      "Iteration 31, train_result 0.421694664904, test_result 0.172828476142\n",
      "Iteration 32, train_result 0.421838668146, test_result 0.173010902307\n",
      "Iteration 33, train_result 0.421661216636, test_result 0.172637500512\n",
      "Iteration 34, train_result 0.421661216636, test_result 0.171903081574\n",
      "Iteration 35, train_result 0.422370916154, test_result 0.171553644967\n",
      "Iteration 36, train_result 0.422206678677, test_result 0.171961233553\n",
      "Iteration 37, train_result 0.422206678677, test_result 0.171864501284\n",
      "Iteration 38, train_result 0.422294315454, test_result 0.17073228193\n",
      "Iteration 39, train_result 0.422500185369, test_result 0.172274063017\n",
      "Iteration 40, train_result 0.42298629082, test_result 0.172680100399\n",
      "Iteration 41, train_result 0.42298629082, test_result 0.173050006881\n",
      "Iteration 42, train_result 0.42298629082, test_result 0.173417245445\n",
      "Iteration 43, train_result 0.422606885236, test_result 0.172649540844\n",
      "Iteration 44, train_result 0.422456921313, test_result 0.172233545384\n",
      "Iteration 45, train_result 0.422804575893, test_result 0.172805090885\n",
      "Iteration 46, train_result 0.422889577333, test_result 0.172909194209\n",
      "Iteration 47, train_result 0.422889577333, test_result 0.172827396141\n",
      "Iteration 48, train_result 0.423199637031, test_result 0.172729528811\n",
      "Iteration 49, train_result 0.423199637031, test_result 0.173487001638\n",
      "Iteration 50, train_result 0.423199637031, test_result 0.173114493143\n",
      "Iteration 51, train_result 0.423199637031, test_result 0.172670008594\n",
      "Iteration 52, train_result 0.423055633789, test_result 0.172166569295\n",
      "Iteration 53, train_result 0.423055633789, test_result 0.172139864742\n",
      "Iteration 54, train_result 0.423055633789, test_result 0.172148687252\n",
      "Iteration 55, train_result 0.423055633789, test_result 0.172464243947\n",
      "Iteration 56, train_result 0.423055633789, test_result 0.172138924061\n",
      "Iteration 57, train_result 0.423055633789, test_result 0.172590628611\n",
      "Iteration 58, train_result 0.423055633789, test_result 0.171726609158\n",
      "Iteration 59, train_result 0.423070900498, test_result 0.172436306432\n",
      "Iteration 60, train_result 0.423070900498, test_result 0.172362666183\n",
      "Iteration 61, train_result 0.423070900498, test_result 0.172748875781\n",
      "Iteration 62, train_result 0.423385210376, test_result 0.173180885508\n",
      "Iteration 63, train_result 0.423385210376, test_result 0.173199555875\n",
      "Iteration 64, train_result 0.423743763487, test_result 0.173165370715\n",
      "Iteration 65, train_result 0.423743763487, test_result 0.173293824047\n",
      "Iteration 66, train_result 0.423743763487, test_result 0.17214725586\n",
      "Iteration 67, train_result 0.423743763487, test_result 0.172547955796\n",
      "Iteration 68, train_result 0.423936481706, test_result 0.17296992948\n",
      "Iteration 69, train_result 0.423936481706, test_result 0.172647402051\n",
      "Iteration 70, train_result 0.423936481706, test_result 0.172536830762\n",
      "Iteration 71, train_result 0.423599760245, test_result 0.172999349384\n",
      "Iteration 72, train_result 0.423995170453, test_result 0.17353927419\n",
      "Iteration 73, train_result 0.423995170453, test_result 0.173955992685\n",
      "Iteration 74, train_result 0.423995170453, test_result 0.173955992685\n",
      "Iteration 75, train_result 0.424244255138, test_result 0.173955992685\n",
      "Iteration 76, train_result 0.424244255138, test_result 0.174255272709\n",
      "Iteration 77, train_result 0.424035665949, test_result 0.173832085493\n",
      "Iteration 78, train_result 0.424035665949, test_result 0.173938775334\n",
      "Iteration 79, train_result 0.424035665949, test_result 0.17391207078\n",
      "Iteration 80, train_result 0.424035665949, test_result 0.173431670116\n",
      "Iteration 81, train_result 0.424035665949, test_result 0.17254976862\n",
      "Iteration 82, train_result 0.424500928887, test_result 0.172681762028\n",
      "Iteration 83, train_result 0.424715478756, test_result 0.173220461595\n",
      "Iteration 84, train_result 0.424715478756, test_result 0.172661642861\n",
      "Iteration 85, train_result 0.425129548642, test_result 0.17304785246\n",
      "Iteration 86, train_result 0.425129548642, test_result 0.174407209706\n",
      "Iteration 87, train_result 0.425164083457, test_result 0.173088522131\n",
      "Iteration 88, train_result 0.425413168142, test_result 0.17380735702\n",
      "Iteration 89, train_result 0.425098858264, test_result 0.172529687619\n",
      "Iteration 90, train_result 0.425098858264, test_result 0.172575487747\n",
      "Iteration 91, train_result 0.425098858264, test_result 0.172575487747\n",
      "Iteration 92, train_result 0.425098858264, test_result 0.172640982698\n",
      "Iteration 93, train_result 0.425264914721, test_result 0.172575487747\n",
      "Iteration 94, train_result 0.425264914721, test_result 0.172727727482\n",
      "Iteration 95, train_result 0.425098858264, test_result 0.172003944785\n",
      "Iteration 96, train_result 0.425098858264, test_result 0.171306547256\n",
      "Iteration 97, train_result 0.425313408134, test_result 0.171306547256\n",
      "Iteration 98, train_result 0.425692943204, test_result 0.170217783388\n",
      "Iteration 99, train_result 0.425692943204, test_result 0.170273541594\n",
      "Iteration 100, train_result 0.425692943204, test_result 0.170239356435\n",
      "Iteration 101, train_result 0.425378633326, test_result 0.170273541594\n",
      "Iteration 102, train_result 0.425378633326, test_result 0.170462169289\n",
      "Iteration 103, train_result 0.425400464977, test_result 0.170462169289\n",
      "Iteration 104, train_result 0.425590398088, test_result 0.170462169289\n",
      "Iteration 105, train_result 0.425590398088, test_result 0.170462169289\n",
      "Iteration 106, train_result 0.425117944134, test_result 0.170488873842\n",
      "Iteration 107, train_result 0.425134036667, test_result 0.170463584815\n",
      "Iteration 108, train_result 0.425134036667, test_result 0.170463584815\n",
      "Iteration 109, train_result 0.425134036667, test_result 0.170352289736\n",
      "Iteration 110, train_result 0.42481972679, test_result 0.170390609257\n",
      "Iteration 111, train_result 0.424653670334, test_result 0.170356424097\n",
      "Iteration 112, train_result 0.424862259522, test_result 0.170095253568\n",
      "Iteration 113, train_result 0.424862259522, test_result 0.170113135611\n",
      "Iteration 114, train_result 0.424862259522, test_result 0.170532518484\n",
      "Iteration 115, train_result 0.425019414461, test_result 0.170532518484\n",
      "Iteration 116, train_result 0.425019414461, test_result 0.170489320369\n",
      "Iteration 117, train_result 0.424396996584, test_result 0.169822608148\n",
      "Iteration 118, train_result 0.424207063473, test_result 0.169822608148\n",
      "Iteration 119, train_result 0.424207063473, test_result 0.169822608148\n",
      "Iteration 120, train_result 0.424272288666, test_result 0.169822608148\n",
      "Iteration 121, train_result 0.424272288666, test_result 0.169788422988\n",
      "Iteration 122, train_result 0.424272288666, test_result 0.169891474719\n",
      "Iteration 123, train_result 0.424272288666, test_result 0.170049593517\n",
      "Iteration 124, train_result 0.424057738797, test_result 0.169891474719\n",
      "Iteration 125, train_result 0.424057738797, test_result 0.170049593517\n",
      "Iteration 126, train_result 0.424250457016, test_result 0.170115088468\n",
      "Iteration 127, train_result 0.424250457016, test_result 0.170675991668\n",
      "Iteration 128, train_result 0.424250457016, test_result 0.170675991668\n",
      "Iteration 129, train_result 0.424235190306, test_result 0.171286945683\n",
      "Iteration 130, train_result 0.424235190306, test_result 0.171286945683\n",
      "Iteration 131, train_result 0.424235190306, test_result 0.171482621261\n",
      "Iteration 132, train_result 0.424235190306, test_result 0.170837053913\n",
      "Iteration 133, train_result 0.424235190306, test_result 0.171486312265\n",
      "Iteration 134, train_result 0.424235190306, test_result 0.171936204034\n",
      "Iteration 135, train_result 0.424235190306, test_result 0.171936204034\n",
      "Iteration 136, train_result 0.424211022079, test_result 0.171936204034\n",
      "Iteration 137, train_result 0.424042472087, test_result 0.172001698985\n",
      "Iteration 138, train_result 0.424042472087, test_result 0.172467893872\n",
      "Iteration 139, train_result 0.424042472087, test_result 0.173238571393\n",
      "Iteration 140, train_result 0.424042472087, test_result 0.173692154166\n",
      "Iteration 141, train_result 0.424042472087, test_result 0.174075818776\n",
      "Iteration 142, train_result 0.424042472087, test_result 0.173661691092\n",
      "Iteration 143, train_result 0.424235190306, test_result 0.173636328443\n",
      "Iteration 144, train_result 0.424392345245, test_result 0.173654210485\n",
      "Iteration 145, train_result 0.424392345245, test_result 0.173654210485\n",
      "Iteration 146, train_result 0.424392345245, test_result 0.173654210485\n",
      "Iteration 147, train_result 0.424542309168, test_result 0.173654210485\n",
      "Iteration 148, train_result 0.424327120052, test_result 0.173654210485\n",
      "Iteration 149, train_result 0.424319929037, test_result 0.17424433901\n",
      "Iteration 150, train_result 0.42446989296, test_result 0.17424433901\n",
      "Iteration 151, train_result 0.42446989296, test_result 0.174672767956\n",
      "Iteration 152, train_result 0.42446989296, test_result 0.174672767956\n",
      "Iteration 153, train_result 0.424319929037, test_result 0.174699472509\n",
      "Iteration 154, train_result 0.424319929037, test_result 0.174699472509\n",
      "Iteration 155, train_result 0.424319929037, test_result 0.174763154679\n",
      "Iteration 156, train_result 0.424319929037, test_result 0.174763154679\n",
      "Iteration 157, train_result 0.424319929037, test_result 0.174763154679\n",
      "Iteration 158, train_result 0.424319929037, test_result 0.174763154679\n",
      "Iteration 159, train_result 0.424319929037, test_result 0.174763154679\n",
      "Iteration 160, train_result 0.424319929037, test_result 0.174763154679\n",
      "Iteration 161, train_result 0.42446989296, test_result 0.174763154679\n",
      "Iteration 162, train_result 0.424319929037, test_result 0.17465258339\n",
      "Iteration 163, train_result 0.42446989296, test_result 0.174763154679\n",
      "Iteration 164, train_result 0.42446989296, test_result 0.175084593116\n",
      "Iteration 165, train_result 0.424155583082, test_result 0.175057888563\n",
      "Iteration 166, train_result 0.424155583082, test_result 0.174975358286\n",
      "Iteration 167, train_result 0.424155583082, test_result 0.174975358286\n",
      "Iteration 168, train_result 0.424155583082, test_result 0.174953785239\n",
      "Iteration 169, train_result 0.424897033604, test_result 0.174964765838\n",
      "Iteration 170, train_result 0.424897033604, test_result 0.174925631886\n",
      "Iteration 171, train_result 0.424897033604, test_result 0.174860860724\n",
      "Iteration 172, train_result 0.424897033604, test_result 0.174860860724\n",
      "Iteration 173, train_result 0.424897033604, test_result 0.174860860724\n",
      "Iteration 174, train_result 0.424897033604, test_result 0.174860860724\n",
      "Iteration 175, train_result 0.424897033604, test_result 0.174906660852\n",
      "Iteration 176, train_result 0.424897033604, test_result 0.174906660852\n",
      "Iteration 177, train_result 0.424897033604, test_result 0.174906660852\n",
      "Iteration 178, train_result 0.424897033604, test_result 0.174906660852\n",
      "Iteration 179, train_result 0.424897033604, test_result 0.174885087805\n",
      "Iteration 180, train_result 0.424897033604, test_result 0.174474651125\n",
      "Iteration 181, train_result 0.424897033604, test_result 0.174474651125\n",
      "Iteration 182, train_result 0.424897033604, test_result 0.174526718328\n",
      "Iteration 183, train_result 0.424897033604, test_result 0.174526718328\n",
      "Iteration 184, train_result 0.424897033604, test_result 0.174461223377\n",
      "Iteration 185, train_result 0.424897033604, test_result 0.174461223377\n",
      "Iteration 186, train_result 0.424897033604, test_result 0.174526718328\n",
      "Iteration 187, train_result 0.424897033604, test_result 0.174526718328\n",
      "Iteration 188, train_result 0.424897033604, test_result 0.174500013775\n",
      "Iteration 189, train_result 0.424897033604, test_result 0.174565508726\n",
      "Iteration 190, train_result 0.424704315385, test_result 0.174565508726\n",
      "Iteration 191, train_result 0.424720407918, test_result 0.174565508726\n",
      "Iteration 192, train_result 0.424720407918, test_result 0.174565508726\n",
      "Iteration 193, train_result 0.424720407918, test_result 0.174565508726\n",
      "Iteration 194, train_result 0.424720407918, test_result 0.174565508726\n",
      "Iteration 195, train_result 0.424720407918, test_result 0.174703298332\n",
      "Iteration 196, train_result 0.424720407918, test_result 0.174869642568\n",
      "Iteration 197, train_result 0.424720407918, test_result 0.174835457408\n",
      "Iteration 198, train_result 0.424720407918, test_result 0.174835457408\n",
      "Iteration 199, train_result 0.424720407918, test_result 0.174835457408\n",
      "Iteration 200, train_result 0.424720407918, test_result 0.174835457408\n",
      "Iteration 201, train_result 0.424720407918, test_result 0.174835457408\n",
      "Iteration 202, train_result 0.424720407918, test_result 0.175387035883\n",
      "Iteration 203, train_result 0.424720407918, test_result 0.17546956616\n",
      "Iteration 204, train_result 0.424720407918, test_result 0.17546956616\n",
      "Iteration 205, train_result 0.424720407918, test_result 0.17546956616\n",
      "Iteration 206, train_result 0.42488895791, test_result 0.175423766032\n",
      "Iteration 207, train_result 0.42488895791, test_result 0.175423766032\n",
      "Iteration 208, train_result 0.42488895791, test_result 0.175423766032\n",
      "Iteration 209, train_result 0.42488895791, test_result 0.174991756306\n",
      "Iteration 210, train_result 0.424913126137, test_result 0.174991756306\n",
      "Iteration 211, train_result 0.424913126137, test_result 0.174891411503\n",
      "Iteration 212, train_result 0.424913126137, test_result 0.174925596663\n",
      "Iteration 213, train_result 0.424913126137, test_result 0.174925596663\n",
      "Iteration 214, train_result 0.424913126137, test_result 0.174925596663\n",
      "Iteration 215, train_result 0.424913126137, test_result 0.174925596663\n",
      "Iteration 216, train_result 0.424913126137, test_result 0.174904023616\n",
      "Iteration 217, train_result 0.424913126137, test_result 0.174904023616\n",
      "Iteration 218, train_result 0.424913126137, test_result 0.174904023616\n",
      "Iteration 219, train_result 0.424913126137, test_result 0.174904023616\n",
      "Iteration 220, train_result 0.424913126137, test_result 0.174904023616\n",
      "Iteration 221, train_result 0.424913126137, test_result 0.174904023616\n",
      "Iteration 222, train_result 0.424913126137, test_result 0.174904023616\n",
      "Iteration 223, train_result 0.424913126137, test_result 0.174904023616\n",
      "Iteration 224, train_result 0.424913126137, test_result 0.174882450569\n",
      "Iteration 225, train_result 0.424664041452, test_result 0.17484826541\n",
      "Iteration 226, train_result 0.424763801461, test_result 0.174882450569\n",
      "Iteration 227, train_result 0.424763801461, test_result 0.174416255683\n",
      "Iteration 228, train_result 0.424763801461, test_result 0.173984245956\n",
      "Iteration 229, train_result 0.424763801461, test_result 0.173984245956\n",
      "Iteration 230, train_result 0.424763801461, test_result 0.173984245956\n",
      "Iteration 231, train_result 0.424763801461, test_result 0.173984245956\n",
      "Iteration 232, train_result 0.424571083242, test_result 0.174045135669\n",
      "Iteration 233, train_result 0.424571083242, test_result 0.174090935797\n",
      "Iteration 234, train_result 0.424571083242, test_result 0.17345327723\n",
      "Iteration 235, train_result 0.424571083242, test_result 0.173021267503\n",
      "Iteration 236, train_result 0.424571083242, test_result 0.173021267503\n",
      "Iteration 237, train_result 0.424571083242, test_result 0.173021267503\n",
      "Iteration 238, train_result 0.424571083242, test_result 0.173086762454\n",
      "Iteration 239, train_result 0.424571083242, test_result 0.173086762454\n",
      "Iteration 240, train_result 0.424571083242, test_result 0.173086762454\n",
      "Iteration 241, train_result 0.424571083242, test_result 0.173086762454\n",
      "Iteration 242, train_result 0.424571083242, test_result 0.173142520661\n",
      "Iteration 243, train_result 0.424571083242, test_result 0.173142520661\n",
      "Iteration 244, train_result 0.424571083242, test_result 0.173142520661\n",
      "Iteration 245, train_result 0.424571083242, test_result 0.172710510934\n",
      "Iteration 246, train_result 0.424571083242, test_result 0.172710510934\n",
      "Iteration 247, train_result 0.425314492857, test_result 0.172710510934\n",
      "Iteration 248, train_result 0.425321683873, test_result 0.172710510934\n",
      "Iteration 249, train_result 0.425321683873, test_result 0.172710510934\n",
      "Iteration 250, train_result 0.425321683873, test_result 0.172710510934\n",
      "Iteration 251, train_result 0.425321683873, test_result 0.172710510934\n",
      "Iteration 252, train_result 0.425321683873, test_result 0.172710510934\n",
      "Iteration 253, train_result 0.425321683873, test_result 0.172710510934\n",
      "Iteration 254, train_result 0.425471647796, test_result 0.172710510934\n",
      "Iteration 255, train_result 0.425471647796, test_result 0.172775282096\n",
      "Iteration 256, train_result 0.425471647796, test_result 0.172741096936\n",
      "Iteration 257, train_result 0.42530559134, test_result 0.172741096936\n",
      "Iteration 258, train_result 0.42530559134, test_result 0.173173106663\n",
      "Iteration 259, train_result 0.42530559134, test_result 0.173138921503\n",
      "Iteration 260, train_result 0.42530559134, test_result 0.173173106663\n",
      "Iteration 261, train_result 0.42530559134, test_result 0.173061811584\n",
      "Iteration 262, train_result 0.425083850455, test_result 0.173061811584\n",
      "Iteration 263, train_result 0.424342399934, test_result 0.172996316633\n",
      "Iteration 264, train_result 0.424342399934, test_result 0.17307884691\n",
      "Iteration 265, train_result 0.424342399934, test_result 0.17307884691\n",
      "Iteration 266, train_result 0.424342399934, test_result 0.173494125346\n",
      "Iteration 267, train_result 0.42440698588, test_result 0.173528310506\n",
      "Iteration 268, train_result 0.424564140819, test_result 0.173580392448\n",
      "Iteration 269, train_result 0.424579407528, test_result 0.173580392448\n",
      "Iteration 270, train_result 0.424579407528, test_result 0.173580392448\n",
      "Iteration 271, train_result 0.424579407528, test_result 0.173580392448\n",
      "Iteration 272, train_result 0.424579407528, test_result 0.173028813973\n",
      "Iteration 273, train_result 0.424579407528, test_result 0.173105200102\n",
      "Iteration 274, train_result 0.424514182335, test_result 0.172682012886\n",
      "Iteration 275, train_result 0.424514182335, test_result 0.173092449566\n",
      "Iteration 276, train_result 0.424514182335, test_result 0.173092449566\n",
      "Iteration 277, train_result 0.424514182335, test_result 0.173092449566\n",
      "Iteration 278, train_result 0.424514182335, test_result 0.173058264406\n",
      "Iteration 279, train_result 0.424514182335, test_result 0.173076146449\n",
      "Iteration 280, train_result 0.424514182335, test_result 0.173326269707\n",
      "Iteration 281, train_result 0.424514182335, test_result 0.173758279434\n",
      "Iteration 282, train_result 0.424514182335, test_result 0.173758279434\n",
      "Iteration 283, train_result 0.424514182335, test_result 0.173758279434\n",
      "Iteration 284, train_result 0.424514182335, test_result 0.173758279434\n",
      "Iteration 285, train_result 0.424514182335, test_result 0.173758279434\n",
      "Iteration 286, train_result 0.424514182335, test_result 0.173758279434\n",
      "Iteration 287, train_result 0.424571577266, test_result 0.173758279434\n",
      "Iteration 288, train_result 0.424571577266, test_result 0.173758279434\n",
      "Iteration 289, train_result 0.424571577266, test_result 0.173779852481\n",
      "Iteration 290, train_result 0.424571577266, test_result 0.173779852481\n",
      "Iteration 291, train_result 0.424571577266, test_result 0.173779852481\n",
      "Iteration 292, train_result 0.424571577266, test_result 0.173228274005\n",
      "Iteration 293, train_result 0.424571577266, test_result 0.173228274005\n",
      "Iteration 294, train_result 0.424606112081, test_result 0.173228274005\n",
      "Iteration 295, train_result 0.424606112081, test_result 0.173228274005\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-e53b92d3b74c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmagic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mu'time clf1 = clf1.fit(X_train, y_train, q_train, queries_train, X_test, y_test, q_test, queries_test)'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Users\\Kirill\\Anaconda2\\lib\\site-packages\\IPython\\core\\interactiveshell.pyc\u001b[0m in \u001b[0;36mmagic\u001b[1;34m(self, arg_s)\u001b[0m\n\u001b[0;32m   2161\u001b[0m         \u001b[0mmagic_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmagic_arg_s\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marg_s\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpartition\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m' '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2162\u001b[0m         \u001b[0mmagic_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmagic_name\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprefilter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mESC_MAGIC\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2163\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmagic_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmagic_arg_s\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2164\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2165\u001b[0m     \u001b[1;31m#-------------------------------------------------------------------------\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Kirill\\Anaconda2\\lib\\site-packages\\IPython\\core\\interactiveshell.pyc\u001b[0m in \u001b[0;36mrun_line_magic\u001b[1;34m(self, magic_name, line)\u001b[0m\n\u001b[0;32m   2082\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'local_ns'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getframe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstack_depth\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf_locals\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2083\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2084\u001b[1;33m                 \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2085\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2086\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<decorator-gen-61>\u001b[0m in \u001b[0;36mtime\u001b[1;34m(self, line, cell, local_ns)\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Kirill\\Anaconda2\\lib\\site-packages\\IPython\\core\\magic.pyc\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(f, *a, **k)\u001b[0m\n\u001b[0;32m    191\u001b[0m     \u001b[1;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    192\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 193\u001b[1;33m         \u001b[0mcall\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    194\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Kirill\\Anaconda2\\lib\\site-packages\\IPython\\core\\magics\\execution.pyc\u001b[0m in \u001b[0;36mtime\u001b[1;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[0;32m   1175\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1176\u001b[0m             \u001b[0mst\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1177\u001b[1;33m             \u001b[1;32mexec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlocal_ns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1178\u001b[0m             \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1179\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-0dde548ff640>\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X_train, y_train, qids_train, queries_train, X_test, y_test, qids_test, queries_test, verbose)\u001b[0m\n\u001b[0;32m     41\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mxrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_estimators\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m             \u001b[0mpredicted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mqids_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mqueries_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 43\u001b[1;33m             \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mqids_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mqueries_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     44\u001b[0m             \u001b[0mtrain_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mndcgl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredicted\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mqids_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m             \u001b[0mtest_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mndcgl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mqids_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-0dde548ff640>\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X, qids, queries, verbose)\u001b[0m\n\u001b[0;32m    112\u001b[0m                 \u001b[0mstep\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1.\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1.\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madaptive_step\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 114\u001b[1;33m                     \u001b[0mstep\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep_estimators\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mest_i\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mqueries\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    115\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m                     \u001b[1;32mprint\u001b[0m \u001b[1;34m\"Step: \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\" for qid=\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mqids\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Kirill\\Anaconda2\\lib\\site-packages\\sklearn\\tree\\tree.pyc\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X, check_input)\u001b[0m\n\u001b[0;32m    429\u001b[0m         \"\"\"\n\u001b[0;32m    430\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 431\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_X_predict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    432\u001b[0m         \u001b[0mproba\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    433\u001b[0m         \u001b[0mn_samples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Kirill\\Anaconda2\\lib\\site-packages\\sklearn\\tree\\tree.pyc\u001b[0m in \u001b[0;36m_validate_X_predict\u001b[1;34m(self, X, check_input)\u001b[0m\n\u001b[0;32m    390\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    391\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 392\u001b[1;33m             \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mDTYPE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"csr\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    393\u001b[0m             if issparse(X) and (X.indices.dtype != np.intc or\n\u001b[0;32m    394\u001b[0m                                 X.indptr.dtype != np.intc):\n",
      "\u001b[1;32mC:\\Users\\Kirill\\Anaconda2\\lib\\site-packages\\sklearn\\utils\\validation.pyc\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[0;32m    358\u001b[0m             \u001b[0mdtype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    359\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 360\u001b[1;33m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    361\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdtype_orig\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mdtype_orig\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    362\u001b[0m             \u001b[1;31m# no dtype conversion required\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%time clf1 = clf1.fit(X_train, y_train, q_train, queries_train, X_test, y_test, q_test, queries_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, train_result 0.0, test_result 0.217260712852\n",
      "Iteration 1, train_result 0.326793823339, test_result 0.167990895184\n",
      "Iteration 2, train_result 0.350218945077, test_result 0.168031252584\n",
      "Iteration 3, train_result 0.376366642672, test_result 0.169533245177\n",
      "Iteration 4, train_result 0.392124451835, test_result 0.169470632113\n",
      "Iteration 5, train_result 0.40035966417, test_result 0.166862935522\n",
      "Iteration 6, train_result 0.406648531108, test_result 0.168544369952\n",
      "Iteration 7, train_result 0.40941328885, test_result 0.169543598297\n",
      "Iteration 8, train_result 0.411843468875, test_result 0.173384550953\n",
      "Iteration 9, train_result 0.412434463142, test_result 0.173335317435\n",
      "Iteration 10, train_result 0.413233253238, test_result 0.175435423385\n",
      "Iteration 11, train_result 0.413649908937, test_result 0.176665511579\n",
      "Iteration 12, train_result 0.41357004004, test_result 0.175898041207\n",
      "Iteration 13, train_result 0.413889205266, test_result 0.175593811644\n",
      "Iteration 14, train_result 0.413968919486, test_result 0.177109531864\n",
      "Iteration 15, train_result 0.41425692597, test_result 0.176637494013\n",
      "Iteration 16, train_result 0.414414080909, test_result 0.172583395821\n",
      "Iteration 17, train_result 0.414406889893, test_result 0.175872007666\n",
      "Iteration 18, train_result 0.41425692597, test_result 0.177422410036\n",
      "Iteration 19, train_result 0.414406889893, test_result 0.173839002821\n",
      "Iteration 20, train_result 0.414550893136, test_result 0.176594856421\n",
      "Iteration 21, train_result 0.414550893136, test_result 0.176381916557\n",
      "Iteration 22, train_result 0.414550893136, test_result 0.174563738655\n",
      "Iteration 23, train_result 0.414550893136, test_result 0.174096698568\n",
      "Iteration 24, train_result 0.414550893136, test_result 0.176344374242\n",
      "Iteration 25, train_result 0.414550893136, test_result 0.173532305634\n",
      "Iteration 26, train_result 0.414550893136, test_result 0.176631246337\n",
      "Iteration 27, train_result 0.414550893136, test_result 0.176572102366\n",
      "Iteration 28, train_result 0.414550893136, test_result 0.176836589099\n",
      "Iteration 29, train_result 0.414550893136, test_result 0.176149109822\n",
      "Iteration 30, train_result 0.414550893136, test_result 0.176323986578\n",
      "Iteration 31, train_result 0.414550893136, test_result 0.176110108942\n",
      "Iteration 32, train_result 0.414708048074, test_result 0.178068612797\n",
      "Iteration 33, train_result 0.414708048074, test_result 0.177908412054\n",
      "Iteration 34, train_result 0.414708048074, test_result 0.177003267642\n",
      "Iteration 35, train_result 0.414708048074, test_result 0.175274931882\n",
      "Iteration 36, train_result 0.414708048074, test_result 0.174006310558\n",
      "Iteration 37, train_result 0.414708048074, test_result 0.174011248754\n",
      "Iteration 38, train_result 0.414708048074, test_result 0.176360424501\n",
      "Iteration 39, train_result 0.414708048074, test_result 0.175563484288\n",
      "Iteration 40, train_result 0.414708048074, test_result 0.175484023613\n",
      "Iteration 41, train_result 0.414708048074, test_result 0.175642225365\n",
      "Iteration 42, train_result 0.414708048074, test_result 0.175482839448\n",
      "Iteration 43, train_result 0.414708048074, test_result 0.176205199144\n",
      "Iteration 44, train_result 0.414708048074, test_result 0.175775442292\n",
      "Iteration 45, train_result 0.414708048074, test_result 0.176133855352\n",
      "Iteration 46, train_result 0.414708048074, test_result 0.176047592044\n",
      "Iteration 47, train_result 0.414708048074, test_result 0.175387277616\n",
      "Iteration 48, train_result 0.414708048074, test_result 0.175193396952\n",
      "Iteration 49, train_result 0.414708048074, test_result 0.177197889446\n",
      "Iteration 50, train_result 0.414708048074, test_result 0.17880462171\n",
      "Iteration 51, train_result 0.414708048074, test_result 0.179044021074\n",
      "Iteration 52, train_result 0.414708048074, test_result 0.178174059218\n",
      "Iteration 53, train_result 0.414708048074, test_result 0.178295112572\n",
      "Iteration 54, train_result 0.414708048074, test_result 0.176960217351\n",
      "Iteration 55, train_result 0.414708048074, test_result 0.177244702188\n",
      "Iteration 56, train_result 0.414708048074, test_result 0.177417657244\n",
      "Iteration 57, train_result 0.414708048074, test_result 0.177051142468\n",
      "Iteration 58, train_result 0.414708048074, test_result 0.178653395197\n",
      "Iteration 59, train_result 0.414708048074, test_result 0.175980201099\n",
      "Iteration 60, train_result 0.414708048074, test_result 0.175398895085\n",
      "Iteration 61, train_result 0.414708048074, test_result 0.174983601373\n",
      "Iteration 62, train_result 0.414708048074, test_result 0.173479011549\n",
      "Iteration 63, train_result 0.414708048074, test_result 0.174465302849\n",
      "Iteration 64, train_result 0.414708048074, test_result 0.174992618926\n",
      "Iteration 65, train_result 0.414708048074, test_result 0.175017249366\n",
      "Iteration 66, train_result 0.414708048074, test_result 0.17593710655\n",
      "Iteration 67, train_result 0.414708048074, test_result 0.176330325878\n",
      "Iteration 68, train_result 0.414708048074, test_result 0.175661111209\n",
      "Iteration 69, train_result 0.414708048074, test_result 0.175234232989\n",
      "Iteration 70, train_result 0.414708048074, test_result 0.175194777899\n",
      "Iteration 71, train_result 0.414708048074, test_result 0.176248315346\n",
      "Iteration 72, train_result 0.414708048074, test_result 0.178914403634\n",
      "Iteration 73, train_result 0.414708048074, test_result 0.17815722737\n",
      "Iteration 74, train_result 0.414708048074, test_result 0.178784912675\n",
      "Iteration 75, train_result 0.414708048074, test_result 0.178246645125\n",
      "Iteration 76, train_result 0.414708048074, test_result 0.17714386572\n",
      "Iteration 77, train_result 0.414708048074, test_result 0.177331008034\n",
      "Iteration 78, train_result 0.414708048074, test_result 0.177397177728\n",
      "Iteration 79, train_result 0.414708048074, test_result 0.178413436916\n",
      "Iteration 80, train_result 0.414708048074, test_result 0.177718916833\n",
      "Iteration 81, train_result 0.414708048074, test_result 0.176987156205\n",
      "Iteration 82, train_result 0.414708048074, test_result 0.177209321155\n",
      "Iteration 83, train_result 0.414708048074, test_result 0.174302014883\n",
      "Iteration 84, train_result 0.414708048074, test_result 0.174235787724\n",
      "Iteration 85, train_result 0.414708048074, test_result 0.17401746399\n",
      "Iteration 86, train_result 0.414708048074, test_result 0.176354467766\n",
      "Iteration 87, train_result 0.414708048074, test_result 0.176660549165\n",
      "Iteration 88, train_result 0.414708048074, test_result 0.17701176953\n",
      "Iteration 89, train_result 0.414708048074, test_result 0.177613610862\n",
      "Iteration 90, train_result 0.414708048074, test_result 0.177023482337\n",
      "Iteration 91, train_result 0.414708048074, test_result 0.176645986963\n",
      "Iteration 92, train_result 0.414708048074, test_result 0.176958668289\n",
      "Iteration 93, train_result 0.414708048074, test_result 0.177013995135\n",
      "Iteration 94, train_result 0.414708048074, test_result 0.176961913192\n",
      "Iteration 95, train_result 0.414708048074, test_result 0.177653804218\n",
      "Iteration 96, train_result 0.414708048074, test_result 0.177673541927\n",
      "Iteration 97, train_result 0.414708048074, test_result 0.177903441618\n",
      "Iteration 98, train_result 0.414708048074, test_result 0.177925014665\n",
      "Iteration 99, train_result 0.414708048074, test_result 0.17753217154\n",
      "Iteration 100, train_result 0.414708048074, test_result 0.175668634436\n",
      "Iteration 101, train_result 0.414708048074, test_result 0.175630868495\n",
      "Iteration 102, train_result 0.414708048074, test_result 0.175356695672\n",
      "Iteration 103, train_result 0.414708048074, test_result 0.176260074333\n",
      "Iteration 104, train_result 0.414708048074, test_result 0.176526866111\n",
      "Iteration 105, train_result 0.414708048074, test_result 0.176205427674\n",
      "Iteration 106, train_result 0.414708048074, test_result 0.176101299374\n",
      "Iteration 107, train_result 0.414708048074, test_result 0.175838223576\n",
      "Iteration 108, train_result 0.414708048074, test_result 0.175999540348\n",
      "Iteration 109, train_result 0.414708048074, test_result 0.175788028237\n",
      "Iteration 110, train_result 0.414708048074, test_result 0.176707873413\n",
      "Iteration 111, train_result 0.414708048074, test_result 0.176231277091\n",
      "Iteration 112, train_result 0.414708048074, test_result 0.178541086571\n",
      "Iteration 113, train_result 0.414708048074, test_result 0.178687733715\n",
      "Iteration 114, train_result 0.414708048074, test_result 0.176615624307\n",
      "Iteration 115, train_result 0.414708048074, test_result 0.178213429858\n",
      "Iteration 116, train_result 0.414708048074, test_result 0.177900813931\n",
      "Iteration 117, train_result 0.414708048074, test_result 0.177790242642\n",
      "Iteration 118, train_result 0.414708048074, test_result 0.177900813931\n",
      "Iteration 119, train_result 0.414708048074, test_result 0.175884500025\n",
      "Iteration 120, train_result 0.414708048074, test_result 0.177677001718\n",
      "Iteration 121, train_result 0.414708048074, test_result 0.175452424899\n",
      "Iteration 122, train_result 0.414708048074, test_result 0.178293639753\n",
      "Iteration 123, train_result 0.414708048074, test_result 0.178359134704\n",
      "Iteration 124, train_result 0.414708048074, test_result 0.178293639753\n",
      "Iteration 125, train_result 0.414708048074, test_result 0.178311521795\n",
      "Iteration 126, train_result 0.414708048074, test_result 0.179447174783\n",
      "Iteration 127, train_result 0.414708048074, test_result 0.179276335586\n",
      "Iteration 128, train_result 0.414708048074, test_result 0.180250926329\n",
      "Iteration 129, train_result 0.414708048074, test_result 0.181269041183\n",
      "Iteration 130, train_result 0.414708048074, test_result 0.181286923226\n",
      "Iteration 131, train_result 0.414708048074, test_result 0.181286923226\n",
      "Iteration 132, train_result 0.414708048074, test_result 0.180854913499\n",
      "Iteration 133, train_result 0.414708048074, test_result 0.179857064857\n",
      "Iteration 134, train_result 0.414708048074, test_result 0.180600748574\n",
      "Iteration 135, train_result 0.414708048074, test_result 0.177858439625\n",
      "Iteration 136, train_result 0.414708048074, test_result 0.177849528752\n",
      "Iteration 137, train_result 0.414708048074, test_result 0.177686415051\n",
      "Iteration 138, train_result 0.414708048074, test_result 0.177772137455\n",
      "Iteration 139, train_result 0.414708048074, test_result 0.177772137455\n",
      "Iteration 140, train_result 0.414708048074, test_result 0.177668533009\n",
      "Iteration 141, train_result 0.414708048074, test_result 0.177723792338\n",
      "Iteration 142, train_result 0.414708048074, test_result 0.176894386218\n",
      "Iteration 143, train_result 0.414708048074, test_result 0.176941275337\n",
      "Iteration 144, train_result 0.414708048074, test_result 0.176972585128\n",
      "Iteration 145, train_result 0.414708048074, test_result 0.176558334516\n",
      "Iteration 146, train_result 0.414708048074, test_result 0.17597763487\n",
      "Iteration 147, train_result 0.414708048074, test_result 0.176525473245\n",
      "Iteration 148, train_result 0.414708048074, test_result 0.176525473245\n",
      "Iteration 149, train_result 0.414708048074, test_result 0.176994158175\n",
      "Iteration 150, train_result 0.414708048074, test_result 0.176584123423\n",
      "Iteration 151, train_result 0.414708048074, test_result 0.176584123423\n",
      "Iteration 152, train_result 0.414708048074, test_result 0.176584123423\n",
      "Iteration 153, train_result 0.414708048074, test_result 0.176491499575\n",
      "Iteration 154, train_result 0.414708048074, test_result 0.176059489849\n",
      "Iteration 155, train_result 0.414708048074, test_result 0.175095710761\n",
      "Iteration 156, train_result 0.414708048074, test_result 0.175095710761\n",
      "Iteration 157, train_result 0.414708048074, test_result 0.175503952819\n",
      "Iteration 158, train_result 0.414708048074, test_result 0.175032488003\n",
      "Iteration 159, train_result 0.414708048074, test_result 0.176631736331\n",
      "Iteration 160, train_result 0.414708048074, test_result 0.175270132677\n",
      "Iteration 161, train_result 0.414708048074, test_result 0.174794951881\n",
      "Iteration 162, train_result 0.414708048074, test_result 0.175507579022\n",
      "Iteration 163, train_result 0.414708048074, test_result 0.174873723637\n",
      "Iteration 164, train_result 0.414708048074, test_result 0.175615330809\n",
      "Iteration 165, train_result 0.414708048074, test_result 0.175166854566\n",
      "Iteration 166, train_result 0.414708048074, test_result 0.175564679133\n",
      "Iteration 167, train_result 0.414708048074, test_result 0.175328344984\n",
      "Iteration 168, train_result 0.414708048074, test_result 0.175150551449\n",
      "Iteration 169, train_result 0.414708048074, test_result 0.175582561175\n",
      "Iteration 170, train_result 0.414708048074, test_result 0.175808699828\n",
      "Iteration 171, train_result 0.414708048074, test_result 0.17496140016\n",
      "Iteration 172, train_result 0.414708048074, test_result 0.174803281361\n",
      "Iteration 173, train_result 0.414708048074, test_result 0.174322926517\n",
      "Iteration 174, train_result 0.414708048074, test_result 0.174720751084\n",
      "Iteration 175, train_result 0.414708048074, test_result 0.174565569169\n",
      "Iteration 176, train_result 0.414708048074, test_result 0.175074478789\n",
      "Iteration 177, train_result 0.414708048074, test_result 0.174800587861\n",
      "Iteration 178, train_result 0.414708048074, test_result 0.174459592262\n",
      "Iteration 179, train_result 0.414708048074, test_result 0.174687613024\n",
      "Iteration 180, train_result 0.414708048074, test_result 0.174378311697\n",
      "Iteration 181, train_result 0.414708048074, test_result 0.17443039364\n",
      "Iteration 182, train_result 0.414708048074, test_result 0.174480786434\n",
      "Iteration 183, train_result 0.414708048074, test_result 0.174423572985\n",
      "Iteration 184, train_result 0.414708048074, test_result 0.173812618971\n",
      "Iteration 185, train_result 0.414708048074, test_result 0.174806517082\n",
      "Iteration 186, train_result 0.414708048074, test_result 0.175029062781\n",
      "Iteration 187, train_result 0.414708048074, test_result 0.174991023217\n",
      "Iteration 188, train_result 0.414708048074, test_result 0.173363545241\n",
      "Iteration 189, train_result 0.414708048074, test_result 0.174604000059\n",
      "Iteration 190, train_result 0.414708048074, test_result 0.17419727936\n",
      "Iteration 191, train_result 0.414708048074, test_result 0.174416011028\n",
      "Iteration 192, train_result 0.414708048074, test_result 0.174848020754\n",
      "Iteration 193, train_result 0.414708048074, test_result 0.174912491249\n",
      "Iteration 194, train_result 0.414708048074, test_result 0.173912698578\n",
      "Iteration 195, train_result 0.414708048074, test_result 0.175294108181\n",
      "Iteration 196, train_result 0.414708048074, test_result 0.175259122126\n",
      "Iteration 197, train_result 0.414708048074, test_result 0.174286034008\n",
      "Iteration 198, train_result 0.414708048074, test_result 0.174286034008\n",
      "Iteration 199, train_result 0.414708048074, test_result 0.174286034008\n",
      "Iteration 200, train_result 0.414708048074, test_result 0.174286034008\n",
      "Iteration 201, train_result 0.414708048074, test_result 0.175285826679\n",
      "Iteration 202, train_result 0.414708048074, test_result 0.175264527254\n",
      "Iteration 203, train_result 0.414708048074, test_result 0.175255541344\n",
      "Iteration 204, train_result 0.414708048074, test_result 0.175221356185\n",
      "Iteration 205, train_result 0.414708048074, test_result 0.17515670795\n",
      "Iteration 206, train_result 0.414708048074, test_result 0.17515670795\n",
      "Iteration 207, train_result 0.414708048074, test_result 0.175183412504\n",
      "Iteration 208, train_result 0.414708048074, test_result 0.174790857866\n",
      "Iteration 209, train_result 0.414708048074, test_result 0.174077610777\n",
      "Iteration 210, train_result 0.414708048074, test_result 0.17413850049\n",
      "Iteration 211, train_result 0.414708048074, test_result 0.174136743825\n",
      "Iteration 212, train_result 0.414708048074, test_result 0.174102558665\n",
      "Iteration 213, train_result 0.414708048074, test_result 0.173643844385\n",
      "Iteration 214, train_result 0.414708048074, test_result 0.174844926925\n",
      "Iteration 215, train_result 0.414708048074, test_result 0.172827369763\n",
      "Iteration 216, train_result 0.414708048074, test_result 0.173854030387\n",
      "Iteration 217, train_result 0.414708048074, test_result 0.172811066646\n",
      "Iteration 218, train_result 0.414708048074, test_result 0.173801948444\n",
      "Iteration 219, train_result 0.414708048074, test_result 0.173819845227\n",
      "Iteration 220, train_result 0.414708048074, test_result 0.173854030387\n",
      "Iteration 221, train_result 0.414708048074, test_result 0.173883010418\n",
      "Iteration 222, train_result 0.414708048074, test_result 0.173818539924\n",
      "Iteration 223, train_result 0.414708048074, test_result 0.173841418274\n",
      "Iteration 224, train_result 0.414708048074, test_result 0.174909671042\n",
      "Iteration 225, train_result 0.414708048074, test_result 0.174909671042\n",
      "Iteration 226, train_result 0.414708048074, test_result 0.174904809046\n",
      "Iteration 227, train_result 0.414708048074, test_result 0.174746690247\n",
      "Iteration 228, train_result 0.414708048074, test_result 0.174746690247\n",
      "Iteration 229, train_result 0.414708048074, test_result 0.174817779402\n",
      "Iteration 230, train_result 0.414708048074, test_result 0.174817779402\n",
      "Iteration 231, train_result 0.414708048074, test_result 0.174792490375\n",
      "Iteration 232, train_result 0.414708048074, test_result 0.174817779402\n",
      "Iteration 233, train_result 0.414708048074, test_result 0.174817779402\n",
      "Iteration 234, train_result 0.414708048074, test_result 0.174735249126\n",
      "Iteration 235, train_result 0.414708048074, test_result 0.174860426618\n",
      "Iteration 236, train_result 0.414708048074, test_result 0.174860426618\n",
      "Iteration 237, train_result 0.414708048074, test_result 0.174860426618\n",
      "Iteration 238, train_result 0.414708048074, test_result 0.174942956895\n",
      "Iteration 239, train_result 0.414708048074, test_result 0.174860426618\n",
      "Iteration 240, train_result 0.414708048074, test_result 0.174796744448\n",
      "Iteration 241, train_result 0.414708048074, test_result 0.173753780707\n",
      "Iteration 242, train_result 0.414708048074, test_result 0.173753780707\n",
      "Iteration 243, train_result 0.414708048074, test_result 0.173671235691\n",
      "Iteration 244, train_result 0.414708048074, test_result 0.173671235691\n",
      "Iteration 245, train_result 0.414708048074, test_result 0.173723317633\n",
      "Iteration 246, train_result 0.414708048074, test_result 0.173723317633\n",
      "Iteration 247, train_result 0.414708048074, test_result 0.173671235691\n",
      "Iteration 248, train_result 0.414708048074, test_result 0.173606765196\n",
      "Iteration 249, train_result 0.414708048074, test_result 0.173658847139\n",
      "Iteration 250, train_result 0.414708048074, test_result 0.173277230206\n",
      "Iteration 251, train_result 0.414708048074, test_result 0.173277230206\n",
      "Iteration 252, train_result 0.414708048074, test_result 0.173289618758\n",
      "Iteration 253, train_result 0.414708048074, test_result 0.173637050531\n",
      "Iteration 254, train_result 0.414708048074, test_result 0.174090856865\n",
      "Iteration 255, train_result 0.414708048074, test_result 0.173624661979\n",
      "Iteration 256, train_result 0.414708048074, test_result 0.173658847139\n",
      "Iteration 257, train_result 0.414708048074, test_result 0.173226837412\n",
      "Iteration 258, train_result 0.414708048074, test_result 0.173226837412\n",
      "Iteration 259, train_result 0.414708048074, test_result 0.173624661979\n",
      "Iteration 260, train_result 0.414708048074, test_result 0.173624661979\n",
      "Iteration 261, train_result 0.414708048074, test_result 0.173624661979\n",
      "Iteration 262, train_result 0.414708048074, test_result 0.173624661979\n",
      "Iteration 263, train_result 0.414708048074, test_result 0.173603088932\n",
      "Iteration 264, train_result 0.414708048074, test_result 0.174056671705\n",
      "Iteration 265, train_result 0.414708048074, test_result 0.174056671705\n",
      "Iteration 266, train_result 0.414708048074, test_result 0.174056671705\n",
      "Iteration 267, train_result 0.414708048074, test_result 0.173863570139\n",
      "Iteration 268, train_result 0.414708048074, test_result 0.173836865586\n",
      "Iteration 269, train_result 0.414708048074, test_result 0.173836865586\n",
      "Iteration 270, train_result 0.414708048074, test_result 0.173953581022\n",
      "Iteration 271, train_result 0.414708048074, test_result 0.173953581022\n",
      "Iteration 272, train_result 0.414708048074, test_result 0.174033703978\n",
      "Iteration 273, train_result 0.414708048074, test_result 0.174134116297\n",
      "Iteration 274, train_result 0.414708048074, test_result 0.173662651481\n",
      "Iteration 275, train_result 0.414708048074, test_result 0.173543169319\n",
      "Iteration 276, train_result 0.414708048074, test_result 0.174107323382\n",
      "Iteration 277, train_result 0.414708048074, test_result 0.174018364855\n",
      "Iteration 278, train_result 0.414708048074, test_result 0.173851796664\n",
      "Iteration 279, train_result 0.414708048074, test_result 0.173491102116\n",
      "Iteration 280, train_result 0.414708048074, test_result 0.173473220074\n",
      "Iteration 281, train_result 0.414708048074, test_result 0.173491102116\n",
      "Iteration 282, train_result 0.414708048074, test_result 0.173491102116\n",
      "Iteration 283, train_result 0.414708048074, test_result 0.173525287276\n",
      "Iteration 284, train_result 0.414708048074, test_result 0.173525287276\n",
      "Iteration 285, train_result 0.414708048074, test_result 0.172439152465\n",
      "Iteration 286, train_result 0.414708048074, test_result 0.172374381304\n",
      "Iteration 287, train_result 0.414708048074, test_result 0.172374381304\n",
      "Iteration 288, train_result 0.414708048074, test_result 0.172439152465\n",
      "Iteration 289, train_result 0.414708048074, test_result 0.172871162192\n",
      "Iteration 290, train_result 0.414708048074, test_result 0.172892735239\n",
      "Iteration 291, train_result 0.414708048074, test_result 0.172871162192\n",
      "Iteration 292, train_result 0.414708048074, test_result 0.173324744965\n",
      "Iteration 293, train_result 0.414858011997, test_result 0.173355253861\n",
      "Iteration 294, train_result 0.414858011997, test_result 0.173321053961\n",
      "Iteration 295, train_result 0.414858011997, test_result 0.173333567476\n",
      "Iteration 296, train_result 0.414858011997, test_result 0.173306862923\n",
      "Iteration 297, train_result 0.414858011997, test_result 0.173324744965\n",
      "Iteration 298, train_result 0.414858011997, test_result 0.173324744965\n",
      "Iteration 299, train_result 0.414858011997, test_result 0.173259973804\n",
      "Wall time: 9h 8min 19s\n"
     ]
    }
   ],
   "source": [
    "%time clf3 = clf3.fit(X_train, y_train, q_train, queries_train, X_test, y_test, q_test, queries_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0xcef40b8>]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYEAAAEACAYAAABVtcpZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzt3Xd4VGX2wPHvSYAQegfpIB0pAgIKShRQsBcsWNYFFbu4\nNnQVQV3WjhUUFLHsCgIuP1FBECSKBRCkBpDeEQglhITUOb8/3lRImZCZZJI5n+eZh7n9nWu8575d\nVBVjjDHBKaS4E2CMMab4WBAwxpggZkHAGGOCmAUBY4wJYhYEjDEmiFkQMMaYIOZVEBCRASKyQUQ2\nisiIHLbfLCKr0j4/i0jHLNsmich+EVnty4QbY4wpvHyDgIiEAO8ClwDtgcEi0uak3bYCF6hqJ+Bf\nwMQs2yanHWuMMSbAeJMT6A5sUtUdqpoMTAWuyrqDqi5W1Zi0xcVAgyzbfgaO+Ci9xhhjfMibINAA\n2JVleTdZHvI5uBOYU5hEGWOMKRplfHkyEbkQGAL09uV5jTHG+Ic3QWAP0DjLcsO0ddmkVQZPBAao\naoGLf0TEBjEyxpgCUlUpzPHeFAf9DrQQkSYiUg64CZiVdQcRaQx8CdymqltyOIekffKkqvZRZdSo\nUcWehkD42H2we2H3Iu+PL+QbBFQ1FXgAmAdEAVNVdb2I3C0iw9J2GwnUAMaLyAoRWZp+vIh8DvwK\ntBKRnSIyxCcpN8YYU2he1Qmo6ndA65PWTcjy/S7grlyOvbkwCTTGGOM/1mM4AEVERBR3EgKC3YdM\ndi8y2b3wLfFVuVJhiYgGSlqMMaYkEBG0CCqGjTHGlFIWBIwxJohZEDDGmCBmQcAYY4KYBQFjjAli\nFgSMMSaIWRAwxpggZkHAGGOCmAUBY4wJYhYEjDEmiFkQMMaYIGZBwBhjgpgFAWOMCWIWBIwxJohZ\nEDDGmCBmQcAYY4KYBQFjjAliFgSMMSaIWRAwxpggZkHAGGOCmAUBY4wJYhYEjDEmiFkQMMaYIGZB\nwBhjgpgFAWOMCWIWBIwxJoiV8WYnERkAvIkLGpNU9eWTtt8MjEhbjAXuU9XV3hxrjAkOqalw222w\nYwdUrQpVqkClSlC+PISFQZ06cOut0KBB5jEHD8Ibb8DRo/Dww9CqVfGlv7TKNwiISAjwLtAX2Av8\nLiJfqeqGLLttBS5Q1Zi0h/5EoKeXxxpjgsCkSS4AvPwyxMS4T1wcJCZCQgJs3gwdOsDFF8Mdd8D8\n+fDBB3DjjVC7NvTqBRdcAPfe64KIKoSGQufO7l9zekRV895BpCcwSlUHpi0/CWhub/QiUg1Yo6qN\nCnKsiGh+aTHGlEzR0dC+PcybB5065b5fTAx8/DFMngznnQdPPgmNG7ttcXEukEyZAsnJIAKHDrnA\nMHmyWw42IoKqFuqXexMErgMuUdVhacu3At1V9aFc9n8MaKWqwwpyrAUBY0qHd96BvXvh+eehbFm3\nbtgwCA+Ht97y7bXi4qBvX7jwQnjxRd+euyTwRRDwqk7AWyJyITAE6O3L8xpjSoatW+G55+Dss+Gi\ni+CLL2D3bvjmG1i3zvfXq1jRnbtXLzjjDHgox1dTkxdvgsAeoHGW5YZp67IRkY64uoABqnqkIMem\nGz16dMb3iIgIIiIivEieMcYfkpLggQdg8GD3pp3VggXwyy/w9NPZy+MffhgeewyeeALGjIFu3aBa\nNVcPUK2af9JZqxbMnQu9e8Phw64YqX17qF+/9BURRUZGEhkZ6dNzelMcFAr8iavc3QcsBQar6vos\n+zQGFgC3qerighybZV8rDjImgLzwAnz7rSva6dcPXn3VVeA++igsWQJ160K7dvDhhxASAl9/7QLA\nmjVQrpw7x/ffw8yZMG6c/x/I69bB+PHu36goiI11ASo1FTye7PuGhECFCu5TvboLdHfe6QJKSVIk\ndQJpFxoAvEVmM8+XRORuXCXvRBH5ALgW2AEIkKyq3XM7NpdrWBAwJkBERUFEBPzxh3uDf+YZV7ST\nkgL33AP//Kd7sA4cCG3bwptvwllnwfvvu9Y9geD4cdeCKCTEfbIGoZQUOHHCfXbvdoFs5ky4+moX\nyNq3L9i1UlNPvUZO4uNdDiu9YruwQafIgkBRsCBgTGBITXVl7EOGwN13Z65fs8ZV7rZokbkuNhYu\nucQVw3ToANOnF316fSU62jVJffNN6N8fRo+GM8+E1avh889drkjV5R7Cw10AOXjQfU6cyAw4YWGu\nTuT8890nLg5++MEVoe3a5fpFlCnjAkGPHjBqlLvfJ4uPd0H13Xehe3cXnLp1y76PBQFjgsT69e7h\nUr++e4jk5OhR2LbNtZs/+Y1U1RXlhIfnf62xY13RzoIF7qGWn5gYVxfw/PPQqFH++we6Y8dcK6a3\n3nJFRcnJcPPNcO21LgDExbkHdPnyroNb7dqughpc7iguDpYtg59+gp9/dvv17esqyjt0yLynSUnw\n6aeu7qRlS5cLqVjR/Tfavt11kjvvPPfw/+03l56mTd197tPHncOCgDFBYM4c9xCqWhX27XO9bBs1\ncu3nGzd2b+6//OICQPXq0LEjTJiQ2fN2zRpXhLN8uXt4XHMNXHmlCyhZbd/urjVypCvzP/PMIv+p\nAeXwYdfaqWtX/9ZnJCXBZ5/B77+7HEV8vPtv/PDD2ftUJCfDjBmueG7gQLfOgoAxpVx8vCtrf+89\nV+zi8bgOUrt2wc6drgeuqitO6NzZfX/xRVcRO2YMbNrkOl89/7wLJHPnurLvOXPcuZo0cQFlyxY4\ncsRd4+9/d2+tJvBZEDDGD06c8K7YpCg8+aR72H/+ecGOW7UK7r8fmjWD115zLXmyUnUP/R073Pkb\nNIAuXbwr/jGBw4KAMT7222+uUvDuu13b9jI+7U5ZMGvWuLLk1auhXr3iS4cJXL4IAhb3jUmzcqWr\nnPvgA1i7FgYMcEUvxcHjcUMt/OtfFgCMf1kQMAb480+49FLXHG/wYNccsEsXOOccV7RS1KZOdZWR\nd95Z9Nc2waUYM7vGFJ/4eNezdMsW1wLkvfdcRer117vtZcrAK6+49t79+sFLL8HQoUU3DMHUqa5M\n38rojb9ZnYApUqquOWPvYhpicMsW13Lmk09c88ozz4TmzV3Tycsuy/mY9eth0CCXKxg3LrNNuL8c\nP+6ab+7c6b/xdkzpYHUCpsTZvNn1opw3L+ft/noP2LDBlff37OmGN/7jD1ixwrW7fuWV3AMAuGER\nli515fQXXujaa/vTd9+5dFoAMEXBgoApUsuWueaIDz3kOslkNXq0exj7MhAcOwaPP+4CzwUXZM5s\n1aRJwc5TsaLLPdSs6Zpcnuzjj11dwsSJLreh6oqcNmxwPUcLEjhmznQduowpClYcZIrUY49BjRqu\nO/1FF7llgFmz3LDF5cu77vHpPSJPx8GD7vyLFrlBzy6+2HWg8kUrmx07XA/SX3/NnO92/nw3d+6z\nz7r1Cxa4Ip2kpMxZsXr3drNi5VenkJTk0rl27ak9eo05mfUTMCVORIQbgbJZMzj3XNcGPi7O9Xid\nNQv273cjVq5cmf+8sUePusG+3nvPvXWHhrqK1NRUN+bK+ee7Fj+dO/v2N7z1lntb/+EHN1TDeefB\ntGmZ47mkd8SqVs2lJy7OpeXGG2HEiLzPPW+eG1Dst998m2ZTOlkQMCWKx+MejNu2uWKVp55ydQR/\n/ukmD7/3XvcAveAC1xJnyJCcz/Hnn671zLhxcMUV7sF6xhnu4Z+a6nIa/px4PH2Uzeuvd3Pb3n+/\nS3tedu925fxvv+0GIsvNvfe6APnEE75NsymdLAiYEuXPP10xz9atbvn4cWjTxvWK/fjjzKKSxYtd\na5yNG92ojTEx7m1/7lxXoVurljtmxIjiG+Rs7VrXfHToUDfcrzdNR5cvdx3Qxo1zfRCaNMmcgxdc\ngGvYECIjM4uajMmLBQFTovz3v/B//5d9zPm9e91DPX0mqnTXX++G161QIbOO4JZb3HjqNWsWbbpz\ns2KFm3zk5LTnZfZsN0Tw5s3ut7do4SrJ//53d7477nATuhjjDQsCpkT5xz9cpWd+5eLgRr88+2xX\ndPLMM6XzzTgpyeV6XnrJ9UpO76/wr38Vd8pMSWFBwJQoF1zgKj379vVu/+Tk7MUlpdmKFa6+4Kmn\nSmfAM/5hQcAEpLVrXUuY1193ZeDgKlOrVXO9YKtXL970GVNaWI9hc9pSUlwTx8hIN4vUxo0FP0dM\njGvSmdX//ud61XbvDi+8kLn+zz/dmPYWAIwJLDaAXJB67DE3u1S9em4SlfRB1NIHUMvN4cPw1Vdu\nuIVFi1xTzEaN3Bj84Cp958xx5fmtW7tOW717u5YxJ0+SbYwpfpYTCEKTJ7uhkhcvhh9/dOPizJjh\n2qYnJOR8zLZtrkfvmWfCN9/ArbfCnj2ud+7Eia6o5+hRN09qt24uODz+uKv0BDdcRNeuRfcbjTHe\nsTqBILNkietgFRkJ7dpl33bNNdCjh5vSMN3hw64J43ffuUlOHnrI++EXEhJcx6d589xE5//6lysq\nMsb4hlUMmwLZt8+V1Y8bB1deeer2zZtdr9aoKFd+f/CgG0u/Tx9Xvl+1asGvmd788euvXc7hdM5h\njMmZBQHjtWPH3Fv4tdfC00/nvt+jj0JsLDz/vGvKee217vvpTqZy9KjrGVu37ulVPhtjcmdBwHgl\nIcENpNamjcsF5PVAP3rUVehWrOjG7hk5svDXf+op1zv2k08Kfy5jTCYLAiZfqalwww1uusTPP/du\nYLVp01zTzwcf9E0aPB4XiCpU8M35jDGOBYESKjrataYp44cGuidOuDF6YmPdg3fJEjeU8TffQFiY\n769njCk+1lmsBFmxwk060rUr1KnjRs08XamprsI1Lu7UbW+/DRMmwK5drh6gZ0839r0FAGNMTrwK\nAiIyQEQ2iMhGETll+C8RaS0iv4pIgog8ctK24SKyJu3zkK8SHghUXRv7/Myd64ZPSEx0I0h++KFb\nd7refttV7r77bvb1CQluxM2PPoKxY91sWk8+CZUqnf61jDGlW77FQSISAmwE+gJ7gd+Bm1R1Q5Z9\nagFNgKuBI6o6Nm19e2AKcA6QAswB7lHVrTlcp8QVB23b5oYCPnoUKlfOeZ99+9zb/5QpmTNP7dkD\nHTvCgQMFn/zkzz/dhCaffOIqbjdtymx2+cEH7q1/9uzT/03GmJKjqIqDugObVHWHqiYDU4Grsu6g\nqtGquhz3oM+qLbBEVRNVNRX4CchjXiXfePll92D2t1WrXKXnsmU5b/d43Nyzw4ZlBgBwE63Xq+eK\niAoiNdWNO//cc25C9ksvdW/86dtee827YZqNMSadN0GgAbAry/LutHXeWAucLyLVRaQCcCnQqGBJ\nLJhjx1xRSUEfsKdj1SpXubt4cc7bX37ZDYf8zDOnbuvXz01QXhBjx0J4eOZUhqNGuSKh6Gg3nk/1\n6m64ZmOM8ZZfB5BT1Q0i8jLwPXAcWAGk5rb/6NGjM75HREQQERFR4Gv++KN7K9661f9DFKxcCVdd\nlXMQ+OMPVz6/bFnOrYD69XOTpGcdoiEvmzbBK6+4sXlC0kJ3s2ZuyOaXXnKDuY0YcfqduowxgS8y\nMpLIyEifntObOoGewGhVHZC2/CSgqvpyDvuOAmLT6wRy2D4G2KWq7+ewzSd1AsOHw6efusm//T1D\nU/PmriXObbe5sv+sD+B//MNNg5hTLgBcE84zznD1At60n3/sMTfByosvZl+/d6/r3FW/Pqxb598J\n1o0xgaWo6gR+B1qISBMRKQfcBMzKK13ZFkRqp/3bGLgG+Pw00+qV+fPhb39zlbb+dOyYe4BfdJF7\n+O/YkX37nDmuzD43lStD585uqOX8JCfDf/7jKoJPVr++qyN46SULAMaYgss3CKRV6D4AzAOigKmq\nul5E7haRYQAiUldEdgH/AJ4WkZ0ikt4w8UsRWQt8Bdynqsf88ktwb8V//QXXXeeKg/xp9Wo3yXho\nqGuLv2RJ5ratW92EK507532O/v29qxf47js3hHNu0w4+8ogbAdQYYwrKqzoBVf0OaH3SuglZvu8n\nlwpfVS2yqsoFC1w9QIsW/s8JrFoFnTq57z16uHqBG290y3PmuH4BIfmE2H793Bj9+Zk82bUKMsYY\nXytVPYa//949WOvVc2/iOfWo9ZWVKzPf9Hv2zF45PHt23kVB6bp3d7mGgwdz3+fgQTcN5A03FC69\nxhiTk1ITBFRd0Ur//u4NvEkT2L7dN+e+/HLX6iirrDmBbt1c8VBiohu7Z9GizOkW81K2rOs/sGCB\n6wT27LMQEeFaFqWbMsVd38bhN8b4Q6kJAuvXu/Fxmjd3y82bn1ovkJjoRsgsiCVL3Jv9+PGZ61JT\n3cQrHTu65UqVXBHUqlUuWHTq5AaI80a/fq4zWUQEHD/u3vgHDMgMOlYUZIzxp1Iz0fz8+e6Bmt5M\ns1mzU+sFfv7ZlduXL5/zzFo5SR+n55133FSLNWq4Nvv16mUfKiK9SGjLFu+KgtINGeKCRu/ema17\nWreGQYNcM9NDh1wLJGOM8YdSkxNIDwLpmjc/NQgsWwbnned63B4+nP859+1zlbyPPgoDB7qiGche\nFJSuR4/MXMPAgd6nu3JlVySUtXln377uPGPHwu2351/BbIwxp6tUPF6Sk13xSdY35mbNTi0OWrYM\n7rsPrr/eTZienwkT4KabXNHOkCFudE5wlcInB4GePd2Y/XFxp247Heec44qc/vnPwp/LGGNyUyqC\nwMKF0LIl1K6duS63nEC3bvDvf7u39q++yv2cSUkuCKQ34ezb17XUWb3a5QRO7gPQpo37d+BA3w3d\nULeuGyvIGGP8pVQEgfHjXeVqVuk5gfSRKKKjXRFQy5ZumIbJk12x0JEjOZ9z+nQ46yxo184th4a6\nopnJk3MuDgoJcX0UrNOWMaYkKfHTS+7YAV26wM6dbnL0rGrWhA0bXA5h7lw3tMLChZnbr77aVcDe\neuup5+3Rw1UIZ61A3rLF5SRUXfCwwdqMMcXJppcE3n/fjRV0cgCA7PUC6UVBWV18sWujf7KoKFcp\nfNll2defeabLAXTsaAHAGFM6lOggkJAAkya5yt6cZG0mmlMQ6NvXBYGTMyDffANXXJHzgGyPPeYq\nlo0xpjQo0UHgiy9cUVDLljlvz1o5nFMQaNXKzf61aVP29d9843rp5uTyy+HBBwuXbmOMCRQlOgiM\nG5f3AGzpxUF//eV646b3Jk4n4voWZC0SOnzYVfyexnw2xhhT4pTYILB0qWvxk1fHrPScwPLlLheQ\nUzl+377Zh3OeO9d13rKmmcaYYFAig8DOnTB0qOvJm9dEKuk5gZyKgtL17QuRkW48IMi7KMgYY0qb\nEhcE/vjDDf1wxx25Vwina9IE9uxxY/rkFgTq13edslaudIFg7tyCjf1jjDElWYkaQG72bNdh6/33\n3exh+SlXzj3gf/jBHZOb9CKhhARo0AAa5Tg9jjHGlD4lJggsXeoCwKxZcO653h/XvLkb479x49z3\n6dfPVTLHxFhRkDEmuJSIIBAd7drmT5xYsAAArl4gPDzvzl19+rhew7t2uX4HxhgTLAI+CKSmwuDB\nbjTP0xmXp0cPlxPIS7VqboygLVvc/sYYEywCPgiMGuUCwZgxp3f8Pfd4t1///m4yl7xaGxljTGkT\n0APIRUW5StvVq6FOHf9ePz4eUlKgShX/XscYY3zFFwPIBXROYOdON26/vwMAuOGljTEm2AR0P4Fj\nx+zN3Bhj/Cmgg0BMDFStWtypMMaY0iugg8CxYxYEjDHGnwI6CMTEWHGQMcb4U8AHAcsJGGOM/3gV\nBERkgIhsEJGNIjIih+2tReRXEUkQkUdO2vYPEVkrIqtF5L8iUs7bxFlxkDHG+Fe+QUBEQoB3gUuA\n9sBgEWlz0m6HgAeBV086tn7a+i6q2hHXJPUmbxNnxUHGGONf3uQEugObVHWHqiYDU4Grsu6gqtGq\nuhxIyeH4UKCiiJQBKgB7vU2cFQcZY4x/eRMEGgC7sizvTluXL1XdC7wO7AT2AEdVdX7eR2WyfgLG\nGONffu0xLCLVcLmGJkAMMENEblbVz3Paf/To0RnfIyIiiImJsJyAMcakiYyMJDIy0qfnzHfsIBHp\nCYxW1QFpy08Cqqov57DvKCBWVcemLQ8CLlHVu9KWbwN6qOop08PnNHZQnTqwZo2bGMYYY0x2vhg7\nyJvioN+BFiLSJK1lz03ArLzSleX7TqCniJQXEQH6Auu9TZwVBxljjH/lWxykqqki8gAwDxc0Jqnq\nehG5223WiSJSF1gGVAY8IjIcaKeqS0VkBrACSE77d6I3CUtMBI8Hypc/vR9mjDEmfwE7lPSBA9C+\nPRw8WIyJMsaYAFZUxUHFwoqCjDHG/wI2CFgfAWOM8T8LAsYYE8QCNghYcZAxxvhfwAYBywkYY4z/\nBXQQsJyAMcb4V8AGARtG2hhj/C9gg4AVBxljjP8FbBCwimFjjPG/gA0ClhMwxhj/syBgjDFBLGCD\ngBUHGWOM/wVsELCcgDHG+J8FAWOMCWIBGwSsOMgYY/wvIOcT8HigbFlISoLQ0GJOmDHGBKhSO59A\nbCxUqGABwBhj/C0gg4ANGWGMMUUjIIOAVQobY0zRCNggYJXCxhjjfwEZBKw4yBhjikZABgErDjLG\nmKIRsEHAioOMMcb/AjIIWHGQMcYUjYAMAlYcZIwxRSMgg4ANGWGMMUUjIIOA5QSMMaZoBGwQsJyA\nMcb4n1dBQEQGiMgGEdkoIiNy2N5aRH4VkQQReSTL+lYiskJE/kj7N0ZEHsrvelYxbIwxRaNMfjuI\nSAjwLtAX2Av8LiJfqeqGLLsdAh4Ers56rKpuBM7Ocp7dwMz8rmnFQcYYUzS8yQl0Bzap6g5VTQam\nAldl3UFVo1V1OZCSx3n6AVtUdVd+F7TiIGOMKRreBIEGQNYH9+60dQV1IzAlv50WblvI4bKrLCdg\njDFFIN/iIF8QkbLAlcCTee03evRo5m2Zx9H4iqxa9TQXXxxRFMkzxpgSITIyksjISJ+e05sgsAdo\nnGW5Ydq6ghgILFfVg3ntNHr0aMpEhrPkYDT9+0cU8BLGGFO6RUREEBERkbH83HPPFfqc3hQH/Q60\nEJEmIlIOuAmYlcf+OU11NhgvioIAKofUoWy1g0ihJkwzxhjjjXxzAqqaKiIPAPNwQWOSqq4Xkbvd\nZp0oInWBZUBlwCMiw4F2qnpcRCrgKoWHeZOgcE9tQivnmWEwxhjjI17VCajqd0Drk9ZNyPJ9P9Ao\nl2PjgdreJqhccm2oaEHAGGOKQsD1GC6TVJvU8APFnQxjjAkKARcEQk7UIaWs5QSMMaYoBFwQSIyt\nCOIhPjm+uJNijDGlXsAFgdhYIVxrczDOcgPGGONvARcEYmKgktTmQJzVCxhjjL8FXBA4dgyqlKnD\nwXjLCRhjjL8FXBBo3x4a1bDiIGOMKQoBFwT+/nfo1KK25QSMMaYIBFwQAKhd0eoEjDGmKARkEKhT\n0eoEjDGmKARkEKhdoWTWCaR6Uos7CcYYUyCBGQQqBk6dQGxiLC/8+AJrD6zNdZ+k1CTeXvI29V6v\nx3u/v1eEqTPGmMIpkkllCqp2heKvE1BVpq+bzqPzHqVzvc6M+30cswbPonuD7hn7eNTDtKhpPP3D\n07Su2Zr3LnuPe7+9l6vaXEX9yvW9uk5iSiLR8dE0qHI6k7UZY4pDfHI8O2N2EpMQQ0xiDLGJsSSk\nJJCQkkBiaiIAghAiIVQOq0zdinWpV6ke4WXD2RWzix0xO9hzbA+VylWiTsU61K5YmyphVSgXWo6w\n0DBEhGOJx4hJiCE2KRaPegiVUEIkhJY1W9KmVhuf/ZaADAJ1KtYp1uKg2MRYBk0fxL7YfUy5bgq9\nG/fmm43fcPnnlzPt+mn0adKHeVvm8dSCpwgNCeXDKz7kwmYXAvDHvj94ZO4jTB00Nd/rJKcmc920\n6/h558/MvXUuPRr2yNjmUQ/vLn2XuhXrckP7GxCbYMGYIpHiSWHelnlMXjmZ/cf3MzpiNBc1uwhw\nL4efr/mcx79/nErlKlGtfDWqlq9KpXKVCC8TTniZcMqFlkNE8KgHj3o4lniM/XH72X98P3HJcTSq\n0ogm1ZrQoHID9sTuYdHORRyIO0BsUixJqUkkpSaR6kmlSlgVqpavSpWwKoRISMb5rmt7nU+DgKiq\nz05WGCKi6WlRVcLHhHNkxBHCy4YXeVqGfT2MhJQEPrrqI8qEZMbJhdsWcuOMG2lZsyWH4g8x5qIx\nXNv22mwP6PjkeDq814Hxl47nkhaX5HoNj3oY8tUQouOjubvr3dw5605m3jiTXo17cSzxGLfNvI3o\n+GiOnDjCWXXOYvxl46lVoZZff7cxJdn6g+v5acdPOW5TlKTUJOKT44lPjifFk0K50HKUCy1HqIQS\nlxxHbGIsRxOPMm/LPBpVacSQzkOoHFaZkQtH0rpma+4/537GLh7LofhDvH/5+/Rs2LOIf+GpRARV\nLdQbYkAGAYBGbzTil6G/0Lhq4zyO8r3Zm2Zz37f3sfre1VQJq3LK9pV/rWTdwXXc0P6GbAEiq+82\nf8f9s+9n7b1rcw1ij897nF92/cL8v82nQtkKzNsyj1v/dyuvXfwaL/38En2a9OGtgW+R6knlmR+e\nYWrUVD644gMubXmpT39voNsXu48fd/zILzt/oWfDntzc4WbLFQUIVQ2I/xY7Y3YyKnIU3278lita\nXZHr/5dhZcKoULYCFcpWIFRCSfYkk5SaRHJqMhXLVaRKWBUql6tMr8a9aFe7XcZxSalJvL/sfSYu\nn8hdXe7i/u7353qNolaqg0CXCV2YeMVEutXv5rdr3jXrLjrV68R959xHiIRw+MRhOr7Xkc+u+Syj\neOd03TjjRlrVaMULF71wyra3l7zNhOUTWDRkETXCa2SsX7B1ATfOuJF/9/03w7pmn4jtx+0/cuvM\nW7nz7DsZ2WckIRKQdfqFEh0fzcTlE9lxdAc7j+1k8+HNRMdHc0GTCzi34blMXzed8DLhvDPwHc4+\n4+ziTm7QWbpnKdOiprHmwBrWHljLwbiDtKjRgta1WnNW7bN4vNfjOb44+Yuq8vyPz/P20re5t9u9\nPHbeY1RC3Es1AAAT/ElEQVQrX63Irh8ISnUQuOQ/l/Bwj4cZ2HKgX6634+gOukzsQttabVGUD6/4\nkBd+eoHaFWrz1sC3Cn3+nTE7OXvC2ay/fz11KtbJWB8dH03rd1uz9M6lnFnjzFOOy+vt6q/jf3Hd\ntOuoVaEWn13zWZH+D1cUnvj+CVbvX81Vra+icdXGNK3WlDa12hAaEgq4JrgfrfiIZxY+wz1d7+G5\nCws2yfaJ5BPFUrxYGszZNIfb/+92hvcYTud6nTmrzlnUqViHzYc3syF6A1+u/5IDcQeYfctsypcp\nXyRpGvPTGKavm86cW+ZwRuUziuSagaZUB4HbZt5G/+b9+Vunv/nleq/+8iqbDm/i/cvf5/1l7zNy\n4UhqVajFirtXUKFsBZ9c44HZDxBeJpxXL341Y90T3z9BbGIs711+ek1Jk1KTGD5nOAu3L2R0xGiu\na3sdZUPL+iS9xcmjHpq82YQ5t8zhrDpn5blvdHw0XSd2ZcLlExjQYkC+505ISeBfP/2L1359jfX3\nr6dZ9Wa+SnZQmLl+Jvd8ew9f3fRVruXgqZ5UBn85mBRPCtOun5atuERV2XJkC7/u+pUlu5ew9/he\nDsUf4tCJQ4SFhtG8enNa1GhBg8oNSPGkZLSwCZXQjHL7s+qcRb/m/TJekCb9MYkxi8bwy9BfgjYA\nQCkPAo/MfYQGlRvw6HmP+uV6XSd25dX+r2bU+u85tocUTwpNqjXx2TX2HNtDh/c6sO7+ddSrVI+/\njv9F+/HtWXXPKhpWaVioc8/6cxZvLH6DjYc2cnfXuxnSeQiNqmZO85yUmsSHf3zIuN/H8fXgr2le\nvXlhf45fRW6PZPh3w1l1zyqv9v9h2w/8bebfWH3v6mxFaif7bddvDJ01lDa12pCYksg1ba7hrq53\n+SrZpZqq8tnqzxgxfwTf3vwtXc7okuf+iSmJXD7lcppUbcJrF7/G91u+55tN3zB381zKhJShV+Ne\nnNvwXBpXbUzN8JrUrFCThJQEthzewubDm9kbu9c1kSwTRlhoGB71kJiaSGJKIgu3L8SjHh459xGq\nhFXhwTkP8uPff6RVzVZFdDcCU6kOAi8uepGjCUd5uf/LPr/WxkMb6fNxH3b/Y3dGUYO/PPzdwwjC\nGwPeYPic4YgIbw5402fnX7N/DeN+H8f0ddNpUaMF17W9jprhNRmzaAytaraiUrlKtK3VNse6iUAy\n7OthtKjRgid6PeH1MQ9/9zB/Hf8roznukRNHeOWXV1i6dynR8dEcij9Eqqby9oC3GdRuEB+v/Ji5\nW+Z61Xw3mO2L3cenqz7lo5Wuddy0QdNoX6e9V8ceTzpO30/7svbAWiKaRnB5y8sZ2HIgTas1LVSa\nVJX5W+fz+m+vs3j3YubdNi9bn51gVaqDwId/fMivu37lo6s+8vm1XvjxBQ7EHeCdS9/x+blP9tfx\nv2g3rh1zbpnDwP8OzMgV+FpyajKR2yP5cv2X7IzZyYheI+jTtA+r96/mss8vY/vw7X4PeKcrMSWR\n+mPrs/LuldlyM/k5kXyCrhO7MqLXCA6dOMRLP7/EtW2vZVC7QRlvmnUr1iWsTBjg6mm6TuzK/sf2\nl8qK9cKITYzl/zb8H5+v/ZzFuxczqO0ghp49lJ4Nexa4BVCKJ4Wk1CSfFauezKMe+++XxhdBIDDa\nOeXAX4PIqSpT1k7hgys+8Pm5c1KvUj2Gnj2U/p/1595u9/olAACUDS1L/zP70//M/tnWd6zbkToV\n67Bg2wIuPvNiv1y7sOZsdvUABQkAAOFlw/nsms/oOaknA1oMIPLvkdma9p2scdXGVC9fnTX719Cp\nXqfCJrvUGLVwFG8ueZM+Tfpwe6fbmXH9DCqWq3ja5ysTUsavTSgtAPhWwAYBfw0it/bAWo4nHefc\nRuf6/Ny5eaLXE/y882ce7/V4kV0zq6GdhzJ55eSADQKfr/mcWzrcclrHdq3v3uzzqhfIql/zfszf\nOt+CQJqJyyfyRdQXbHxgI3Ur1S3u5JhiELAh1V+DyE1dO5Ub299YpG8TdSrWYfGdi4utx+/gDoOZ\ns2kOR04cKdLrqirrD65n/O/j+XjlxzmOsnos8Rhzt8xlULtBp30dbwMAQN9mfVmwbcFpXyurOZvm\nMGPdDH7b9Rs7Y3aWuFFkI7dHMnLhSL4e/LUFgCAW0DkBXw0ip6okpCRwIuUEU6OmMm3QNJ+ct6So\nEV6DAS0GMGXtFO475z6/X+9A3AGeXfgsX/35FWGhYVzY7EK2HtnKG4vf4M1L3szWEe9/6//HhU0v\nLNCDvDAubHYhQ74aQlJqEuVCy532ecb8NIZJKybRqV4n9hzbw57YPSSnJnNV66u4tu21XNTsooy6\niEC09chWbppxE1Oum0LLmi2LOzmmGAVsEKgSVoWk1CQSUhIK1flk5A8jGbNoDOVCy1G+THm61e+W\nb1O30mhI5yE8s/CZQgWB7Ue3M3P9TGISYzLKfRtUbkDvxr1pWq0pHvUwYfkERkeO5vZOt/Pr0F8z\n2uSrKl+u/5Khs4ZmtBTZc2wPu4/tZtr1RReUa4TXoFXNVizZvYTzm5yfbZuqMi1qGo/Me4To+OiM\n9T0b9uTFvi9yXqPzUFVGLhzJzA0zT2mjvu3INmZumMmYRWO4cuqVgCu/DpEQqpevTr1K9ahbqS63\ndbyNmzvcfEravlz3JXUq1qF3494+H45h2d5ljP1tLCdSTpCUmsTq/asZ1WdURhNpE7wCtnUQQMOx\nDfntjt8KXGGYbtOhTZw76Vyi7osK+uxuqieVpm81ZfbNs+lQt0Oe+/626zeW7lma0V77SMIRpq+b\nzqZDm7imzTXUr1yfFE8KyZ5kdsTsYNGORYgIVcKqULtCbcZfNj7XDl8JKQnM3jSbqmFVaVClAQ0q\nN6ByWGV//ORcjfh+BOFlwxkdMTpj3d7Yvdz77b1sPryZD6/4MONFwaMevoj6gmcXPsvZZ5xN/Ur1\n+W33b3x/2/fUrlg712ukeFLwqAdVJVVTOXziMPuP72dHzA7u+eYevrn5m2xNHL/b/B1DvhpClbAq\nhIWGcf8593NLx1uoVK5SoX9v1IEo+n7alxG9RtCkWhPCQsOoEV6jSOvFjH/4onUQqprvBxgAbAA2\nAiNy2N4a+BVIAB45aVtVYDqwHogCeuRyDT1Z5/c76/K9y09Z762rp16tLy568bSPL21eWvSSth/X\nXtcdWJfj9h1Hd+hNM27ShmMb6gPfPqDDZg3T22fersNmDdNvN36rSSlJOR7n8Xh086HNunDbQvV4\nPP78CT4xb/M87TWpV8byZ6s+01qv1NKRP4zUhOSEHI85kXxCX/vlNb1x+o16KP5Qoa4/I2qGNn2z\nqR6OP6yqqtuObNO6r9bVn7b/pKmeVJ23eZ5eNeUqrfJiFb1+2vU6I2qGxiXFnda1th7eqg3HNtT/\nrPpPodJsAlPac9Or53huH28CQAiwGWgClAVWAm1O2qcW0BV4IYcg8DEwJO17GaBKLtc55Qf2/7S/\nztk057Ruzg9bf9CmbzbVE8knTuv40sjj8egHyz/QWq/U0k9Xfqqqqsmpyfrrzl/1iXlPaI2Xa+iz\nPzyrxxOPF3NK/SsuKU4r/buS7o7ZrTd/ebO2ebeN/rH3jyJNw4OzH9Srp16tJ5JPaNcJXfX1X18/\nZZ+DcQd14rKJ2u/TflrlxSp66X8v1bcWv6XrD67X2MTYjM++2H26dPdSnRE1Q8cvHa8zombo0t1L\nNepAlJ751pn6zpJ3ivS3maLjiyCQb3GQiPQERqnqwLTlJ9MufEpXXhEZBcSq6ti05SrAClU9daS0\nU4/Vk9Ny6/9u5ZIzL+G2TrfleMz+4/tZtX8Vq/evJjYxlju63EHjqo1J9aTS7YNuPNX7KW5of0N+\nlw46q/ev5obpN1CtfDU2RG+gabWm9Gvejwe7P+jTYTMC2UWfXMTyfcu5pcMtvHbxa37r2JSbxJRE\nek/uTUJKAm1qtWHaoGl51gMcPnGY+VvnM3fzXL7f+j2HTxzO2FahbAUaVW1E46qNqVOhDgfiD7Az\nZie7j+1meI/h/PP8fxbFTzLFoKg6izUAdmVZ3g1421+7GRAtIpOBTsAyYLiqnvDm4NoVcm8m+sov\nr/DvRf/m7DPOpmOdjihK5/c7c2nLS2levTkVylbg+nbXe5nM4NKxbkeWDVvGwm0L6dGwR7ZRToPF\n0+c/TUJKApe1uqxYrh9WJowvBn3BUwue4sMrPsy3IrhGeA1uaH+DvdQYn/N366AyQBfgflVdJiJv\nAk8Co7w5uHbF3DuMRW6P5NNrPuXK1ldmrHv+wueZsGwCk1ZM4j/X/icgJrwIVJXKVeKK1lcUdzKK\nTd/mfYs7CTSv3pwvBn1R3MkwQc6bILAHyDq9V8O0dd7YDexS1WVpyzOAEbntPHr06IzvERER1K5c\nmy2Ht+S479oDa2lfO/ugVtXKV2NE7xGM6J3rJYwxpsSKjIwkMjLSp+f0Jgj8DrQQkSbAPuAmYHAe\n+2e8fqvqfhHZJSKtVHUj0BdYl9uBWYMAQMyGmByLg44lHuPQiUM2LrwxJqhEREQQERGRsfzccwWb\nWCkn+QYBVU0VkQeAebiWQpNUdb2I3O0260QRqYsr768MeERkONBOVY8DDwH/FZGywFZgiLeJO6Py\nGew6tuuU9VEHomhbq60NJGWMMYXkVZ2Aqn6H6wuQdd2ELN/3Azn26FLVVcA5p5O4DnU68Gf0n6dM\nC7j2wNp8Z58yxhiTv4B+lQ4vG07b2m1Z+dfKbOujDkZZEDDGGB8I6CAA0L1+d5bsWZJtXU6VwsYY\nYwou4INAj4Y9WLpnabZ1VhxkjDG+EfBBoHuD7DmB6PhoTqScKPRE7cYYY0pAEGhTqw3R8dEZQ/tG\nHYiife321hHMGGN8IOCDQIiE0K1+t4wiIasUNsYY3wn4IADQo0FmvYDVBxhjjO+UiCCQtV7AWgYZ\nY4zvlIggkJ4TUFUrDjLGGB8qEUHgjMpnUKFsBX7e+TMhEhKUQx8bY4w/lIggAC438NHKj6xlkDHG\n+FCJCQLdG3RnWtQ0KwoyxhgfKjFBoEeDHsQnx1sQMMYYHyoxQaBr/a6ESIi1DDLGGB8qMUGgUrlK\n/Puif9PljC7FnRRjjCk1RFWLOw0AiIgGSlqMMaYkEBFUtVAtZUpMTsAYY4zvWRAwxpggZkHAGGOC\nmAUBY4wJYhYEjDEmiFkQMMaYIGZBwBhjgpgFAWOMCWIWBIwxJohZEDDGmCBmQcAYY4KYBQFjjAli\nXgUBERkgIhtEZKOIjMhhe2sR+VVEEkTkkZO2bReRVSKyQkSW+irhxhhjCi/fICAiIcC7wCVAe2Cw\niLQ5abdDwIPAqzmcwgNEqOrZqtq9kOkNCpGRkcWdhIBg9yGT3YtMdi98y5ucQHdgk6ruUNVkYCpw\nVdYdVDVaVZcDKTkcL15ex6SxP3LH7kMmuxeZ7F74ljcP5wbArizLu9PWeUuB70XkdxG5qyCJM8YY\n419liuAavVR1n4jUxgWD9ar6cxFc1xhjTD7ynVlMRHoCo1V1QNryk4Cq6ss57DsKiFXVsbmcK9ft\nImLTihljTAEVdmYxb3ICvwMtRKQJsA+4CRicx/4ZCRKRCkCIqh4XkYrAxcBzOR1U2B9ijDGm4PIN\nAqqaKiIPAPNwdQiTVHW9iNztNutEEakLLAMqAx4RGQ60A2oDM9Pe8ssA/1XVef76McYYYwomYCaa\nN8YYU/SKvelmfh3RSjMRaSgiP4hIlIisEZGH0tZXF5F5IvKniMwVkarFndaiIiIhIvKHiMxKWw7K\neyEiVUVkuoisT/v76BHE9+IfIrJWRFaLyH9FpFyw3AsRmSQi+0VkdZZ1uf52EXlKRDal/d1c7M01\nijUIeNkRrTRLAR5R1fbAucD9ab//SWC+qrYGfgCeKsY0FrXhwLosy8F6L94CZqtqW6ATsIEgvBci\nUh/XEbWLqnbEFSsPJnjuxWTc8zGrHH+7iLQDbgDaAgOB8SKSb11rcecE8u2IVpqp6l+qujLt+3Fg\nPdAQdw8+SdvtE+Dq4klh0RKRhsClwIdZVgfdvRCRKsD5qjoZQFVTVDWGILwXaUKBiiJSBggH9hAk\n9yKtOf2Rk1bn9tuvBKam/b1sBzbhnrF5Ku4gUNiOaKWGiDQFOgOLgbqquh9coADqFF/KitQbwOO4\nDobpgvFeNAOiRWRyWtHYxLSWdkF3L1R1L/A6sBP38I9R1fkE4b3Iok4uv/3k5+kevHieFncQMICI\nVAJmAMPTcgQn19aX+tp7EbkM2J+WM8orC1vq7wWuyKMLME5VuwBxuCKAYPy7qIZ7820C1MflCG4h\nCO9FHgr124s7COwBGmdZbpi2LmikZXFnAJ+p6ldpq/enNbtFROoBB4orfUWoF3CliGwFpgAXichn\nwF9BeC92A7tUdVna8pe4oBCMfxf9gK2qelhVU4GZwHkE571Il9tv3wM0yrKfV8/T4g4CGR3RRKQc\nriParGJOU1H7CFinqm9lWTcL+Hva99uBr04+qLRR1X+qamNVbY77O/hBVW8Dvib47sV+YJeItEpb\n1ReIIgj/LnDFQD1FpHxaJWdfXMOBYLoXQvbccW6/fRZwU1rrqWZACyDf4fuLvZ+AiAzAtYRI74j2\nUrEmqAiJSC/gJ2ANLkunwD9x/+Gm4aL6DuAGVT1aXOksaiLSB3hUVa8UkRoE4b0QkU64CvKywFZg\nCK6CNBjvxSjci0EysAK4E9cxtdTfCxH5HIgAagL7gVHA/wHTyeG3i8hTwB24ezXcm865xR4EjDHG\nFJ/iLg4yxhhTjCwIGGNMELMgYIwxQcyCgDHGBDELAsYYE8QsCBhjTBCzIGCMMUHMgoAxxgSx/wfs\njHblfqenXwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x85e0cc0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Визуализация\n",
    "axis_x = map(lambda x: x[0], results_buf1)[1:100]\n",
    "train_y1 = map(lambda x: x[1], results_buf1)[1:100]\n",
    "test_y1 = map(lambda x: x[2], results_buf1)[1:100]\n",
    "train_y2 = map(lambda x: x[1], results_buf2)[1:100]\n",
    "test_y2 = map(lambda x: x[2], results_buf2)[1:100]\n",
    "#pylab.plot(axis_x, train_y)\n",
    "pylab.plot(axis_x, test_y1)\n",
    "pylab.plot(axis_x, test_y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(301, 296, 300)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(results_buf1), len(results_buf2), len(results_buf3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: Qt4Agg\n"
     ]
    }
   ],
   "source": [
    "%matplotlib auto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Визуализация\n",
    "axis_x = map(lambda x: x[0], results_buf1)[1:295]\n",
    "train_y1 = map(lambda x: x[1], results_buf1)[1:295]\n",
    "test_y1 = map(lambda x: x[2], results_buf1)[1:295]\n",
    "train_y2 = map(lambda x: x[1], results_buf2)[1:295]\n",
    "test_y2 = map(lambda x: x[2], results_buf2)[1:295]\n",
    "train_y3 = map(lambda x: x[1], results_buf3)[1:295]\n",
    "test_y3 = map(lambda x: x[2], results_buf3)[1:295]\n",
    "#pylab.plot(axis_x, train_y)\n",
    "pylab.plot(axis_x, test_y1)\n",
    "pylab.plot(axis_x, test_y2)\n",
    "pylab.plot(axis_x, test_y3)\n",
    "blue_patch = mpatches.Patch(color='blue', label='With feature selection')\n",
    "green_patch = mpatches.Patch(color='green', label='No feature selection')\n",
    "red_patch = mpatches.Patch(color='red', label='Stochastic + fs')\n",
    "pylab.legend(handles=[blue_patch, green_patch, red_patch])\n",
    "pylab.xticks(np.arange(0, 301, 20))\n",
    "pylab.yticks(np.arange(0.14, 0.205, 0.005))\n",
    "pylab.grid(True)\n",
    "#pylab.plot(axis_x, test_y3)\n",
    "#pylab.plot(axis_x, train_y1)\n",
    "#pylab.plot(axis_x, train_y2)\n",
    "#pylab.plot(axis_x, train_y3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x13c17cc0>]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAEACAYAAABYq7oeAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x9a387b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "axis_x = map(lambda x: x[0], results_buf1)[1:]\n",
    "train_y = map(lambda x: x[1], results_buf1)[1:]\n",
    "test_y = map(lambda x: x[2], results_buf1)[1:]\n",
    "#pylab.plot(axis_x, train_y)\n",
    "pylab.plot(axis_x, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x163685c0>]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "axis_x2 = map(lambda x: x[0], results_buf2)[1:]\n",
    "train_y2 = map(lambda x: x[1], results_buf2)[1:]\n",
    "test_y2 = map(lambda x: x[2], results_buf2)[1:]\n",
    "pylab.plot(axis_x2, train_y2)\n",
    "#pylab.plot(axis_x2, test_y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Сохраним пока данные\n",
    "with open('results_no_fs_300.txt','w+') as fout:\n",
    "    for i, train, test in results_buf2:\n",
    "        s = \"Iteration {}, train_result {}, test_result {}\\n\".format(i, train, test)\n",
    "        fout.write(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Сохраним пока данные\n",
    "with open('results_fs_300.txt','w+') as fout:\n",
    "    for i, train, test in results_buf1:\n",
    "        s = \"Iteration {}, train_result {}, test_result {}\\n\".format(i, train, test)\n",
    "        fout.write(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Сохраним пока данные\n",
    "with open('results_stoch_300.txt','w+') as fout:\n",
    "    for i, train, test in results_buf3:\n",
    "        s = \"Iteration {}, train_result {}, test_result {}\\n\".format(i, train, test)\n",
    "        fout.write(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Откроём сохранённые результаты fs\n",
    "with open('results_fs_300.txt','r+') as fin:\n",
    "    results_buf1 = []\n",
    "    lines = fin.readlines()\n",
    "    for line in lines:\n",
    "        tokens = line.split(' ')\n",
    "        results_buf1.append((int(tokens[1][:-1]), float(tokens[3][:-1]), float(tokens[5])))\n",
    "        #print tokens[1][:-1]\n",
    "        #print tokens[3][:-1]\n",
    "        #print tokens[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Откроём сохранённые результаты no_fs\n",
    "with open('results_no_fs_300.txt','r+') as fin:\n",
    "    results_buf2 = []\n",
    "    lines = fin.readlines()\n",
    "    for line in lines:\n",
    "        tokens = line.split(' ')\n",
    "        results_buf2.append((int(tokens[1][:-1]), float(tokens[3][:-1]), float(tokens[5])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Откроём сохранённые результаты stoch\n",
    "with open('results_stoch_300.txt','r+') as fin:\n",
    "    results_buf3 = []\n",
    "    lines = fin.readlines()\n",
    "    for line in lines:\n",
    "        tokens = line.split(' ')\n",
    "        results_buf3.append((int(tokens[1][:-1]), float(tokens[3][:-1]), float(tokens[5])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x9f852b0>]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAEACAYAAABYq7oeAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n6QAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x864f588>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "axis_x1 = map(lambda x: x[0], results_buf2)[1:]\n",
    "train_y1 = map(lambda x: x[1], results_buf2)[1:]\n",
    "test_y1 = map(lambda x: x[2], results_buf2)[1:]\n",
    "#pylab.plot(axis_x, train_y)\n",
    "pylab.plot(axis_x1, test_y1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x8761c18>]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAEACAYAAABYq7oeAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/luse20AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x85b4780>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "axis_x3 = map(lambda x: x[0], results_buf3)[1:]\n",
    "train_y3 = map(lambda x: x[1], results_buf3)[1:]\n",
    "test_y3 = map(lambda x: x[2], results_buf3)[1:]\n",
    "#pylab.plot(axis_x, train_y)\n",
    "pylab.plot(axis_x3, test_y3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: Qt4Agg\n"
     ]
    }
   ],
   "source": [
    "%matplotlib auto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.patches as mpatches\n",
    "\n",
    "# Визуализация\n",
    "axis_x = map(lambda x: x[0], results_buf1)[:200]\n",
    "train_y1 = map(lambda x: x[1], results_buf1)[:200]\n",
    "test_y1 = map(lambda x: x[2], results_buf1)[:200]\n",
    "train_y2 = map(lambda x: x[1], results_buf2)[1:200]\n",
    "test_y2 = map(lambda x: x[2], results_buf2)[1:200]\n",
    "#pylab.plot(axis_x, train_y)\n",
    "pylab.plot(axis_x, test_y1)\n",
    "pylab.plot(axis_x, test_y2)\n",
    "blue_patch = mpatches.Patch(color='blue', label='With feature selection')\n",
    "green_patch = mpatches.Patch(color='green', label='No feature selection')\n",
    "pylab.legend(handles=[blue_patch, green_patch])\n",
    "pylab.xticks(np.arange(0, 201, 20))\n",
    "pylab.yticks(np.arange(0.14, 0.21, 0.005))\n",
    "pylab.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0. ,  0.2,  0.4,  0.6,  0.8,  1. ]),\n",
       " <a list of 6 Text xticklabel objects>)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Результаты на 5000 элементах"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n = 200\n",
    "clf2 = LambdaMART(n, alpha=0.5, beta=1., feature_subset=True, feature_fraction=0.3)\n",
    "clf1 = LambdaMART(n, alpha=0.5, beta=1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration  1  result  0.0287593537783\n",
      "Iteration  31  result  0.852402040229\n",
      "Iteration  61  result  0.918656263683\n",
      "Iteration  91  result  0.935536614135\n",
      "Iteration  121  result  0.935536614135\n",
      "Iteration  151  result  0.935536614135\n",
      "Iteration  181  result  0.935536614135\n",
      "Wall time: 5h 40min 27s\n"
     ]
    }
   ],
   "source": [
    "%time clf2 = clf2.fit(X_full_train, y_full_train, q_full_train, queries_full_train, X_test, y_test, q_test, queries_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration  1  result  0.24469999225\n",
      "Iteration  31  result  0.803626307519\n",
      "Iteration  61  result  0.802526356657\n",
      "Iteration  91  result  0.802526356657\n",
      "Iteration  121  result  0.805159791912\n",
      "Iteration  151  result  0.868780580111\n",
      "Iteration  181  result  0.805159791912\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-bba5a5d603af>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmagic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mu'time clf1 = clf1.fit(X_full_train, y_full_train, q_full_train, queries_full_train, X_test, y_test, q_test, queries_test)'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32mD:\\Programs\\lib\\site-packages\\IPython\\core\\interactiveshell.pyc\u001b[0m in \u001b[0;36mmagic\u001b[0;34m(self, arg_s)\u001b[0m\n\u001b[1;32m   2156\u001b[0m         \u001b[0mmagic_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmagic_arg_s\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marg_s\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpartition\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m' '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   2157\u001b[0m         \u001b[0mmagic_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmagic_name\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprefilter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mESC_MAGIC\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2158\u001b[0;31m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmagic_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmagic_arg_s\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2159\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   2160\u001b[0m     \u001b[1;31m#-------------------------------------------------------------------------\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mD:\\Programs\\lib\\site-packages\\IPython\\core\\interactiveshell.pyc\u001b[0m in \u001b[0;36mrun_line_magic\u001b[0;34m(self, magic_name, line)\u001b[0m\n\u001b[1;32m   2077\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'local_ns'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getframe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstack_depth\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf_locals\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   2078\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2079\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2080\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   2081\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<decorator-gen-60>\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n",
      "\u001b[0;32mD:\\Programs\\lib\\site-packages\\IPython\\core\\magic.pyc\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[1;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mD:\\Programs\\lib\\site-packages\\IPython\\core\\magics\\execution.pyc\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m   1178\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m             \u001b[0mst\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1180\u001b[0;31m             \u001b[1;32mexec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlocal_ns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1181\u001b[0m             \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-9a8df8d650bb>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X_train, y_train, qids_train, queries_train, X_test, y_test, qids_test, queries_test, verbose)\u001b[0m\n\u001b[1;32m     55\u001b[0m                 \u001b[0mqueries\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mqueries_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msub_idx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m                 \u001b[0mgrad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredicted\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mid_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m                 \u001b[1;32mprint\u001b[0m \u001b[1;34m\"Grad:\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-9a8df8d650bb>\u001b[0m in \u001b[0;36mloss_grad\u001b[0;34m(self, pred_y, y, id_y)\u001b[0m\n\u001b[1;32m    128\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mbuf\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m0.\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m                     \u001b[0mgrad\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mdelta_ndcg\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mbuf\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m                 \u001b[0mh\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mdelta_ndcg\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrho\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred_y\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpred_y\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrho\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred_y\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpred_y\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mgrad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-9a8df8d650bb>\u001b[0m in \u001b[0;36mrho\u001b[0;34m(self, a, b)\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mrho\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 148\u001b[0;31m         \u001b[1;32mreturn\u001b[0m \u001b[1;36m1.\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1.\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%time clf1 = clf1.fit(X_full_train, y_full_train, q_full_train, queries_full_train, X_test, y_test, q_test, queries_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5 1 0.5 0.336099646865 0.334095124305\n",
      "0.5 1 1.0 0.284810928463 0.336022586491\n",
      "0.5 1 2.0 0.278983133131 0.235084867792\n",
      "0.5 2 0.5 0.41276830698 0.325806649599\n",
      "0.5 2 1.0 0.269452898252 0.219380711384\n",
      "0.5 2 2.0 0.357579678573 0.328906145272\n",
      "0.5 5 0.5 0.360515329939 0.291737452324\n",
      "0.5 5 1.0 0.396132881545 0.352325317071\n",
      "0.5 5 2.0 0.357491363348 0.307072058994\n",
      "0.5 7 0.5 0.307196757893 0.243569464569\n",
      "0.5 7 1.0 0.468726043507 0.381784322914\n",
      "0.5 7 2.0 0.433379695178 0.35421116786\n",
      "1.0 1 0.5 0.298903738381 0.329808291355\n",
      "1.0 1 1.0 0.452061919927 0.348656410939\n",
      "1.0 1 2.0 0.43510791712 0.279133197285\n",
      "1.0 2 0.5 0.298245437817 0.270975813759\n",
      "1.0 2 1.0 0.426176753028 0.298526360796\n",
      "1.0 2 2.0 0.30197343755 0.296332263381\n",
      "1.0 5 0.5 0.444450124696 0.334779000506\n",
      "1.0 5 1.0 0.431277962121 0.373842327464\n",
      "1.0 5 2.0 0.429456655432 0.344643182745\n",
      "1.0 7 0.5 0.438494402494 0.358339454444\n",
      "1.0 7 1.0 0.431007588023 0.367884660402\n",
      "1.0 7 2.0 0.446516329361 0.336559264488\n",
      "2.0 1 0.5 0.259869836501 0.210006019129\n",
      "2.0 1 1.0 0.405543147091 0.281986651447\n",
      "2.0 1 2.0 0.313821884084 0.268496532276\n",
      "2.0 2 0.5 0.458445432413 0.350849454051\n",
      "2.0 2 1.0 0.257580355035 0.30694327258\n",
      "2.0 2 2.0 0.333093525779 0.259259257385\n",
      "2.0 5 0.5 0.34329521534 0.222004082956\n",
      "2.0 5 1.0 0.34956759061 0.263642761974\n",
      "2.0 5 2.0 0.345498792654 0.35035288334\n",
      "2.0 7 0.5 0.442491213356 0.37117135548\n",
      "2.0 7 1.0 0.425625394444 0.319880540672\n",
      "2.0 7 2.0 0.434023061723 0.394820552599\n"
     ]
    }
   ],
   "source": [
    "# Небольшой GridSearch с кросс-валидацией\n",
    "from numpy import unravel_index\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "arr_a = [0.5, 1., 2.] # Одно значение в целях оптимизации\n",
    "arr_L = [1, 2, 5, 7]\n",
    "arr_b = [0.5, 1., 2.] # Одно значение в целях оптимизации\n",
    "\n",
    "results1 = np.zeros((len(arr_a), len(arr_L), len(arr_b))) # С адаптивным шагом\n",
    "results2 = np.zeros((len(arr_a), len(arr_L), len(arr_b))) # Без адаптивного шага\n",
    "\n",
    "for i_a, a in enumerate(arr_a):\n",
    "    for i_L, L in enumerate(arr_L):\n",
    "        for i_b, b in enumerate(arr_b):\n",
    "            for k in xrange(5):\n",
    "                # Перемешаем\n",
    "                shuffle_idx = np.random.permutation(num_elems)\n",
    "                X_train = X[shuffle_idx][:num_elems/2.]\n",
    "                X_valid = X[shuffle_idx][num_elems/2.:num_elems*3./4.]\n",
    "\n",
    "                y_train = y[shuffle_idx][:num_elems/2.]\n",
    "                y_valid = y[shuffle_idx][num_elems/2.:num_elems*3./4.]\n",
    "\n",
    "                q_train = queries[shuffle_idx][:num_elems/2.]\n",
    "                q_valid = queries[shuffle_idx][num_elems/2.:num_elems*3./4.]\n",
    "\n",
    "                queries_train = q[shuffle_idx][:num_elems/2.]\n",
    "                queries_valid = q[shuffle_idx][num_elems/2.:num_elems*3./4.]\n",
    "                \n",
    "                clf1 = LambdaMART(L, alpha=a, beta=b).fit(X_train, y_train, q_train, queries_train)\n",
    "                y_pred1 = clf1.predict(X_valid, q_valid, queries_valid)\n",
    "                results1[i_a, i_L, i_b] += ndcgl(y_pred1, y_valid) / 5.\n",
    "\n",
    "                clf2 = LambdaMART(L, alpha=a, beta=b, adaptive_step=False).fit(X_train, y_train, q_train, queries_train)\n",
    "                y_pred2 = clf2.predict(X_valid, q_valid, queries_valid)\n",
    "                results2[i_a, i_L, i_b] += ndcgl(y_pred2, y_valid) / 5.\n",
    "\n",
    "                # Значения параметров и результаты с адаптивным шагом и без\n",
    "            print a, L, b, results1[i_a, i_L, i_b], results2[i_a, i_L, i_b]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max with adaptive step - Alpha=0.5, L=7, beta=1.0\n",
      "Max without adaptive step - Alpha=2.0, L=7, beta=2.0\n",
      "Maximum with adaptive step= 0.468726043507\n",
      "Maximum without adaptive step= 0.394820552599\n"
     ]
    }
   ],
   "source": [
    "ad_max = unravel_index(np.argmax(results1), results1.shape)\n",
    "non_ad_max = unravel_index(np.argmax(results2), results2.shape)\n",
    "        \n",
    "print \"Max with adaptive step - Alpha={}, L={}, beta={}\".format(arr_a[ad_max[0]], arr_L[ad_max[1]], arr_b[ad_max[2]])\n",
    "print \"Max without adaptive step - Alpha={}, L={}, beta={}\".format(arr_a[non_ad_max[0]], arr_L[non_ad_max[1]], arr_b[non_ad_max[2]])\n",
    "\n",
    "print \"Maximum with adaptive step=\", np.max(results1)\n",
    "print \"Maximum without adaptive step=\", np.max(results2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "В принципе, из списка на всех параметрах видны корреляции для адаптивного и неадаптивного методов. И как правило, увеличение числа итераций хорошо влияет на результат. Можно было продолжить и далее, но в виду ограниченных мощностей, пока это не главное."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adaptive= 0.473884366146\n",
      "Non-adaptive= 0.328375116338\n"
     ]
    }
   ],
   "source": [
    "result1 = 0.\n",
    "result2 = 0.\n",
    "\n",
    "for k in xrange(5):\n",
    "    shuffle_idx = np.random.permutation(num_elems)\n",
    "    X_train = X[shuffle_idx][:num_elems/2.]\n",
    "    X_valid = X[shuffle_idx][num_elems/2.:num_elems*3./4.]\n",
    "    X_test = X[shuffle_idx][num_elems*3./4.:]\n",
    "\n",
    "    y_train = y[shuffle_idx][:num_elems/2.]\n",
    "    y_valid = y[shuffle_idx][num_elems/2.:num_elems*3./4.]\n",
    "    y_test = y[shuffle_idx][num_elems*3./4.:]\n",
    "\n",
    "    q_train = queries[shuffle_idx][:num_elems/2.]\n",
    "    q_valid = queries[shuffle_idx][num_elems/2.:num_elems*3./4.]\n",
    "    q_test = queries[shuffle_idx][num_elems*3./4.:]\n",
    "\n",
    "    queries_train = q[shuffle_idx][:num_elems/2.]\n",
    "    queries_valid = q[shuffle_idx][num_elems/2.:num_elems*3./4.]\n",
    "    queries_test = q[shuffle_idx][num_elems*3./4.:]\n",
    "    \n",
    "    X_full_train = np.append(X_train, X_valid, axis=0)\n",
    "    y_full_train = np.append(y_train, y_valid)\n",
    "    q_full_train = np.append(q_train, q_valid)\n",
    "    queries_full_train = np.append(queries_train, queries_valid, axis=0)\n",
    "    \n",
    "    clf1 = LambdaMART(7, alpha=0.5, beta=1.).fit(X_full_train, y_full_train, q_full_train, queries_full_train)\n",
    "    y_pred1 = clf1.predict(X_test, q_test, queries_test)\n",
    "\n",
    "    clf2 = LambdaMART(7, alpha=2.0, beta=2.0, adaptive_step=False).fit(X_full_train, y_full_train, q_full_train, queries_full_train)\n",
    "    y_pred2 = clf2.predict(X_test, q_test, queries_test)\n",
    "    \n",
    "    result1 += ndcgl(y_pred1, y_test) / 5.\n",
    "    result2 += ndcgl(y_pred2, y_test) / 5.\n",
    "\n",
    "print \"Adaptive=\", result1\n",
    "print \"Non-adaptive=\", result2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.349995209906\n"
     ]
    }
   ],
   "source": [
    "# Обычный DecisionTreeRegressor\n",
    "tempresult = 0.\n",
    "\n",
    "for k in xrange(5):\n",
    "    shuffle_idx = np.random.permutation(num_elems)\n",
    "    X_train = X[shuffle_idx][:num_elems/2.]\n",
    "    X_valid = X[shuffle_idx][num_elems/2.:num_elems*3./4.]\n",
    "    X_test = X[shuffle_idx][num_elems*3./4.:]\n",
    "\n",
    "    y_train = y[shuffle_idx][:num_elems/2.]\n",
    "    y_valid = y[shuffle_idx][num_elems/2.:num_elems*3./4.]\n",
    "    y_test = y[shuffle_idx][num_elems*3./4.:]\n",
    "    \n",
    "    X_full_train = np.append(X_train, X_valid, axis=0)\n",
    "    y_full_train = np.append(y_train, y_valid)\n",
    "\n",
    "    tempclf = DecisionTreeRegressor().fit(X_full_train, y_full_train)\n",
    "    tempresult += ndcgl(tempclf.predict(X_test), y_test) / 5.\n",
    "    \n",
    "print tempresult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.424421739725\n"
     ]
    }
   ],
   "source": [
    "# GDBT\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "tempresult = 0.\n",
    "\n",
    "for k in xrange(5):\n",
    "    shuffle_idx = np.random.permutation(num_elems)\n",
    "    X_train = X[shuffle_idx][:num_elems/2.]\n",
    "    X_valid = X[shuffle_idx][num_elems/2.:num_elems*3./4.]\n",
    "    X_test = X[shuffle_idx][num_elems*3./4.:]\n",
    "\n",
    "    y_train = y[shuffle_idx][:num_elems/2.]\n",
    "    y_valid = y[shuffle_idx][num_elems/2.:num_elems*3./4.]\n",
    "    y_test = y[shuffle_idx][num_elems*3./4.:]\n",
    "    \n",
    "    X_full_train = np.append(X_train, X_valid, axis=0)\n",
    "    y_full_train = np.append(y_train, y_valid)\n",
    "\n",
    "    tempclf = GradientBoostingRegressor().fit(X_full_train, y_full_train)\n",
    "    tempresult += ndcgl(tempclf.predict(X_test), y_test) / 5.\n",
    "    \n",
    "print tempresult"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LambdaMART из XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Необходимые технические строки. Исполняйте их в случае ошибки WindowsError Error 127. \n",
    "# Путь заменить на своё расположение mingw-64\n",
    "dir = r'C:\\Program Files\\mingw-w64\\x86_64-6.2.0-posix-seh-rt_v5-rev1\\mingw64\\bin'\n",
    "import os\n",
    "\n",
    "os.environ['PATH'].find(dir)\n",
    "os.environ['PATH'] = dir + ';' + os.environ['PATH']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.147214344763\n",
      "0.410943681227\n"
     ]
    }
   ],
   "source": [
    "clf_temp = xgb.XGBClassifier(objective='rank:ndcg')\n",
    "clf_temp.fit(X_train, y_train)\n",
    "print ndcgl(clf_temp.predict(X_test), y_test, q_test)\n",
    "print ndcgl(clf_temp.predict(X_train), y_train, q_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.434840804088\n"
     ]
    }
   ],
   "source": [
    "tempresult = 0.\n",
    "\n",
    "for k in xrange(5):\n",
    "    shuffle_idx = np.random.permutation(num_elems)\n",
    "    X_train = X[shuffle_idx][:num_elems/2.]\n",
    "    X_valid = X[shuffle_idx][num_elems/2.:num_elems*3./4.]\n",
    "    X_test = X[shuffle_idx][num_elems*3./4.:]\n",
    "\n",
    "    y_train = y[shuffle_idx][:num_elems/2.]\n",
    "    y_valid = y[shuffle_idx][num_elems/2.:num_elems*3./4.]\n",
    "    y_test = y[shuffle_idx][num_elems*3./4.:]\n",
    "    \n",
    "    X_full_train = np.append(X_train, X_valid, axis=0)\n",
    "    y_full_train = np.append(y_train, y_valid)\n",
    "    \n",
    "    clf = xgb.XGBClassifier(objective='rank:ndcg')\n",
    "    clf.fit(X_full_train,y_full_train)\n",
    "    tempresult += ndcgl(clf.predict(X_test), y_test) / 5.\n",
    "    \n",
    "print tempresult"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Работа на 4000 элементах, 70 итераций бустинга"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration # 0\n",
      "Iteration # 1\n",
      "Iteration # 2\n",
      "Iteration # 3\n",
      "Iteration # 4\n",
      "Iteration # 5\n",
      "Iteration # 6\n",
      "Iteration # 7\n",
      "Iteration # 8\n",
      "Iteration # 9\n",
      "Iteration # 10\n",
      "Iteration # 11\n",
      "Iteration # 12\n",
      "Iteration # 13\n",
      "Iteration # 14\n",
      "Iteration # 15\n",
      "Iteration # 16\n",
      "Iteration # 17\n",
      "Iteration # 18\n",
      "Iteration # 19\n",
      "Iteration # 20\n",
      "Iteration # 21\n",
      "Iteration # 22\n",
      "Iteration # 23\n",
      "Iteration # 24\n",
      "Iteration # 25\n",
      "Iteration # 26\n",
      "Iteration # 27\n",
      "Iteration # 28\n",
      "Iteration # 29\n",
      "Iteration # 30\n",
      "Iteration # 31\n",
      "Iteration # 32\n",
      "Iteration # 33\n",
      "Iteration # 34\n",
      "Iteration # 35\n",
      "Iteration # 36\n",
      "Iteration # 37\n",
      "Iteration # 38\n",
      "Iteration # 39\n",
      "Iteration # 40\n",
      "Iteration # 41\n",
      "Iteration # 42\n",
      "Iteration # 43\n",
      "Iteration # 44\n",
      "Iteration # 45\n",
      "Iteration # 46\n",
      "Iteration # 47\n",
      "Iteration # 48\n",
      "Iteration # 49\n",
      "Iteration # 50\n",
      "Iteration # 51\n",
      "Iteration # 52\n",
      "Iteration # 53\n",
      "Iteration # 54\n",
      "Iteration # 55\n",
      "Iteration # 56\n",
      "Iteration # 57\n",
      "Iteration # 58\n",
      "Iteration # 59\n",
      "Iteration # 60\n",
      "Iteration # 61\n",
      "Iteration # 62\n",
      "Iteration # 63\n",
      "Iteration # 64\n",
      "Iteration # 65\n",
      "Iteration # 66\n",
      "Iteration # 67\n",
      "Iteration # 68\n",
      "Iteration # 69\n",
      "Wall time: 3h 41min 39s\n"
     ]
    }
   ],
   "source": [
    "shuffle_idx = np.random.permutation(num_elems)\n",
    "X_train = X[shuffle_idx][:num_elems/2.]\n",
    "X_valid = X[shuffle_idx][num_elems/2.:num_elems*3./4.]\n",
    "X_test = X[shuffle_idx][num_elems*3./4.:]\n",
    "\n",
    "y_train = y[shuffle_idx][:num_elems/2.]\n",
    "y_valid = y[shuffle_idx][num_elems/2.:num_elems*3./4.]\n",
    "y_test = y[shuffle_idx][num_elems*3./4.:]\n",
    "\n",
    "q_train = queries[shuffle_idx][:num_elems/2.]\n",
    "q_valid = queries[shuffle_idx][num_elems/2.:num_elems*3./4.]\n",
    "q_test = queries[shuffle_idx][num_elems*3./4.:]\n",
    "\n",
    "queries_train = q[shuffle_idx][:num_elems/2.]\n",
    "queries_valid = q[shuffle_idx][num_elems/2.:num_elems*3./4.]\n",
    "queries_test = q[shuffle_idx][num_elems*3./4.:]\n",
    "\n",
    "X_full_train = np.append(X_train, X_valid, axis=0)\n",
    "y_full_train = np.append(y_train, y_valid)\n",
    "q_full_train = np.append(q_train, q_valid)\n",
    "queries_full_train = np.append(queries_train, queries_valid, axis=0)\n",
    "\n",
    "%time clf_big = LambdaMART(70, alpha=0.5, beta=1., ).fit(X_full_train, y_full_train, q_full_train, queries_full_train)\n",
    "#y_pred1 = clf1.predict(X_test, q_test, queries_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.896477581668\n"
     ]
    }
   ],
   "source": [
    "y_pred_big = clf_big.predict(X_test, q_test, queries_test)\n",
    "print ndcgl(y_pred_big, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.733113475428\n"
     ]
    }
   ],
   "source": [
    "# xgboost\n",
    "clf = xgb.XGBClassifier(objective='rank:ndcg')\n",
    "clf.fit(X_full_train,y_full_train)\n",
    "print ndcgl(clf.predict(X_test), y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Проверка случайного выбора фичей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "Without subset 0.589559508361\n",
      "With subset 0.525202845018\n",
      "Stochastic 0.465555208682\n",
      "40\n",
      "Without subset 0.670012899467\n",
      "With subset 0.851643743565\n",
      "Stochastic 0.79434887815\n",
      "70\n",
      "Without subset 0.595825485951\n",
      "With subset 0.837945450273\n",
      "Stochastic 0.732566107838\n",
      "100\n",
      "Without subset 0.625597894605\n",
      "With subset 0.805930029937\n",
      "Stochastic 0.621950881675\n",
      "130\n",
      "Without subset 0.635118472228\n",
      "With subset 0.838777861945\n",
      "Stochastic 0.646343802709\n",
      "160\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-ea5e83d9c0ab>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[0mclf1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLambdaMART\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbeta\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1.\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_full_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_full_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mq_full_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mqueries_full_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m     \u001b[0mclf2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLambdaMART\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbeta\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1.\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeature_subset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeature_fraction\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_full_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_full_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mq_full_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mqueries_full_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m     \u001b[0mclf3\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLambdaMART\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbeta\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1.\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeature_subset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeature_fraction\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.3\u001b[0m\u001b[1;33m,\u001b[0m                       \u001b[0mstochastic\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_full_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_full_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mq_full_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mqueries_full_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-6943b8d58004>\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X_train, y_train, qids_train, queries_train, verbose)\u001b[0m\n\u001b[0;32m     52\u001b[0m                 \u001b[0mqueries\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mqueries_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msub_idx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m                 \u001b[0mgrad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredicted\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mid_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     55\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m                 \u001b[1;32mprint\u001b[0m \u001b[1;34m\"Grad:\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-6943b8d58004>\u001b[0m in \u001b[0;36mloss_grad\u001b[1;34m(self, pred_y, y, id_y)\u001b[0m\n\u001b[0;32m    118\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mxrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_elems\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m                 \u001b[0mbuf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 120\u001b[1;33m                 \u001b[0mdelta_ndcg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndcg_replace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mid_y\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mid_y\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    121\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m                     \u001b[0mbuf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrho\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred_y\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpred_y\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-6943b8d58004>\u001b[0m in \u001b[0;36mndcg_replace\u001b[1;34m(self, value_a, value_b, id_a, id_b)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    141\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mndcg_replace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue_a\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue_b\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mid_a\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mid_b\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 142\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m2.\u001b[0m \u001b[1;33m**\u001b[0m \u001b[0mvalue_b\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m2.\u001b[0m \u001b[1;33m**\u001b[0m \u001b[0mvalue_a\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1.\u001b[0m\u001b[1;33m/\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mid_a\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1.\u001b[0m\u001b[1;33m/\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mid_b\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    143\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    144\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mrho\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "shuffle_idx = np.random.permutation(num_elems)\n",
    "X_train = X[shuffle_idx][:num_elems/2.]\n",
    "X_valid = X[shuffle_idx][num_elems/2.:num_elems*3./4.]\n",
    "X_test = X[shuffle_idx][num_elems*3./4.:]\n",
    "\n",
    "y_train = y[shuffle_idx][:num_elems/2.]\n",
    "y_valid = y[shuffle_idx][num_elems/2.:num_elems*3./4.]\n",
    "y_test = y[shuffle_idx][num_elems*3./4.:]\n",
    "\n",
    "q_train = queries[shuffle_idx][:num_elems/2.]\n",
    "q_valid = queries[shuffle_idx][num_elems/2.:num_elems*3./4.]\n",
    "q_test = queries[shuffle_idx][num_elems*3./4.:]\n",
    "\n",
    "queries_train = q[shuffle_idx][:num_elems/2.]\n",
    "queries_valid = q[shuffle_idx][num_elems/2.:num_elems*3./4.]\n",
    "queries_test = q[shuffle_idx][num_elems*3./4.:]\n",
    "\n",
    "X_full_train = np.append(X_train, X_valid, axis=0)\n",
    "y_full_train = np.append(y_train, y_valid)\n",
    "q_full_train = np.append(q_train, q_valid)\n",
    "queries_full_train = np.append(queries_train, queries_valid, axis=0)\n",
    "\n",
    "results = []\n",
    "\n",
    "for n in xrange(10, 600, 30):\n",
    "    print n\n",
    "\n",
    "    clf1 = LambdaMART(n, alpha=0.5, beta=1.).fit(X_full_train, y_full_train, q_full_train, queries_full_train)\n",
    "    clf2 = LambdaMART(n, alpha=0.5, beta=1., feature_subset=True, feature_fraction=0.3).fit(X_full_train, y_full_train, q_full_train, queries_full_train)\n",
    "    clf3 = LambdaMART(n, alpha=0.5, beta=1., feature_subset=True, feature_fraction=0.3, \\\n",
    "                      stochastic=True).fit(X_full_train, y_full_train, q_full_train, queries_full_train)\n",
    "    \n",
    "    result1 = ndcgl(clf1.predict(X_test, q_test, queries_test), y_test)\n",
    "    result2 = ndcgl(clf2.predict(X_test, q_test, queries_test), y_test)\n",
    "    result3 = ndcgl(clf3.predict(X_test, q_test, queries_test), y_test)\n",
    "    results.append((result1, result2, result3))\n",
    "    \n",
    "    print \"Without subset\", result1\n",
    "    print \"With subset\", result2\n",
    "    print \"Stochastic\", result3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.619296498687\n",
      "0.446995121205\n"
     ]
    }
   ],
   "source": [
    "print ndcgl(clf1.predict(X_test, q_test, queries_test), y_test)\n",
    "print ndcgl(clf2.predict(X_test, q_test, queries_test), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.58955950836136284, 0.52520284501832992, 0.46555520868212591),\n",
       " (0.67001289946652387, 0.85164374356483075, 0.79434887815035293),\n",
       " (0.59582548595062079, 0.83794545027274325, 0.73256610783800979),\n",
       " (0.62559789460458759, 0.80593002993736984, 0.6219508816753645),\n",
       " (0.63511847222811579, 0.83877786194521575, 0.64634380270875258)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
