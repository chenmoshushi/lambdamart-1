{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class LambdaMART:\n",
    "    def __init__(self, n_estimators, base_estimator=DecisionTreeRegressor(), step_estimator=DecisionTreeRegressor(),\n",
    "                 alpha=1., beta=1., adaptive_step=True):\n",
    "        \"\"\"\n",
    "            n_estimators : число деревьев в обучении\n",
    "            base_estimator : выбор модели для каждой итерации\n",
    "            step_estimator : выбор модели для предсказания темпа обучения\n",
    "            alpha : коэффициент регуляризации XGBoost\n",
    "            beta : коэффициент при предсказании темпа обучения\n",
    "            adaptive_step : использовать адаптивный шаг бустинга или нет\n",
    "        \"\"\"\n",
    "        self.estimators = []\n",
    "        self.step_estimators = []\n",
    "        self.n_estimators = n_estimators\n",
    "        self.base_estimator = base_estimator\n",
    "        self.step_estimator = step_estimator\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.adaptive_step = adaptive_step\n",
    "        #self.q_dict = defaultdict(lambda : np.zeros(query_d))\n",
    "        #self.query_d = query_d\n",
    "        \n",
    "    def fit(self, X, y, qids, queries, verbose=False):\n",
    "        \"\"\"\n",
    "            X : признаки пар запрос-документ\n",
    "            y : метки релевантности\n",
    "            qids : id запросов\n",
    "            queries : признаки запросов\n",
    "        \"\"\"\n",
    "        self.estimators = []\n",
    "        self.step_estimators = []\n",
    "        for i in xrange(self.n_estimators):\n",
    "            predicted = self.predict(X, qids, queries, verbose)\n",
    "            grad = self.loss_grad(predicted, y)\n",
    "            h = self.double_grad(predicted, y)\n",
    "            if verbose:\n",
    "                print \"Grad:\"\n",
    "                print grad\n",
    "                print \"H:\"\n",
    "                print h\n",
    "            estimator = deepcopy(self.base_estimator)\n",
    "            estimator.fit(X, -grad/ (self.alpha + h))\n",
    "            self.estimators.append(estimator)\n",
    "            if verbose:\n",
    "                print \"Predict:\"\n",
    "                print predicted\n",
    "            # Обучим предсказатель шага\n",
    "            if self.adaptive_step:\n",
    "                step_estimator = deepcopy(self.step_estimator)\n",
    "                # Список всех запросов(уникальных)\n",
    "                qs = np.unique(qids)\n",
    "                # Список значений того, что нужно предсказывать\n",
    "                pred_values = np.zeros(len(qs))\n",
    "                q_list = np.zeros((len(qs), queries.shape[1]))\n",
    "                for idx, q in enumerate(qs):\n",
    "                    # Суммируем значения по всем элементам с данным запросом q\n",
    "                    pred_values[idx] = 0.\n",
    "                    for j in xrange(X.shape[0]):\n",
    "                        if qids[j] == q:\n",
    "                            q_list[idx] = queries[j]\n",
    "                            pred_values[idx] += 1. / (1. + self.beta * h[j])            \n",
    "                    #q_list[idx] = q_dict[q]\n",
    "                sum_pred_values = np.sum(pred_values)\n",
    "                step_estimator.fit(q_list, pred_values * len(qs) / sum_pred_values)\n",
    "                self.step_estimators.append(step_estimator)\n",
    "        return self\n",
    "            \n",
    "    def predict(self, X, qids, queries, verbose=False):\n",
    "        y_pred = np.zeros(X.shape[0])\n",
    "        for est_i, estimator in enumerate(self.estimators):\n",
    "            est_result = estimator.predict(X)\n",
    "            for i in xrange(X.shape[0]):\n",
    "                step = 1. / (i + 1.)\n",
    "                if self.adaptive_step:\n",
    "                    step = self.step_estimators[est_i].predict(queries[i].reshape(1,-1)[0])\n",
    "                if verbose==True:\n",
    "                    print \"Step: \", step, \" for qid=\", qids[i]\n",
    "                y_pred[i] += est_result[i] * step\n",
    "        return y_pred\n",
    "    \n",
    "    def loss_grad(self, pred_y, y):\n",
    "        n_elems = y.shape[0]\n",
    "        grad = np.zeros(n_elems)\n",
    "        # id_y - индексы в массиве pred_y, отсортированные по убыванию ранжирующей функции \n",
    "        id_y = np.argsort(np.argsort(y)[::-1])\n",
    "        for i in xrange(n_elems):\n",
    "            for j in xrange(n_elems):\n",
    "                buf = 0.\n",
    "                if y[i] > y[j]:\n",
    "                    buf = -self.rho(pred_y[i], pred_y[j])\n",
    "                if y[i] < y[j]:\n",
    "                    buf = self.rho(pred_y[j], pred_y[i])\n",
    "                if buf != 0.:\n",
    "                    grad[i] += np.abs(self.ndcg_replace(y[i], y[j], id_y[i], id_y[j])) * buf\n",
    "        return grad\n",
    "        \n",
    "    def double_grad(self, pred_y, y):\n",
    "        n_elems = y.shape[0]\n",
    "        h = np.zeros(n_elems)\n",
    "        # id_y - индексы в массиве pred_y, отсортированные по убыванию ранжирующей функции \n",
    "        id_y = np.argsort(np.argsort(y)[::-1])\n",
    "        for i in xrange(n_elems):\n",
    "            for j in xrange(n_elems):\n",
    "                delta_ndcg = np.abs(self.ndcg_replace(y[i], y[j], id_y[i], id_y[j]))\n",
    "                h[i] += delta_ndcg * self.rho(pred_y[i], pred_y[j]) * (1 - self.rho(pred_y[i], pred_y[j]))\n",
    "        return h\n",
    "                \n",
    "    def ndcg_replace(self, value_a, value_b, id_a, id_b):\n",
    "        return (2. ** value_b - 2. ** value_a) * (1./ np.log2(2 + id_a) - 1./ np.log2(2 + id_b))\n",
    "    \n",
    "    def rho(self, a, b):\n",
    "        return 1. / (1. + np.exp(a - b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Считаем часть датасета MQ2007"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Проверка на части датасета MQ2007\n",
    "fin = open('train.txt', 'r')\n",
    "\n",
    "num_elems = 500\n",
    "X = np.zeros((num_elems, 46))\n",
    "y = np.zeros(num_elems)\n",
    "queries = np.zeros(num_elems)\n",
    "\n",
    "num = 0\n",
    "i = 0\n",
    "\n",
    "lines = fin.readlines()[:num_elems]\n",
    "\n",
    "for i, line in enumerate(lines):\n",
    "    parsed = line.split(' ')\n",
    "    y[i] = int(parsed[0])\n",
    "    queries[i] = int(parsed[1][4:])\n",
    "    for j in xrange(46):\n",
    "        if j < 9:\n",
    "            X[i][j] = float(parsed[j + 2][2:])\n",
    "        else:\n",
    "            X[i][j] = float(parsed[j + 2][3:])\n",
    "                \n",
    "# Нормализуем y\n",
    "y /= np.max(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Генерируем признаки для запросов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# Признаки для запросов\n",
    "query_d = 20 # Число признаков в запросе. Берём 0-4, 10-14 признаки пары и усредняем\n",
    "q_dict = defaultdict(lambda : np.zeros(query_d)) # dictionary, который по id запроса возвращает его признаки\n",
    "\n",
    "unique_q, counts = np.unique(queries, return_counts=True)\n",
    "for i,q in enumerate(unique_q):\n",
    "    for j in xrange(X.shape[0]):\n",
    "        if queries[j] == q:\n",
    "            q_dict[q] += X[j][11:31]/ counts[i]\n",
    "\n",
    "# Финальная матрица для запросов. По индексу получаем признаки запроса, соответствующие элементу из X с этим индексом\n",
    "q = np.zeros((num_elems, query_d))\n",
    "for i in xrange(num_elems):\n",
    "    q[i] = q_dict[queries[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ndcgl(y_pred, y, L=10):\n",
    "    \"\"\"\n",
    "        y_pred : предсказанные значения функции ранжирования\n",
    "        y : метки релевантности\n",
    "    \"\"\"\n",
    "    dcgl = 0.\n",
    "    idcgl = 0.\n",
    "    # Отсортируем значения по убыванию функции ранжирования\n",
    "    idx = np.argsort(y_pred)[::-1]\n",
    "    # Метки релевантности для отсортированного массива y_pred\n",
    "    l = y[idx]\n",
    "    for i in xrange(L):\n",
    "        dcgl += (2. ** l[i] - 1) / np.log2(i + 2)\n",
    "        idcgl += 1 / np.log2(i + 2)\n",
    "    return dcgl / idcgl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5 1 0.5 0.336099646865 0.334095124305\n",
      "0.5 1 1.0 0.284810928463 0.336022586491\n",
      "0.5 1 2.0 0.278983133131 0.235084867792\n",
      "0.5 2 0.5 0.41276830698 0.325806649599\n",
      "0.5 2 1.0 0.269452898252 0.219380711384\n",
      "0.5 2 2.0 0.357579678573 0.328906145272\n",
      "0.5 5 0.5 0.360515329939 0.291737452324\n",
      "0.5 5 1.0 0.396132881545 0.352325317071\n",
      "0.5 5 2.0 0.357491363348 0.307072058994\n",
      "0.5 7 0.5 0.307196757893 0.243569464569\n",
      "0.5 7 1.0 0.468726043507 0.381784322914\n",
      "0.5 7 2.0 0.433379695178 0.35421116786\n",
      "1.0 1 0.5 0.298903738381 0.329808291355\n",
      "1.0 1 1.0 0.452061919927 0.348656410939\n",
      "1.0 1 2.0 0.43510791712 0.279133197285\n",
      "1.0 2 0.5 0.298245437817 0.270975813759\n",
      "1.0 2 1.0 0.426176753028 0.298526360796\n",
      "1.0 2 2.0 0.30197343755 0.296332263381\n",
      "1.0 5 0.5 0.444450124696 0.334779000506\n",
      "1.0 5 1.0 0.431277962121 0.373842327464\n",
      "1.0 5 2.0 0.429456655432 0.344643182745\n",
      "1.0 7 0.5 0.438494402494 0.358339454444\n",
      "1.0 7 1.0 0.431007588023 0.367884660402\n",
      "1.0 7 2.0 0.446516329361 0.336559264488\n",
      "2.0 1 0.5 0.259869836501 0.210006019129\n",
      "2.0 1 1.0 0.405543147091 0.281986651447\n",
      "2.0 1 2.0 0.313821884084 0.268496532276\n",
      "2.0 2 0.5 0.458445432413 0.350849454051\n",
      "2.0 2 1.0 0.257580355035 0.30694327258\n",
      "2.0 2 2.0 0.333093525779 0.259259257385\n",
      "2.0 5 0.5 0.34329521534 0.222004082956\n",
      "2.0 5 1.0 0.34956759061 0.263642761974\n",
      "2.0 5 2.0 0.345498792654 0.35035288334\n",
      "2.0 7 0.5 0.442491213356 0.37117135548\n",
      "2.0 7 1.0 0.425625394444 0.319880540672\n",
      "2.0 7 2.0 0.434023061723 0.394820552599\n"
     ]
    }
   ],
   "source": [
    "# Небольшой GridSearch с кросс-валидацией\n",
    "from numpy import unravel_index\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "arr_a = [0.5, 1., 2.] # Одно значение в целях оптимизации\n",
    "arr_L = [1, 2, 5, 7]\n",
    "arr_b = [0.5, 1., 2.] # Одно значение в целях оптимизации\n",
    "\n",
    "results1 = np.zeros((len(arr_a), len(arr_L), len(arr_b))) # С адаптивным шагом\n",
    "results2 = np.zeros((len(arr_a), len(arr_L), len(arr_b))) # Без адаптивного шага\n",
    "\n",
    "for i_a, a in enumerate(arr_a):\n",
    "    for i_L, L in enumerate(arr_L):\n",
    "        for i_b, b in enumerate(arr_b):\n",
    "            for k in xrange(5):\n",
    "                # Перемешаем\n",
    "                shuffle_idx = np.random.permutation(num_elems)\n",
    "                X_train = X[shuffle_idx][:num_elems/2.]\n",
    "                X_valid = X[shuffle_idx][num_elems/2.:num_elems*3./4.]\n",
    "\n",
    "                y_train = y[shuffle_idx][:num_elems/2.]\n",
    "                y_valid = y[shuffle_idx][num_elems/2.:num_elems*3./4.]\n",
    "\n",
    "                q_train = queries[shuffle_idx][:num_elems/2.]\n",
    "                q_valid = queries[shuffle_idx][num_elems/2.:num_elems*3./4.]\n",
    "\n",
    "                queries_train = q[shuffle_idx][:num_elems/2.]\n",
    "                queries_valid = q[shuffle_idx][num_elems/2.:num_elems*3./4.]\n",
    "                \n",
    "                clf1 = LambdaMART(L, alpha=a, beta=b).fit(X_train, y_train, q_train, queries_train)\n",
    "                y_pred1 = clf1.predict(X_valid, q_valid, queries_valid)\n",
    "                results1[i_a, i_L, i_b] += ndcgl(y_pred1, y_valid) / 5.\n",
    "\n",
    "                clf2 = LambdaMART(L, alpha=a, beta=b, adaptive_step=False).fit(X_train, y_train, q_train, queries_train)\n",
    "                y_pred2 = clf2.predict(X_valid, q_valid, queries_valid)\n",
    "                results2[i_a, i_L, i_b] += ndcgl(y_pred2, y_valid) / 5.\n",
    "\n",
    "                # Значения параметров и результаты с адаптивным шагом и без\n",
    "            print a, L, b, results1[i_a, i_L, i_b], results2[i_a, i_L, i_b]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max with adaptive step - Alpha=0.5, L=7, beta=1.0\n",
      "Max without adaptive step - Alpha=2.0, L=7, beta=2.0\n",
      "Maximum with adaptive step= 0.468726043507\n",
      "Maximum without adaptive step= 0.394820552599\n"
     ]
    }
   ],
   "source": [
    "ad_max = unravel_index(np.argmax(results1), results1.shape)\n",
    "non_ad_max = unravel_index(np.argmax(results2), results2.shape)\n",
    "        \n",
    "print \"Max with adaptive step - Alpha={}, L={}, beta={}\".format(arr_a[ad_max[0]], arr_L[ad_max[1]], arr_b[ad_max[2]])\n",
    "print \"Max without adaptive step - Alpha={}, L={}, beta={}\".format(arr_a[non_ad_max[0]], arr_L[non_ad_max[1]], arr_b[non_ad_max[2]])\n",
    "\n",
    "print \"Maximum with adaptive step=\", np.max(results1)\n",
    "print \"Maximum without adaptive step=\", np.max(results2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "На выборке из 1000 элементов видно, что 5 деревьев - оптимально. А вообще в целом, при увеличении числа деревьев адаптивный шаг даёт более хороший результат. Проверим эти параметры на тестовой выборке"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adaptive= 0.473884366146\n",
      "Non-adaptive= 0.328375116338\n"
     ]
    }
   ],
   "source": [
    "result1 = 0.\n",
    "result2 = 0.\n",
    "\n",
    "for k in xrange(5):\n",
    "    shuffle_idx = np.random.permutation(num_elems)\n",
    "    X_train = X[shuffle_idx][:num_elems/2.]\n",
    "    X_valid = X[shuffle_idx][num_elems/2.:num_elems*3./4.]\n",
    "    X_test = X[shuffle_idx][num_elems*3./4.:]\n",
    "\n",
    "    y_train = y[shuffle_idx][:num_elems/2.]\n",
    "    y_valid = y[shuffle_idx][num_elems/2.:num_elems*3./4.]\n",
    "    y_test = y[shuffle_idx][num_elems*3./4.:]\n",
    "\n",
    "    q_train = queries[shuffle_idx][:num_elems/2.]\n",
    "    q_valid = queries[shuffle_idx][num_elems/2.:num_elems*3./4.]\n",
    "    q_test = queries[shuffle_idx][num_elems*3./4.:]\n",
    "\n",
    "    queries_train = q[shuffle_idx][:num_elems/2.]\n",
    "    queries_valid = q[shuffle_idx][num_elems/2.:num_elems*3./4.]\n",
    "    queries_test = q[shuffle_idx][num_elems*3./4.:]\n",
    "    \n",
    "    X_full_train = np.append(X_train, X_valid, axis=0)\n",
    "    y_full_train = np.append(y_train, y_valid)\n",
    "    q_full_train = np.append(q_train, q_valid)\n",
    "    queries_full_train = np.append(queries_train, queries_valid, axis=0)\n",
    "    \n",
    "    clf1 = LambdaMART(7, alpha=0.5, beta=1.).fit(X_full_train, y_full_train, q_full_train, queries_full_train)\n",
    "    y_pred1 = clf1.predict(X_test, q_test, queries_test)\n",
    "\n",
    "    clf2 = LambdaMART(7, alpha=2.0, beta=2.0, adaptive_step=False).fit(X_full_train, y_full_train, q_full_train, queries_full_train)\n",
    "    y_pred2 = clf2.predict(X_test, q_test, queries_test)\n",
    "    \n",
    "    result1 += ndcgl(y_pred1, y_test) / 5.\n",
    "    result2 += ndcgl(y_pred2, y_test) / 5.\n",
    "\n",
    "print \"Adaptive=\", result1\n",
    "print \"Non-adaptive=\", result2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.349995209906\n"
     ]
    }
   ],
   "source": [
    "# Обычный DecisionTreeRegressor\n",
    "tempresult = 0.\n",
    "\n",
    "for k in xrange(5):\n",
    "    shuffle_idx = np.random.permutation(num_elems)\n",
    "    X_train = X[shuffle_idx][:num_elems/2.]\n",
    "    X_valid = X[shuffle_idx][num_elems/2.:num_elems*3./4.]\n",
    "    X_test = X[shuffle_idx][num_elems*3./4.:]\n",
    "\n",
    "    y_train = y[shuffle_idx][:num_elems/2.]\n",
    "    y_valid = y[shuffle_idx][num_elems/2.:num_elems*3./4.]\n",
    "    y_test = y[shuffle_idx][num_elems*3./4.:]\n",
    "    \n",
    "    X_full_train = np.append(X_train, X_valid, axis=0)\n",
    "    y_full_train = np.append(y_train, y_valid)\n",
    "\n",
    "    tempclf = DecisionTreeRegressor().fit(X_full_train, y_full_train)\n",
    "    tempresult += ndcgl(tempclf.predict(X_test), y_test) / 5.\n",
    "    \n",
    "print tempresult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.424421739725\n"
     ]
    }
   ],
   "source": [
    "# GDBT\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "tempresult = 0.\n",
    "\n",
    "for k in xrange(5):\n",
    "    shuffle_idx = np.random.permutation(num_elems)\n",
    "    X_train = X[shuffle_idx][:num_elems/2.]\n",
    "    X_valid = X[shuffle_idx][num_elems/2.:num_elems*3./4.]\n",
    "    X_test = X[shuffle_idx][num_elems*3./4.:]\n",
    "\n",
    "    y_train = y[shuffle_idx][:num_elems/2.]\n",
    "    y_valid = y[shuffle_idx][num_elems/2.:num_elems*3./4.]\n",
    "    y_test = y[shuffle_idx][num_elems*3./4.:]\n",
    "    \n",
    "    X_full_train = np.append(X_train, X_valid, axis=0)\n",
    "    y_full_train = np.append(y_train, y_valid)\n",
    "\n",
    "    tempclf = GradientBoostingRegressor().fit(X_full_train, y_full_train)\n",
    "    tempresult += ndcgl(tempclf.predict(X_test), y_test) / 5.\n",
    "    \n",
    "print tempresult"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LambdaMART из XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Необходимые технические строки. Исполняйте их в случае ошибки WindowsError Error 127. \n",
    "# Путь заменить на своё расположение mingw-64\n",
    "dir = r'C:\\Program Files\\mingw-w64\\x86_64-6.2.0-posix-seh-rt_v5-rev1\\mingw64\\bin'\n",
    "import os\n",
    "\n",
    "os.environ['PATH'].find(dir)\n",
    "os.environ['PATH'] = dir + ';' + os.environ['PATH']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kirill\\Anaconda2\\lib\\site-packages\\sklearn\\cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.434840804088\n"
     ]
    }
   ],
   "source": [
    "tempresult = 0.\n",
    "\n",
    "for k in xrange(5):\n",
    "    shuffle_idx = np.random.permutation(num_elems)\n",
    "    X_train = X[shuffle_idx][:num_elems/2.]\n",
    "    X_valid = X[shuffle_idx][num_elems/2.:num_elems*3./4.]\n",
    "    X_test = X[shuffle_idx][num_elems*3./4.:]\n",
    "\n",
    "    y_train = y[shuffle_idx][:num_elems/2.]\n",
    "    y_valid = y[shuffle_idx][num_elems/2.:num_elems*3./4.]\n",
    "    y_test = y[shuffle_idx][num_elems*3./4.:]\n",
    "    \n",
    "    X_full_train = np.append(X_train, X_valid, axis=0)\n",
    "    y_full_train = np.append(y_train, y_valid)\n",
    "    \n",
    "    clf = xgb.XGBClassifier(objective='rank:ndcg')\n",
    "    clf.fit(X_full_train,y_full_train)\n",
    "    tempresult += ndcgl(clf.predict(X_test), y_test) / 5.\n",
    "    \n",
    "print tempresult"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
