{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from copy import deepcopy\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class LambdaMART:\n",
    "    def __init__(self, n_estimators, base_estimator=DecisionTreeRegressor(), step_estimator=DecisionTreeRegressor(),\n",
    "                 alpha=1., beta=1., adaptive_step=True, feature_subset=False, feature_fraction=1., stochastic=False):\n",
    "        \"\"\"\n",
    "            n_estimators : число деревьев в обучении\n",
    "            base_estimator : выбор модели для каждой итерации\n",
    "            step_estimator : выбор модели для предсказания темпа обучения\n",
    "            alpha : коэффициент регуляризации XGBoost\n",
    "            beta : коэффициент при предсказании темпа обучения\n",
    "            adaptive_step : использовать адаптивный шаг бустинга или нет\n",
    "            feature_subset : использовать случайный набор признаков или нет\n",
    "        \"\"\"\n",
    "        self.estimators = []\n",
    "        self.step_estimators = []\n",
    "        self.feature_idx = []\n",
    "        self.n_estimators = n_estimators\n",
    "        self.base_estimator = base_estimator\n",
    "        self.step_estimator = step_estimator\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.adaptive_step = adaptive_step\n",
    "        self.feature_subset = feature_subset\n",
    "        self.feature_fraction = feature_fraction\n",
    "        self.stochastic = stochastic\n",
    "        \n",
    "    def fit(self, X_train, y_train, qids_train, queries_train, X_test, y_test, qids_test, queries_test, verbose=False):\n",
    "        \"\"\"\n",
    "            X : признаки пар запрос-документ\n",
    "            y : метки релевантности\n",
    "            qids : id запросов\n",
    "            queries : признаки запросов\n",
    "        \"\"\"\n",
    "        X = X_train\n",
    "        y = y_train\n",
    "        qids = qids_train\n",
    "        queries = queries_train\n",
    "        self.estimators = []\n",
    "        self.step_estimators = []\n",
    "        # Препроцессинг для ускорения подсчитывания градиента\n",
    "        id_y = np.argsort(np.argsort(y_train)[::-1])\n",
    "        for i in xrange(self.n_estimators):\n",
    "            if i % 30 == 1:\n",
    "                y_pred = self.predict(X_test, qids_test, queries_test)\n",
    "                print \"Iteration \", i, \" result \", ndcgl(y_pred, y_test)\n",
    "            #print \"Iteration #\", i\n",
    "            predicted = self.predict(X_train, qids_train, queries_train, verbose)\n",
    "            sub_idx = []\n",
    "            if self.stochastic:\n",
    "                # Выбираем подвыборку элементов\n",
    "                sub_idx = np.random.choice(np.arange(X_train.shape[0]), int(0.6 * X_train.shape[0]), replace=False)\n",
    "                grad, h = self.loss_grad(predicted[sub_idx], y_train[sub_idx], id_y[sub_idx])\n",
    "                X = X_train[sub_idx]\n",
    "                y = y_train[sub_idx]\n",
    "                qids = qids_train[sub_idx]\n",
    "                queries = queries_train[sub_idx]\n",
    "            else:\n",
    "                grad, h = self.loss_grad(predicted, y, id_y)\n",
    "            if verbose:\n",
    "                print \"Grad:\"\n",
    "                print grad\n",
    "                print \"H:\"\n",
    "                print h\n",
    "            estimator = deepcopy(self.base_estimator)\n",
    "            if self.feature_subset:\n",
    "                # Список индексов фичей\n",
    "                feature_idx = np.random.choice(np.arange(X.shape[1]), int(self.feature_fraction * X.shape[1]), replace=False)\n",
    "                self.feature_idx.append(feature_idx)\n",
    "                estimator.fit(X.T[feature_idx].T, -grad/ (self.alpha + h))\n",
    "            else:\n",
    "                estimator.fit(X, -grad/ (self.alpha + h))\n",
    "            self.estimators.append(estimator)\n",
    "            if verbose:\n",
    "                print \"Predict:\"\n",
    "                print predicted\n",
    "            # Обучим предсказатель шага\n",
    "            if self.adaptive_step:\n",
    "                step_estimator = deepcopy(self.step_estimator)\n",
    "                # Список всех запросов(уникальных)\n",
    "                qs = np.unique(qids)\n",
    "                # Список значений того, что нужно предсказывать\n",
    "                pred_values = np.zeros(len(qs))\n",
    "                q_list = np.zeros((len(qs), queries.shape[1]))\n",
    "                for idx, q in enumerate(qs):\n",
    "                    # Суммируем значения по всем элементам с данным запросом q\n",
    "                    pred_values[idx] = 0.\n",
    "                    for j in xrange(X.shape[0]):\n",
    "                        if qids[j] == q:\n",
    "                            q_list[idx] = queries[j]\n",
    "                            pred_values[idx] += 1. / (1. + self.beta * h[j])            \n",
    "                    #q_list[idx] = q_dict[q]\n",
    "                sum_pred_values = np.sum(pred_values)\n",
    "                step_estimator.fit(q_list, pred_values * len(qs) / sum_pred_values)\n",
    "                self.step_estimators.append(step_estimator)\n",
    "        return self\n",
    "            \n",
    "    def predict(self, X, qids, queries, verbose=False):\n",
    "        y_pred = np.zeros(X.shape[0])\n",
    "        for est_i, estimator in enumerate(self.estimators):\n",
    "            if self.feature_subset:\n",
    "                feature_idx = self.feature_idx[est_i]\n",
    "                est_result = estimator.predict(X.T[feature_idx].T)\n",
    "            else:\n",
    "                est_result = estimator.predict(X)\n",
    "            for i in xrange(X.shape[0]):\n",
    "                step = 1. / (i + 1.)\n",
    "                if self.adaptive_step:\n",
    "                    step = self.step_estimators[est_i].predict(queries[i].reshape(1,-1)[0])\n",
    "                if verbose:\n",
    "                    print \"Step: \", step, \" for qid=\", qids[i]\n",
    "                y_pred[i] += est_result[i] * step\n",
    "        return y_pred\n",
    "    \n",
    "    def loss_grad(self, pred_y, y, id_y):\n",
    "        n_elems = y.shape[0]\n",
    "        grad = np.zeros(n_elems)\n",
    "        h = np.zeros(n_elems)\n",
    "        # id_y - индексы в массиве pred_y, отсортированные по убыванию ранжирующей функции \n",
    "        #id_y = np.argsort(np.argsort(y)[::-1])\n",
    "        \n",
    "        for i in xrange(n_elems):\n",
    "            for j in xrange(n_elems):\n",
    "                buf = 0.\n",
    "                delta_ndcg = np.abs(self.ndcg_replace(y[i], y[j], id_y[i], id_y[j]))\n",
    "                if y[i] > y[j]:\n",
    "                    buf = -self.rho(pred_y[i], pred_y[j])\n",
    "                if y[i] < y[j]:\n",
    "                    buf = self.rho(pred_y[j], pred_y[i])\n",
    "                if buf != 0.:\n",
    "                    grad[i] += delta_ndcg * buf\n",
    "                h[i] += delta_ndcg * self.rho(pred_y[i], pred_y[j]) * (1 - self.rho(pred_y[i], pred_y[j]))\n",
    "        return grad, h\n",
    "        \n",
    "    def double_grad(self, pred_y, y):\n",
    "        n_elems = y.shape[0]\n",
    "        h = np.zeros(n_elems)\n",
    "        # id_y - индексы в массиве pred_y, отсортированные по убыванию ранжирующей функции \n",
    "        id_y = np.argsort(np.argsort(y)[::-1])\n",
    "        for i in xrange(n_elems):\n",
    "            for j in xrange(n_elems):\n",
    "                delta_ndcg = np.abs(self.ndcg_replace(y[i], y[j], id_y[i], id_y[j]))\n",
    "                h[i] += delta_ndcg * self.rho(pred_y[i], pred_y[j]) * (1 - self.rho(pred_y[i], pred_y[j]))\n",
    "        return h\n",
    "    \n",
    "    def ndcg_replace(self, value_a, value_b, id_a, id_b):\n",
    "        return (2. ** value_b - 2. ** value_a) * (1./ np.log2(2 + id_a) - 1./ np.log2(2 + id_b))\n",
    "    \n",
    "    def rho(self, a, b):\n",
    "        return 1. / (1. + np.exp(a - b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Считаем часть датасета MQ2007"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Проверка на части датасета MQ2007\n",
    "fin = open('train.txt', 'r')\n",
    "\n",
    "num_elems = 2000\n",
    "X = np.zeros((num_elems, 46))\n",
    "y = np.zeros(num_elems)\n",
    "queries = np.zeros(num_elems)\n",
    "\n",
    "num = 0\n",
    "i = 0\n",
    "\n",
    "lines = fin.readlines()[:num_elems]\n",
    "\n",
    "for i, line in enumerate(lines):\n",
    "    parsed = line.split(' ')\n",
    "    y[i] = int(parsed[0])\n",
    "    queries[i] = int(parsed[1][4:])\n",
    "    for j in xrange(46):\n",
    "        if j < 9:\n",
    "            X[i][j] = float(parsed[j + 2][2:])\n",
    "        else:\n",
    "            X[i][j] = float(parsed[j + 2][3:])\n",
    "                \n",
    "# Нормализуем y\n",
    "y /= np.max(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Генерируем признаки для запросов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# Признаки для запросов\n",
    "query_d = 20 # Число признаков в запросе. \n",
    "q_dict = defaultdict(lambda : np.zeros(query_d)) # dictionary, который по id запроса возвращает его признаки\n",
    "\n",
    "unique_q, counts = np.unique(queries, return_counts=True)\n",
    "for i,q in enumerate(unique_q):\n",
    "    for j in xrange(X.shape[0]):\n",
    "        if queries[j] == q:\n",
    "            q_dict[q] += X[j][11:31]/ counts[i]\n",
    "\n",
    "# Финальная матрица для запросов. По индексу получаем признаки запроса, соответствующие элементу из X с этим индексом\n",
    "q = np.zeros((num_elems, query_d))\n",
    "for i in xrange(num_elems):\n",
    "    q[i] = q_dict[queries[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ndcgl(y_pred, y, L=10):\n",
    "    \"\"\"\n",
    "        y_pred : предсказанные значения функции ранжирования\n",
    "        y : метки релевантности\n",
    "    \"\"\"\n",
    "    dcgl = 0.\n",
    "    idcgl = 0.\n",
    "    # Отсортируем значения по убыванию функции ранжирования\n",
    "    idx = np.argsort(y_pred)[::-1]\n",
    "    # Метки релевантности для отсортированного массива y_pred\n",
    "    l = y[idx]\n",
    "    for i in xrange(L):\n",
    "        dcgl += (2. ** l[i] - 1) / np.log2(i + 2)\n",
    "        idcgl += 1 / np.log2(i + 2)\n",
    "    return dcgl / idcgl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "shuffle_idx = np.random.permutation(num_elems)\n",
    "X_train = X[shuffle_idx][:num_elems/2.]\n",
    "X_valid = X[shuffle_idx][num_elems/2.:num_elems*3./4.]\n",
    "X_test = X[shuffle_idx][num_elems*3./4.:]\n",
    "\n",
    "y_train = y[shuffle_idx][:num_elems/2.]\n",
    "y_valid = y[shuffle_idx][num_elems/2.:num_elems*3./4.]\n",
    "y_test = y[shuffle_idx][num_elems*3./4.:]\n",
    "\n",
    "q_train = queries[shuffle_idx][:num_elems/2.]\n",
    "q_valid = queries[shuffle_idx][num_elems/2.:num_elems*3./4.]\n",
    "q_test = queries[shuffle_idx][num_elems*3./4.:]\n",
    "\n",
    "queries_train = q[shuffle_idx][:num_elems/2.]\n",
    "queries_valid = q[shuffle_idx][num_elems/2.:num_elems*3./4.]\n",
    "queries_test = q[shuffle_idx][num_elems*3./4.:]\n",
    "\n",
    "X_full_train = np.append(X_train, X_valid, axis=0)\n",
    "y_full_train = np.append(y_train, y_valid)\n",
    "q_full_train = np.append(q_train, q_valid)\n",
    "queries_full_train = np.append(queries_train, queries_valid, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 1s\n"
     ]
    }
   ],
   "source": [
    "%time clf1 = LambdaMART(20, alpha=0.5, beta=1., feature_subset=True, feature_fraction=0.3).fit(X_full_train, y_full_train, q_full_train, queries_full_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "225\n",
      "225\n",
      "225\n",
      "225\n",
      "225\n",
      "225\n",
      "225\n",
      "225\n",
      "225\n",
      "225\n",
      "225\n",
      "225\n",
      "225\n",
      "225\n",
      "225\n",
      "225\n",
      "225\n",
      "225\n",
      "225\n",
      "225\n",
      "Wall time: 25.3 s\n"
     ]
    }
   ],
   "source": [
    "%time clf2 = LambdaMART(20, alpha=0.5, beta=1., feature_subset=True, \\\n",
    "                        feature_fraction=0.3, stochastic=True).fit(X_full_train, y_full_train, q_full_train, queries_full_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.586115924681\n",
      "0.480710997422\n"
     ]
    }
   ],
   "source": [
    "print ndcgl(clf1.predict(X_test, q_test, queries_test), y_test)\n",
    "print ndcgl(clf2.predict(X_test, q_test, queries_test), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 71 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(criterion='mse', max_depth=None, max_features=None,\n",
       "           max_leaf_nodes=None, min_impurity_split=1e-07,\n",
       "           min_samples_leaf=1, min_samples_split=2,\n",
       "           min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "           splitter='best')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time DecisionTreeRegressor().fit(X_full_train, y_full_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5 1 0.5 0.336099646865 0.334095124305\n",
      "0.5 1 1.0 0.284810928463 0.336022586491\n",
      "0.5 1 2.0 0.278983133131 0.235084867792\n",
      "0.5 2 0.5 0.41276830698 0.325806649599\n",
      "0.5 2 1.0 0.269452898252 0.219380711384\n",
      "0.5 2 2.0 0.357579678573 0.328906145272\n",
      "0.5 5 0.5 0.360515329939 0.291737452324\n",
      "0.5 5 1.0 0.396132881545 0.352325317071\n",
      "0.5 5 2.0 0.357491363348 0.307072058994\n",
      "0.5 7 0.5 0.307196757893 0.243569464569\n",
      "0.5 7 1.0 0.468726043507 0.381784322914\n",
      "0.5 7 2.0 0.433379695178 0.35421116786\n",
      "1.0 1 0.5 0.298903738381 0.329808291355\n",
      "1.0 1 1.0 0.452061919927 0.348656410939\n",
      "1.0 1 2.0 0.43510791712 0.279133197285\n",
      "1.0 2 0.5 0.298245437817 0.270975813759\n",
      "1.0 2 1.0 0.426176753028 0.298526360796\n",
      "1.0 2 2.0 0.30197343755 0.296332263381\n",
      "1.0 5 0.5 0.444450124696 0.334779000506\n",
      "1.0 5 1.0 0.431277962121 0.373842327464\n",
      "1.0 5 2.0 0.429456655432 0.344643182745\n",
      "1.0 7 0.5 0.438494402494 0.358339454444\n",
      "1.0 7 1.0 0.431007588023 0.367884660402\n",
      "1.0 7 2.0 0.446516329361 0.336559264488\n",
      "2.0 1 0.5 0.259869836501 0.210006019129\n",
      "2.0 1 1.0 0.405543147091 0.281986651447\n",
      "2.0 1 2.0 0.313821884084 0.268496532276\n",
      "2.0 2 0.5 0.458445432413 0.350849454051\n",
      "2.0 2 1.0 0.257580355035 0.30694327258\n",
      "2.0 2 2.0 0.333093525779 0.259259257385\n",
      "2.0 5 0.5 0.34329521534 0.222004082956\n",
      "2.0 5 1.0 0.34956759061 0.263642761974\n",
      "2.0 5 2.0 0.345498792654 0.35035288334\n",
      "2.0 7 0.5 0.442491213356 0.37117135548\n",
      "2.0 7 1.0 0.425625394444 0.319880540672\n",
      "2.0 7 2.0 0.434023061723 0.394820552599\n"
     ]
    }
   ],
   "source": [
    "# Небольшой GridSearch с кросс-валидацией\n",
    "from numpy import unravel_index\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "arr_a = [0.5, 1., 2.] # Одно значение в целях оптимизации\n",
    "arr_L = [1, 2, 5, 7]\n",
    "arr_b = [0.5, 1., 2.] # Одно значение в целях оптимизации\n",
    "\n",
    "results1 = np.zeros((len(arr_a), len(arr_L), len(arr_b))) # С адаптивным шагом\n",
    "results2 = np.zeros((len(arr_a), len(arr_L), len(arr_b))) # Без адаптивного шага\n",
    "\n",
    "for i_a, a in enumerate(arr_a):\n",
    "    for i_L, L in enumerate(arr_L):\n",
    "        for i_b, b in enumerate(arr_b):\n",
    "            for k in xrange(5):\n",
    "                # Перемешаем\n",
    "                shuffle_idx = np.random.permutation(num_elems)\n",
    "                X_train = X[shuffle_idx][:num_elems/2.]\n",
    "                X_valid = X[shuffle_idx][num_elems/2.:num_elems*3./4.]\n",
    "\n",
    "                y_train = y[shuffle_idx][:num_elems/2.]\n",
    "                y_valid = y[shuffle_idx][num_elems/2.:num_elems*3./4.]\n",
    "\n",
    "                q_train = queries[shuffle_idx][:num_elems/2.]\n",
    "                q_valid = queries[shuffle_idx][num_elems/2.:num_elems*3./4.]\n",
    "\n",
    "                queries_train = q[shuffle_idx][:num_elems/2.]\n",
    "                queries_valid = q[shuffle_idx][num_elems/2.:num_elems*3./4.]\n",
    "                \n",
    "                clf1 = LambdaMART(L, alpha=a, beta=b).fit(X_train, y_train, q_train, queries_train)\n",
    "                y_pred1 = clf1.predict(X_valid, q_valid, queries_valid)\n",
    "                results1[i_a, i_L, i_b] += ndcgl(y_pred1, y_valid) / 5.\n",
    "\n",
    "                clf2 = LambdaMART(L, alpha=a, beta=b, adaptive_step=False).fit(X_train, y_train, q_train, queries_train)\n",
    "                y_pred2 = clf2.predict(X_valid, q_valid, queries_valid)\n",
    "                results2[i_a, i_L, i_b] += ndcgl(y_pred2, y_valid) / 5.\n",
    "\n",
    "                # Значения параметров и результаты с адаптивным шагом и без\n",
    "            print a, L, b, results1[i_a, i_L, i_b], results2[i_a, i_L, i_b]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max with adaptive step - Alpha=0.5, L=7, beta=1.0\n",
      "Max without adaptive step - Alpha=2.0, L=7, beta=2.0\n",
      "Maximum with adaptive step= 0.468726043507\n",
      "Maximum without adaptive step= 0.394820552599\n"
     ]
    }
   ],
   "source": [
    "ad_max = unravel_index(np.argmax(results1), results1.shape)\n",
    "non_ad_max = unravel_index(np.argmax(results2), results2.shape)\n",
    "        \n",
    "print \"Max with adaptive step - Alpha={}, L={}, beta={}\".format(arr_a[ad_max[0]], arr_L[ad_max[1]], arr_b[ad_max[2]])\n",
    "print \"Max without adaptive step - Alpha={}, L={}, beta={}\".format(arr_a[non_ad_max[0]], arr_L[non_ad_max[1]], arr_b[non_ad_max[2]])\n",
    "\n",
    "print \"Maximum with adaptive step=\", np.max(results1)\n",
    "print \"Maximum without adaptive step=\", np.max(results2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "В принципе, из списка на всех параметрах видны корреляции для адаптивного и неадаптивного методов. И как правило, увеличение числа итераций хорошо влияет на результат. Можно было продолжить и далее, но в виду ограниченных мощностей, пока это не главное."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adaptive= 0.473884366146\n",
      "Non-adaptive= 0.328375116338\n"
     ]
    }
   ],
   "source": [
    "result1 = 0.\n",
    "result2 = 0.\n",
    "\n",
    "for k in xrange(5):\n",
    "    shuffle_idx = np.random.permutation(num_elems)\n",
    "    X_train = X[shuffle_idx][:num_elems/2.]\n",
    "    X_valid = X[shuffle_idx][num_elems/2.:num_elems*3./4.]\n",
    "    X_test = X[shuffle_idx][num_elems*3./4.:]\n",
    "\n",
    "    y_train = y[shuffle_idx][:num_elems/2.]\n",
    "    y_valid = y[shuffle_idx][num_elems/2.:num_elems*3./4.]\n",
    "    y_test = y[shuffle_idx][num_elems*3./4.:]\n",
    "\n",
    "    q_train = queries[shuffle_idx][:num_elems/2.]\n",
    "    q_valid = queries[shuffle_idx][num_elems/2.:num_elems*3./4.]\n",
    "    q_test = queries[shuffle_idx][num_elems*3./4.:]\n",
    "\n",
    "    queries_train = q[shuffle_idx][:num_elems/2.]\n",
    "    queries_valid = q[shuffle_idx][num_elems/2.:num_elems*3./4.]\n",
    "    queries_test = q[shuffle_idx][num_elems*3./4.:]\n",
    "    \n",
    "    X_full_train = np.append(X_train, X_valid, axis=0)\n",
    "    y_full_train = np.append(y_train, y_valid)\n",
    "    q_full_train = np.append(q_train, q_valid)\n",
    "    queries_full_train = np.append(queries_train, queries_valid, axis=0)\n",
    "    \n",
    "    clf1 = LambdaMART(7, alpha=0.5, beta=1.).fit(X_full_train, y_full_train, q_full_train, queries_full_train)\n",
    "    y_pred1 = clf1.predict(X_test, q_test, queries_test)\n",
    "\n",
    "    clf2 = LambdaMART(7, alpha=2.0, beta=2.0, adaptive_step=False).fit(X_full_train, y_full_train, q_full_train, queries_full_train)\n",
    "    y_pred2 = clf2.predict(X_test, q_test, queries_test)\n",
    "    \n",
    "    result1 += ndcgl(y_pred1, y_test) / 5.\n",
    "    result2 += ndcgl(y_pred2, y_test) / 5.\n",
    "\n",
    "print \"Adaptive=\", result1\n",
    "print \"Non-adaptive=\", result2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.349995209906\n"
     ]
    }
   ],
   "source": [
    "# Обычный DecisionTreeRegressor\n",
    "tempresult = 0.\n",
    "\n",
    "for k in xrange(5):\n",
    "    shuffle_idx = np.random.permutation(num_elems)\n",
    "    X_train = X[shuffle_idx][:num_elems/2.]\n",
    "    X_valid = X[shuffle_idx][num_elems/2.:num_elems*3./4.]\n",
    "    X_test = X[shuffle_idx][num_elems*3./4.:]\n",
    "\n",
    "    y_train = y[shuffle_idx][:num_elems/2.]\n",
    "    y_valid = y[shuffle_idx][num_elems/2.:num_elems*3./4.]\n",
    "    y_test = y[shuffle_idx][num_elems*3./4.:]\n",
    "    \n",
    "    X_full_train = np.append(X_train, X_valid, axis=0)\n",
    "    y_full_train = np.append(y_train, y_valid)\n",
    "\n",
    "    tempclf = DecisionTreeRegressor().fit(X_full_train, y_full_train)\n",
    "    tempresult += ndcgl(tempclf.predict(X_test), y_test) / 5.\n",
    "    \n",
    "print tempresult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.424421739725\n"
     ]
    }
   ],
   "source": [
    "# GDBT\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "tempresult = 0.\n",
    "\n",
    "for k in xrange(5):\n",
    "    shuffle_idx = np.random.permutation(num_elems)\n",
    "    X_train = X[shuffle_idx][:num_elems/2.]\n",
    "    X_valid = X[shuffle_idx][num_elems/2.:num_elems*3./4.]\n",
    "    X_test = X[shuffle_idx][num_elems*3./4.:]\n",
    "\n",
    "    y_train = y[shuffle_idx][:num_elems/2.]\n",
    "    y_valid = y[shuffle_idx][num_elems/2.:num_elems*3./4.]\n",
    "    y_test = y[shuffle_idx][num_elems*3./4.:]\n",
    "    \n",
    "    X_full_train = np.append(X_train, X_valid, axis=0)\n",
    "    y_full_train = np.append(y_train, y_valid)\n",
    "\n",
    "    tempclf = GradientBoostingRegressor().fit(X_full_train, y_full_train)\n",
    "    tempresult += ndcgl(tempclf.predict(X_test), y_test) / 5.\n",
    "    \n",
    "print tempresult"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LambdaMART из XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Необходимые технические строки. Исполняйте их в случае ошибки WindowsError Error 127. \n",
    "# Путь заменить на своё расположение mingw-64\n",
    "dir = r'C:\\Program Files\\mingw-w64\\x86_64-6.2.0-posix-seh-rt_v5-rev1\\mingw64\\bin'\n",
    "import os\n",
    "\n",
    "os.environ['PATH'].find(dir)\n",
    "os.environ['PATH'] = dir + ';' + os.environ['PATH']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.434840804088\n"
     ]
    }
   ],
   "source": [
    "tempresult = 0.\n",
    "\n",
    "for k in xrange(5):\n",
    "    shuffle_idx = np.random.permutation(num_elems)\n",
    "    X_train = X[shuffle_idx][:num_elems/2.]\n",
    "    X_valid = X[shuffle_idx][num_elems/2.:num_elems*3./4.]\n",
    "    X_test = X[shuffle_idx][num_elems*3./4.:]\n",
    "\n",
    "    y_train = y[shuffle_idx][:num_elems/2.]\n",
    "    y_valid = y[shuffle_idx][num_elems/2.:num_elems*3./4.]\n",
    "    y_test = y[shuffle_idx][num_elems*3./4.:]\n",
    "    \n",
    "    X_full_train = np.append(X_train, X_valid, axis=0)\n",
    "    y_full_train = np.append(y_train, y_valid)\n",
    "    \n",
    "    clf = xgb.XGBClassifier(objective='rank:ndcg')\n",
    "    clf.fit(X_full_train,y_full_train)\n",
    "    tempresult += ndcgl(clf.predict(X_test), y_test) / 5.\n",
    "    \n",
    "print tempresult"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Работа на 4000 элементах, 70 итераций бустинга"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration # 0\n",
      "Iteration # 1\n",
      "Iteration # 2\n",
      "Iteration # 3\n",
      "Iteration # 4\n",
      "Iteration # 5\n",
      "Iteration # 6\n",
      "Iteration # 7\n",
      "Iteration # 8\n",
      "Iteration # 9\n",
      "Iteration # 10\n",
      "Iteration # 11\n",
      "Iteration # 12\n",
      "Iteration # 13\n",
      "Iteration # 14\n",
      "Iteration # 15\n",
      "Iteration # 16\n",
      "Iteration # 17\n",
      "Iteration # 18\n",
      "Iteration # 19\n",
      "Iteration # 20\n",
      "Iteration # 21\n",
      "Iteration # 22\n",
      "Iteration # 23\n",
      "Iteration # 24\n",
      "Iteration # 25\n",
      "Iteration # 26\n",
      "Iteration # 27\n",
      "Iteration # 28\n",
      "Iteration # 29\n",
      "Iteration # 30\n",
      "Iteration # 31\n",
      "Iteration # 32\n",
      "Iteration # 33\n",
      "Iteration # 34\n",
      "Iteration # 35\n",
      "Iteration # 36\n",
      "Iteration # 37\n",
      "Iteration # 38\n",
      "Iteration # 39\n",
      "Iteration # 40\n",
      "Iteration # 41\n",
      "Iteration # 42\n",
      "Iteration # 43\n",
      "Iteration # 44\n",
      "Iteration # 45\n",
      "Iteration # 46\n",
      "Iteration # 47\n",
      "Iteration # 48\n",
      "Iteration # 49\n",
      "Iteration # 50\n",
      "Iteration # 51\n",
      "Iteration # 52\n",
      "Iteration # 53\n",
      "Iteration # 54\n",
      "Iteration # 55\n",
      "Iteration # 56\n",
      "Iteration # 57\n",
      "Iteration # 58\n",
      "Iteration # 59\n",
      "Iteration # 60\n",
      "Iteration # 61\n",
      "Iteration # 62\n",
      "Iteration # 63\n",
      "Iteration # 64\n",
      "Iteration # 65\n",
      "Iteration # 66\n",
      "Iteration # 67\n",
      "Iteration # 68\n",
      "Iteration # 69\n",
      "Wall time: 3h 41min 39s\n"
     ]
    }
   ],
   "source": [
    "shuffle_idx = np.random.permutation(num_elems)\n",
    "X_train = X[shuffle_idx][:num_elems/2.]\n",
    "X_valid = X[shuffle_idx][num_elems/2.:num_elems*3./4.]\n",
    "X_test = X[shuffle_idx][num_elems*3./4.:]\n",
    "\n",
    "y_train = y[shuffle_idx][:num_elems/2.]\n",
    "y_valid = y[shuffle_idx][num_elems/2.:num_elems*3./4.]\n",
    "y_test = y[shuffle_idx][num_elems*3./4.:]\n",
    "\n",
    "q_train = queries[shuffle_idx][:num_elems/2.]\n",
    "q_valid = queries[shuffle_idx][num_elems/2.:num_elems*3./4.]\n",
    "q_test = queries[shuffle_idx][num_elems*3./4.:]\n",
    "\n",
    "queries_train = q[shuffle_idx][:num_elems/2.]\n",
    "queries_valid = q[shuffle_idx][num_elems/2.:num_elems*3./4.]\n",
    "queries_test = q[shuffle_idx][num_elems*3./4.:]\n",
    "\n",
    "X_full_train = np.append(X_train, X_valid, axis=0)\n",
    "y_full_train = np.append(y_train, y_valid)\n",
    "q_full_train = np.append(q_train, q_valid)\n",
    "queries_full_train = np.append(queries_train, queries_valid, axis=0)\n",
    "\n",
    "%time clf_big = LambdaMART(70, alpha=0.5, beta=1., ).fit(X_full_train, y_full_train, q_full_train, queries_full_train)\n",
    "#y_pred1 = clf1.predict(X_test, q_test, queries_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.896477581668\n"
     ]
    }
   ],
   "source": [
    "y_pred_big = clf_big.predict(X_test, q_test, queries_test)\n",
    "print ndcgl(y_pred_big, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.733113475428\n"
     ]
    }
   ],
   "source": [
    "# xgboost\n",
    "clf = xgb.XGBClassifier(objective='rank:ndcg')\n",
    "clf.fit(X_full_train,y_full_train)\n",
    "print ndcgl(clf.predict(X_test), y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Проверка случайного выбора фичей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "Without subset 0.589559508361\n",
      "With subset 0.525202845018\n",
      "Stochastic 0.465555208682\n",
      "40\n",
      "Without subset 0.670012899467\n",
      "With subset 0.851643743565\n",
      "Stochastic 0.79434887815\n",
      "70\n",
      "Without subset 0.595825485951\n",
      "With subset 0.837945450273\n",
      "Stochastic 0.732566107838\n",
      "100\n",
      "Without subset 0.625597894605\n",
      "With subset 0.805930029937\n",
      "Stochastic 0.621950881675\n",
      "130\n",
      "Without subset 0.635118472228\n",
      "With subset 0.838777861945\n",
      "Stochastic 0.646343802709\n",
      "160\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-ea5e83d9c0ab>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[0mclf1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLambdaMART\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbeta\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1.\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_full_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_full_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mq_full_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mqueries_full_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m     \u001b[0mclf2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLambdaMART\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbeta\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1.\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeature_subset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeature_fraction\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_full_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_full_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mq_full_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mqueries_full_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m     \u001b[0mclf3\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLambdaMART\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbeta\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1.\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeature_subset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeature_fraction\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.3\u001b[0m\u001b[1;33m,\u001b[0m                       \u001b[0mstochastic\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_full_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_full_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mq_full_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mqueries_full_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-6943b8d58004>\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X_train, y_train, qids_train, queries_train, verbose)\u001b[0m\n\u001b[0;32m     52\u001b[0m                 \u001b[0mqueries\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mqueries_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msub_idx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m                 \u001b[0mgrad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredicted\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mid_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     55\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m                 \u001b[1;32mprint\u001b[0m \u001b[1;34m\"Grad:\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-6943b8d58004>\u001b[0m in \u001b[0;36mloss_grad\u001b[1;34m(self, pred_y, y, id_y)\u001b[0m\n\u001b[0;32m    118\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mxrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_elems\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m                 \u001b[0mbuf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 120\u001b[1;33m                 \u001b[0mdelta_ndcg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndcg_replace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mid_y\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mid_y\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    121\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m                     \u001b[0mbuf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrho\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred_y\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpred_y\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-6943b8d58004>\u001b[0m in \u001b[0;36mndcg_replace\u001b[1;34m(self, value_a, value_b, id_a, id_b)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    141\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mndcg_replace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue_a\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue_b\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mid_a\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mid_b\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 142\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m2.\u001b[0m \u001b[1;33m**\u001b[0m \u001b[0mvalue_b\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m2.\u001b[0m \u001b[1;33m**\u001b[0m \u001b[0mvalue_a\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1.\u001b[0m\u001b[1;33m/\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mid_a\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1.\u001b[0m\u001b[1;33m/\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mid_b\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    143\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    144\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mrho\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "shuffle_idx = np.random.permutation(num_elems)\n",
    "X_train = X[shuffle_idx][:num_elems/2.]\n",
    "X_valid = X[shuffle_idx][num_elems/2.:num_elems*3./4.]\n",
    "X_test = X[shuffle_idx][num_elems*3./4.:]\n",
    "\n",
    "y_train = y[shuffle_idx][:num_elems/2.]\n",
    "y_valid = y[shuffle_idx][num_elems/2.:num_elems*3./4.]\n",
    "y_test = y[shuffle_idx][num_elems*3./4.:]\n",
    "\n",
    "q_train = queries[shuffle_idx][:num_elems/2.]\n",
    "q_valid = queries[shuffle_idx][num_elems/2.:num_elems*3./4.]\n",
    "q_test = queries[shuffle_idx][num_elems*3./4.:]\n",
    "\n",
    "queries_train = q[shuffle_idx][:num_elems/2.]\n",
    "queries_valid = q[shuffle_idx][num_elems/2.:num_elems*3./4.]\n",
    "queries_test = q[shuffle_idx][num_elems*3./4.:]\n",
    "\n",
    "X_full_train = np.append(X_train, X_valid, axis=0)\n",
    "y_full_train = np.append(y_train, y_valid)\n",
    "q_full_train = np.append(q_train, q_valid)\n",
    "queries_full_train = np.append(queries_train, queries_valid, axis=0)\n",
    "\n",
    "results = []\n",
    "\n",
    "for n in xrange(10, 600, 30):\n",
    "    print n\n",
    "\n",
    "    clf1 = LambdaMART(n, alpha=0.5, beta=1.).fit(X_full_train, y_full_train, q_full_train, queries_full_train)\n",
    "    clf2 = LambdaMART(n, alpha=0.5, beta=1., feature_subset=True, feature_fraction=0.3).fit(X_full_train, y_full_train, q_full_train, queries_full_train)\n",
    "    clf3 = LambdaMART(n, alpha=0.5, beta=1., feature_subset=True, feature_fraction=0.3, \\\n",
    "                      stochastic=True).fit(X_full_train, y_full_train, q_full_train, queries_full_train)\n",
    "    \n",
    "    result1 = ndcgl(clf1.predict(X_test, q_test, queries_test), y_test)\n",
    "    result2 = ndcgl(clf2.predict(X_test, q_test, queries_test), y_test)\n",
    "    result3 = ndcgl(clf3.predict(X_test, q_test, queries_test), y_test)\n",
    "    results.append((result1, result2, result3))\n",
    "    \n",
    "    print \"Without subset\", result1\n",
    "    print \"With subset\", result2\n",
    "    print \"Stochastic\", result3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.619296498687\n",
      "0.446995121205\n"
     ]
    }
   ],
   "source": [
    "print ndcgl(clf1.predict(X_test, q_test, queries_test), y_test)\n",
    "print ndcgl(clf2.predict(X_test, q_test, queries_test), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.58955950836136284, 0.52520284501832992, 0.46555520868212591),\n",
       " (0.67001289946652387, 0.85164374356483075, 0.79434887815035293),\n",
       " (0.59582548595062079, 0.83794545027274325, 0.73256610783800979),\n",
       " (0.62559789460458759, 0.80593002993736984, 0.6219508816753645),\n",
       " (0.63511847222811579, 0.83877786194521575, 0.64634380270875258)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
